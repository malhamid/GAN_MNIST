{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Generative Adversarial Model on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Supporting Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eBoStgCOlD6V",
    "outputId": "b35d9172-40ac-494b-9c38-6b1c892f7ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The used TensorFlow version: 1.15.0\n",
      "The used Numpy version: 1.17.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Loading tensorflow related libraries \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Reshape, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# We will be using MNIST in this experiment\n",
    "from keras.datasets.mnist import load_data\n",
    "\n",
    "print('The used TensorFlow version:', tf.__version__)\n",
    "print('The used Numpy version:', np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "vJrX749GlD6k",
    "outputId": "3cdecd15-5704-4840-a984-649fb104ca42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data (60000, 28, 28) , the shape of the labels: (60000,)\n",
      "Shape of the testing data (10000, 28, 28) , the shape of the labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the training data', X_train.shape, ', the shape of the labels:', y_train.shape)\n",
    "print('Shape of the testing data', X_test.shape, ', the shape of the labels:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us visualize 10 examples: \n",
    "# set the subplot\n",
    "fig, axs = plt.subplots(2, 5)\n",
    "for i in range(2):\n",
    "    for j in range(5):  \n",
    "    # plot image pixesles\n",
    "        axs[i,j].imshow(X_train[i+j], cmap=plt.get_cmap('gray'))\n",
    "# Display the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resahping the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 28 * 28))\n",
    "X_test =  np.reshape(X_test, (X_test.shape[0], 28 * 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to rescale the images to values between [0,1]\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "colab_type": "code",
    "id": "JHga1nihlD6q",
    "outputId": "2270db26-8e3b-4ec1-b89d-005a886b8f68"
   },
   "outputs": [],
   "source": [
    "# Defining the discriminator model \n",
    "def building_discriminator():\n",
    "    # The image dimensions provided as inputs\n",
    "    image_shape = (28, 28, 1)\n",
    "    disModel = Sequential()\n",
    "    disModel.add(Conv2D(64, 3, strides=2, input_shape=image_shape))\n",
    "    disModel.add(LeakyReLU())\n",
    "    disModel.add(Dropout(0.4))\n",
    "    # Second layer\n",
    "    disModel.add(Conv2D(64, 3, strides=2))\n",
    "    disModel.add(LeakyReLU()) \n",
    "    disModel.add(Dropout(0.4))\n",
    "    # Flatten the output\n",
    "    disModel.add(Flatten()) \n",
    "    disModel.add(Dense(1, activation='sigmoid'))\n",
    "    # Optimization function\n",
    "    opt = tf.keras.optimizers.Adam(lr=2e-4, beta_1=0.5)\n",
    "    # Compile the model\n",
    "    disModel.compile(loss='binary_crossentropy', optimizer=opt, metrics = ['accuracy'])\n",
    "    return disModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mohammedalhamid/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/mohammedalhamid/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 13, 13, 64)        640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 6, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2305      \n",
      "=================================================================\n",
      "Total params: 39,873\n",
      "Trainable params: 39,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = building_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to train the discriminator model using real and fake examples. \n",
    "def generate_real_images(n_samples):\n",
    "    real_imgs = X_train[randint(0, X_train.shape[0], n_samples)]\n",
    "    # Making sure the size of the images is (n_samples, 28, 28, 1)\n",
    "    real_imgs = real_imgs.reshape(real_imgs.shape[0], 28, 28, 1)\n",
    "    y_real = np.ones((n_samples, 1))\n",
    "    return real_imgs, y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us visualize 10 examples from the real images: \n",
    "X_real_imgs, _ = generate_real_images(int(256/2))\n",
    "# set the subplot\n",
    "fig, axs = plt.subplots(2, 5)\n",
    "for i in range(2):\n",
    "    for j in range(5):  \n",
    "    # plot image pixesles\n",
    "        axs[i,j].imshow(np.reshape(X_real_imgs[i+j], (28,28)), cmap='binary')\n",
    "# Display the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_images(n_samples):\n",
    "    fake_imgs = np.random.rand(28 * 28 * n_samples)\n",
    "    # Making sure the size of the images is (n_samples, 28, 28, 1)\n",
    "    fake_imgs = np.reshape(fake_imgs, (n_samples, 28, 28, 1))\n",
    "    y_fake = np.zeros((n_samples, 1))\n",
    "    return fake_imgs, y_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us visualize 10 examples from the fake images: \n",
    "X_fake_imgs, _ = generate_fake_images(int(256/2))\n",
    "# Let us visualize 10 examples: \n",
    "# set the subplot\n",
    "fig, axs = plt.subplots(2, 5)\n",
    "for i in range(2):\n",
    "    for j in range(5):  \n",
    "    # plot image pixesles\n",
    "        axs[i,j].imshow(np.reshape(X_fake_imgs[i+j], (28,28)), cmap='binary')\n",
    "# Display the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in epoch 1, on real images=70% , on fake images=12%\n",
      "Accuracy in epoch 2, on real images=71% , on fake images=20%\n",
      "Accuracy in epoch 3, on real images=71% , on fake images=36%\n",
      "Accuracy in epoch 4, on real images=73% , on fake images=59%\n",
      "Accuracy in epoch 5, on real images=73% , on fake images=70%\n",
      "Accuracy in epoch 6, on real images=74% , on fake images=78%\n",
      "Accuracy in epoch 7, on real images=78% , on fake images=92%\n",
      "Accuracy in epoch 8, on real images=78% , on fake images=98%\n",
      "Accuracy in epoch 9, on real images=77% , on fake images=99%\n",
      "Accuracy in epoch 10, on real images=72% , on fake images=100%\n",
      "Accuracy in epoch 11, on real images=81% , on fake images=100%\n",
      "Accuracy in epoch 12, on real images=74% , on fake images=100%\n",
      "Accuracy in epoch 13, on real images=75% , on fake images=100%\n",
      "Accuracy in epoch 14, on real images=77% , on fake images=100%\n",
      "Accuracy in epoch 15, on real images=77% , on fake images=100%\n",
      "Accuracy in epoch 16, on real images=83% , on fake images=100%\n",
      "Accuracy in epoch 17, on real images=80% , on fake images=100%\n",
      "Accuracy in epoch 18, on real images=75% , on fake images=100%\n",
      "Accuracy in epoch 19, on real images=77% , on fake images=100%\n",
      "Accuracy in epoch 20, on real images=84% , on fake images=100%\n",
      "Accuracy in epoch 21, on real images=82% , on fake images=100%\n",
      "Accuracy in epoch 22, on real images=86% , on fake images=100%\n",
      "Accuracy in epoch 23, on real images=88% , on fake images=100%\n",
      "Accuracy in epoch 24, on real images=84% , on fake images=100%\n",
      "Accuracy in epoch 25, on real images=88% , on fake images=100%\n",
      "Accuracy in epoch 26, on real images=91% , on fake images=100%\n",
      "Accuracy in epoch 27, on real images=94% , on fake images=100%\n",
      "Accuracy in epoch 28, on real images=89% , on fake images=100%\n",
      "Accuracy in epoch 29, on real images=94% , on fake images=100%\n",
      "Accuracy in epoch 30, on real images=95% , on fake images=100%\n",
      "Accuracy in epoch 31, on real images=93% , on fake images=100%\n",
      "Accuracy in epoch 32, on real images=96% , on fake images=100%\n",
      "Accuracy in epoch 33, on real images=98% , on fake images=100%\n",
      "Accuracy in epoch 34, on real images=95% , on fake images=100%\n",
      "Accuracy in epoch 35, on real images=95% , on fake images=100%\n",
      "Accuracy in epoch 36, on real images=98% , on fake images=100%\n",
      "Accuracy in epoch 37, on real images=99% , on fake images=100%\n",
      "Accuracy in epoch 38, on real images=98% , on fake images=100%\n",
      "Accuracy in epoch 39, on real images=98% , on fake images=100%\n",
      "Accuracy in epoch 40, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 41, on real images=98% , on fake images=100%\n",
      "Accuracy in epoch 42, on real images=99% , on fake images=100%\n",
      "Accuracy in epoch 43, on real images=99% , on fake images=100%\n",
      "Accuracy in epoch 44, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 45, on real images=99% , on fake images=100%\n",
      "Accuracy in epoch 46, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 47, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 48, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 49, on real images=99% , on fake images=100%\n",
      "Accuracy in epoch 50, on real images=99% , on fake images=100%\n",
      "Accuracy in epoch 51, on real images=99% , on fake images=100%\n",
      "Accuracy in epoch 52, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 53, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 54, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 55, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 56, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 57, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 58, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 59, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 60, on real images=99% , on fake images=100%\n",
      "Accuracy in epoch 61, on real images=99% , on fake images=100%\n",
      "Accuracy in epoch 62, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 63, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 64, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 65, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 66, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 67, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 68, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 69, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 70, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 71, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 72, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 73, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 74, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 75, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 76, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 77, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 78, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 79, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 80, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 81, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 82, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 83, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 84, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 85, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 86, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 87, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 88, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 89, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 90, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 91, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 92, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 93, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 94, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 95, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 96, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 97, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 98, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 99, on real images=100% , on fake images=100%\n",
      "Accuracy in epoch 100, on real images=100% , on fake images=100%\n"
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "n_batch=256\n",
    "for i in range(epochs):\n",
    "    # Generate true samples \n",
    "    X_real_imgs, y_real = generate_real_images(int(n_batch/2))\n",
    "    # train the model on a collected batch\n",
    "    _, acc_on_real = discriminator.train_on_batch(X_real_imgs, y_real)\n",
    "    # Generate fake samples\n",
    "    X_fake_imgs, y_fake = generate_fake_images(int(n_batch/2))\n",
    "    # train the model on a collected batch\n",
    "    _, acc_on_fake = discriminator.train_on_batch(X_fake_imgs, y_fake)\n",
    "    # Display training performance\n",
    "    print('Accuracy in epoch %d, on real images=%.0f%% , on fake images=%.0f%%' % (i+1, acc_on_real * 100, acc_on_fake * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Generator Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the generator model \n",
    "def building_generator(noise_dim):\n",
    "    genModel = Sequential()\n",
    "    genModel.add(Dense(128 * 6 * 6, input_dim=noise_dim))\n",
    "    genModel.add(LeakyReLU())\n",
    "    genModel.add(Reshape((6,6,128)))\n",
    "    # Second layer\n",
    "    genModel.add(Conv2DTranspose(128, (4,4), strides=(2,2)))\n",
    "    genModel.add(LeakyReLU())\n",
    "    # Third layer\n",
    "    genModel.add(Conv2DTranspose(128, (4,4), strides=(2,2)))\n",
    "    genModel.add(LeakyReLU())\n",
    "    genModel.add(Conv2D(1, (3,3), activation='sigmoid'))\n",
    "    return genModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4608)              465408    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 30, 30, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 1)         1153      \n",
      "=================================================================\n",
      "Total params: 991,105\n",
      "Trainable params: 991,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = building_generator(100)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Synthetic Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img_using_model(generator, noise_dim, n_samples):\n",
    "    noise = np.random.randn(noise_dim * n_samples)\n",
    "    noise = noise.reshape(n_samples, noise_dim)\n",
    "    fake_imgs = generator.predict(noise)\n",
    "    y_fake = np.zeros((n_samples, 1))\n",
    "    return fake_imgs, y_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real, y_real = generate_real_images(int(256/2))\n",
    "X_fake, y_fake = generate_img_using_model(generator, 100, int(256/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"1280\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us visualize 10 examples from the real images: \n",
    "X_real_imgs, _ = generate_real_images(int(256/2))\n",
    "# set the subplot\n",
    "fig, axs = plt.subplots(2, 5)\n",
    "for i in range(2):\n",
    "    for j in range(5):  \n",
    "    # plot image pixesles\n",
    "        if i == 0: \n",
    "            axs[i,j].imshow(X_real[j, :, :, 0], cmap='gray')\n",
    "        else: \n",
    "            axs[i,j].imshow(X_fake[j, :, :, 0], cmap='gray')\n",
    "# Display the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Generative Adversarial Network (GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "6YdMQyzclD69",
    "outputId": "159ad710-ce47-4c1a-9459-b14e348ad80d"
   },
   "outputs": [],
   "source": [
    "def building_gan(generator, discriminator):\n",
    "    GAN = Sequential()\n",
    "    discriminator.trainable = False\n",
    "    # Adding the generator and the discriminator\n",
    "    GAN.add(generator)\n",
    "    GAN.add(discriminator)\n",
    "    # Optimization function\n",
    "    opt = tf.keras.optimizers.Adam(lr=2e-4, beta_1=0.5)\n",
    "    # Compile the model \n",
    "    GAN.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_3 (Sequential)    (None, 28, 28, 1)         991105    \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 1)                 39873     \n",
      "=================================================================\n",
      "Total params: 1,030,978\n",
      "Trainable params: 991,105\n",
      "Non-trainable params: 39,873\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = building_discriminator()\n",
    "generator = building_generator(100)\n",
    "gan_model = building_gan(generator,discriminator )\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_gan(gan_model, discriminator, generator, batch_size=256, epochs=100, epoch_steps=468, noise_dim=100):\n",
    "    # Training the model by enumerating epochs \n",
    "    for epoch in range(0,epochs): \n",
    "        for step in range(0, epoch_steps):\n",
    "            # Generating fake images \n",
    "            X_fake, y_fake = generate_img_using_model(generator, noise_dim, batch_size)\n",
    "            # Generating real images \n",
    "            X_real, y_real = generate_real_images(batch_size)\n",
    "            # Creating training set\n",
    "            X_batch = np.concatenate([X_real, X_fake], axis = 0)\n",
    "            y_batch = np.concatenate([y_real, y_fake], axis = 0)      \n",
    "            # Training the discriminator\n",
    "            d_loss, d_acc = discriminator.train_on_batch(X_batch, y_batch)\n",
    "            # Gnerating noise input for the generator \n",
    "            X_gan = np.random.randn(noise_dim * batch_size)\n",
    "            X_gan = X_gan.reshape(batch_size, noise_dim)\n",
    "            y_gan = np.ones((batch_size, 1))\n",
    "            # Training the GAN model using the generated noise \n",
    "            gan_loss = gan_model.train_on_batch(X_gan,y_gan)\n",
    "            # Report the trai\n",
    "            report_porgress(epoch=epoch, step=step, d_loss=d_loss, gan_loss=gan_loss, noise_dim=noise_dim, epoch_steps=epoch_steps)\n",
    "        # Report the progress on the full epoch\n",
    "        report_porgress(epoch=epoch, step=step, d_loss=d_loss, gan_loss=gan_loss, noise_dim=noise_dim, epoch_steps=epoch_steps, generator=generator, discriminator=discriminator, eoe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_porgress(epoch, step, d_loss, gan_loss, noise_dim = None, epoch_steps= None, generator=None, discriminator=None, n_samples=100, eoe= False):\n",
    "    if eoe and step == (epoch_steps-1):\n",
    "        # Report a full epoch training performance\n",
    "        # Sample some real images from the training set\n",
    "        X_real, y_real = generate_real_images(n_samples)\n",
    "        # Measure the accuracy of the discrinminator on real sampled images\n",
    "        _ , acc_real = discriminator.evaluate(X_real, y_real, verbose=0)\n",
    "        # Generates fake examples\n",
    "        X_fake, y_fake = generate_img_using_model(generator, noise_dim, n_samples)\n",
    "        # evaluate discriminator on fake images\n",
    "        _, acc_fake = discriminator.evaluate(X_fake, y_fake, verbose=0)\n",
    "        # summarize discriminator performance\n",
    "        # plot images\n",
    "        for i in range(10 * 10):\n",
    "            # define subplot\n",
    "            plt.subplot(10, 10, 1 + i)\n",
    "            # turn off axis\n",
    "            plt.axis('off')\n",
    "            # plot raw pixel data\n",
    "            plt.imshow(X_fake[i, :, :, 0], cmap='gray_r')\n",
    "            #plt.show()\n",
    "        filename = 'generated_examples_epoch%04d.png' % (epoch+1)\n",
    "        plt.savefig(filename)\n",
    "        print('Disciminator Accuracy on real images: %.0f%%, on fake images: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "        # save the generator model tile file\n",
    "        filename = 'generator_model_%04d.h5' % epoch\n",
    "        generator.save(filename)\n",
    "    else:\n",
    "        # Report a single step training performance \n",
    "        print('Training progress in epoch #%d, step %d, discriminator loss=%.3f , generator loss=%.3f' % (epoch, step ,d_loss, gan_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HEb3xQ0LlD6_",
    "outputId": "7045c7f6-8cc4-46e0-b411-fbdfc1d7fbbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 256 \n",
    "epoch_steps = int((2 * X_train.shape[0]/batch_size)/2)\n",
    "print(epoch_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "Training progress in epoch #0, step 0, discriminator loss=0.688 , generator loss=0.724\n",
      "Training progress in epoch #0, step 1, discriminator loss=0.679 , generator loss=0.739\n",
      "Training progress in epoch #0, step 2, discriminator loss=0.674 , generator loss=0.752\n",
      "Training progress in epoch #0, step 3, discriminator loss=0.670 , generator loss=0.761\n",
      "Training progress in epoch #0, step 4, discriminator loss=0.661 , generator loss=0.778\n",
      "Training progress in epoch #0, step 5, discriminator loss=0.656 , generator loss=0.791\n",
      "Training progress in epoch #0, step 6, discriminator loss=0.650 , generator loss=0.801\n",
      "Training progress in epoch #0, step 7, discriminator loss=0.644 , generator loss=0.813\n",
      "Training progress in epoch #0, step 8, discriminator loss=0.639 , generator loss=0.816\n",
      "Training progress in epoch #0, step 9, discriminator loss=0.637 , generator loss=0.821\n",
      "Training progress in epoch #0, step 10, discriminator loss=0.634 , generator loss=0.820\n",
      "Training progress in epoch #0, step 11, discriminator loss=0.633 , generator loss=0.813\n",
      "Training progress in epoch #0, step 12, discriminator loss=0.630 , generator loss=0.808\n",
      "Training progress in epoch #0, step 13, discriminator loss=0.630 , generator loss=0.797\n",
      "Training progress in epoch #0, step 14, discriminator loss=0.629 , generator loss=0.788\n",
      "Training progress in epoch #0, step 15, discriminator loss=0.632 , generator loss=0.773\n",
      "Training progress in epoch #0, step 16, discriminator loss=0.627 , generator loss=0.760\n",
      "Training progress in epoch #0, step 17, discriminator loss=0.625 , generator loss=0.748\n",
      "Training progress in epoch #0, step 18, discriminator loss=0.619 , generator loss=0.740\n",
      "Training progress in epoch #0, step 19, discriminator loss=0.615 , generator loss=0.732\n",
      "Training progress in epoch #0, step 20, discriminator loss=0.606 , generator loss=0.727\n",
      "Training progress in epoch #0, step 21, discriminator loss=0.606 , generator loss=0.722\n",
      "Training progress in epoch #0, step 22, discriminator loss=0.596 , generator loss=0.719\n",
      "Training progress in epoch #0, step 23, discriminator loss=0.589 , generator loss=0.715\n",
      "Training progress in epoch #0, step 24, discriminator loss=0.578 , generator loss=0.713\n",
      "Training progress in epoch #0, step 25, discriminator loss=0.570 , generator loss=0.711\n",
      "Training progress in epoch #0, step 26, discriminator loss=0.560 , generator loss=0.709\n",
      "Training progress in epoch #0, step 27, discriminator loss=0.551 , generator loss=0.707\n",
      "Training progress in epoch #0, step 28, discriminator loss=0.542 , generator loss=0.707\n",
      "Training progress in epoch #0, step 29, discriminator loss=0.530 , generator loss=0.707\n",
      "Training progress in epoch #0, step 30, discriminator loss=0.519 , generator loss=0.707\n",
      "Training progress in epoch #0, step 31, discriminator loss=0.508 , generator loss=0.707\n",
      "Training progress in epoch #0, step 32, discriminator loss=0.501 , generator loss=0.708\n",
      "Training progress in epoch #0, step 33, discriminator loss=0.489 , generator loss=0.709\n",
      "Training progress in epoch #0, step 34, discriminator loss=0.480 , generator loss=0.709\n",
      "Training progress in epoch #0, step 35, discriminator loss=0.469 , generator loss=0.711\n",
      "Training progress in epoch #0, step 36, discriminator loss=0.461 , generator loss=0.713\n",
      "Training progress in epoch #0, step 37, discriminator loss=0.452 , generator loss=0.711\n",
      "Training progress in epoch #0, step 38, discriminator loss=0.444 , generator loss=0.713\n",
      "Training progress in epoch #0, step 39, discriminator loss=0.435 , generator loss=0.717\n",
      "Training progress in epoch #0, step 40, discriminator loss=0.430 , generator loss=0.717\n",
      "Training progress in epoch #0, step 41, discriminator loss=0.426 , generator loss=0.716\n",
      "Training progress in epoch #0, step 42, discriminator loss=0.428 , generator loss=0.713\n",
      "Training progress in epoch #0, step 43, discriminator loss=0.426 , generator loss=0.709\n",
      "Training progress in epoch #0, step 44, discriminator loss=0.425 , generator loss=0.695\n",
      "Training progress in epoch #0, step 45, discriminator loss=0.430 , generator loss=0.690\n",
      "Training progress in epoch #0, step 46, discriminator loss=0.433 , generator loss=0.682\n",
      "Training progress in epoch #0, step 47, discriminator loss=0.441 , generator loss=0.665\n",
      "Training progress in epoch #0, step 48, discriminator loss=0.445 , generator loss=0.652\n",
      "Training progress in epoch #0, step 49, discriminator loss=0.476 , generator loss=0.666\n",
      "Training progress in epoch #0, step 50, discriminator loss=0.453 , generator loss=0.657\n",
      "Training progress in epoch #0, step 51, discriminator loss=0.487 , generator loss=0.638\n",
      "Training progress in epoch #0, step 52, discriminator loss=0.536 , generator loss=0.627\n",
      "Training progress in epoch #0, step 53, discriminator loss=0.535 , generator loss=0.648\n",
      "Training progress in epoch #0, step 54, discriminator loss=0.553 , generator loss=0.615\n",
      "Training progress in epoch #0, step 55, discriminator loss=0.580 , generator loss=0.588\n",
      "Training progress in epoch #0, step 56, discriminator loss=0.602 , generator loss=0.573\n",
      "Training progress in epoch #0, step 57, discriminator loss=0.628 , generator loss=0.558\n",
      "Training progress in epoch #0, step 58, discriminator loss=0.650 , generator loss=0.524\n",
      "Training progress in epoch #0, step 59, discriminator loss=0.720 , generator loss=0.472\n",
      "Training progress in epoch #0, step 60, discriminator loss=0.695 , generator loss=0.540\n",
      "Training progress in epoch #0, step 61, discriminator loss=0.595 , generator loss=0.689\n",
      "Training progress in epoch #0, step 62, discriminator loss=0.510 , generator loss=0.839\n",
      "Training progress in epoch #0, step 63, discriminator loss=0.463 , generator loss=0.972\n",
      "Training progress in epoch #0, step 64, discriminator loss=0.441 , generator loss=1.056\n",
      "Training progress in epoch #0, step 65, discriminator loss=0.419 , generator loss=1.071\n",
      "Training progress in epoch #0, step 66, discriminator loss=0.426 , generator loss=1.006\n",
      "Training progress in epoch #0, step 67, discriminator loss=0.452 , generator loss=0.918\n",
      "Training progress in epoch #0, step 68, discriminator loss=0.482 , generator loss=0.816\n",
      "Training progress in epoch #0, step 69, discriminator loss=0.511 , generator loss=0.714\n",
      "Training progress in epoch #0, step 70, discriminator loss=0.541 , generator loss=0.684\n",
      "Training progress in epoch #0, step 71, discriminator loss=0.556 , generator loss=0.662\n",
      "Training progress in epoch #0, step 72, discriminator loss=0.562 , generator loss=0.656\n",
      "Training progress in epoch #0, step 73, discriminator loss=0.561 , generator loss=0.637\n",
      "Training progress in epoch #0, step 74, discriminator loss=0.568 , generator loss=0.641\n",
      "Training progress in epoch #0, step 75, discriminator loss=0.562 , generator loss=0.646\n",
      "Training progress in epoch #0, step 76, discriminator loss=0.559 , generator loss=0.637\n",
      "Training progress in epoch #0, step 77, discriminator loss=0.573 , generator loss=0.629\n",
      "Training progress in epoch #0, step 78, discriminator loss=0.571 , generator loss=0.640\n",
      "Training progress in epoch #0, step 79, discriminator loss=0.585 , generator loss=0.656\n",
      "Training progress in epoch #0, step 80, discriminator loss=0.591 , generator loss=0.653\n",
      "Training progress in epoch #0, step 81, discriminator loss=0.575 , generator loss=0.654\n",
      "Training progress in epoch #0, step 82, discriminator loss=0.564 , generator loss=0.671\n",
      "Training progress in epoch #0, step 83, discriminator loss=0.573 , generator loss=0.666\n",
      "Training progress in epoch #0, step 84, discriminator loss=0.577 , generator loss=0.669\n",
      "Training progress in epoch #0, step 85, discriminator loss=0.564 , generator loss=0.689\n",
      "Training progress in epoch #0, step 86, discriminator loss=0.570 , generator loss=0.689\n",
      "Training progress in epoch #0, step 87, discriminator loss=0.592 , generator loss=0.701\n",
      "Training progress in epoch #0, step 88, discriminator loss=0.562 , generator loss=0.702\n",
      "Training progress in epoch #0, step 89, discriminator loss=0.575 , generator loss=0.694\n",
      "Training progress in epoch #0, step 90, discriminator loss=0.585 , generator loss=0.698\n",
      "Training progress in epoch #0, step 91, discriminator loss=0.575 , generator loss=0.716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #0, step 92, discriminator loss=0.569 , generator loss=0.724\n",
      "Training progress in epoch #0, step 93, discriminator loss=0.559 , generator loss=0.744\n",
      "Training progress in epoch #0, step 94, discriminator loss=0.574 , generator loss=0.717\n",
      "Training progress in epoch #0, step 95, discriminator loss=0.560 , generator loss=0.753\n",
      "Training progress in epoch #0, step 96, discriminator loss=0.560 , generator loss=0.781\n",
      "Training progress in epoch #0, step 97, discriminator loss=0.564 , generator loss=0.746\n",
      "Training progress in epoch #0, step 98, discriminator loss=0.557 , generator loss=0.769\n",
      "Training progress in epoch #0, step 99, discriminator loss=0.561 , generator loss=0.771\n",
      "Training progress in epoch #0, step 100, discriminator loss=0.576 , generator loss=0.783\n",
      "Training progress in epoch #0, step 101, discriminator loss=0.559 , generator loss=0.778\n",
      "Training progress in epoch #0, step 102, discriminator loss=0.550 , generator loss=0.785\n",
      "Training progress in epoch #0, step 103, discriminator loss=0.556 , generator loss=0.767\n",
      "Training progress in epoch #0, step 104, discriminator loss=0.561 , generator loss=0.794\n",
      "Training progress in epoch #0, step 105, discriminator loss=0.559 , generator loss=0.818\n",
      "Training progress in epoch #0, step 106, discriminator loss=0.556 , generator loss=0.812\n",
      "Training progress in epoch #0, step 107, discriminator loss=0.555 , generator loss=0.786\n",
      "Training progress in epoch #0, step 108, discriminator loss=0.559 , generator loss=0.824\n",
      "Training progress in epoch #0, step 109, discriminator loss=0.549 , generator loss=0.817\n",
      "Training progress in epoch #0, step 110, discriminator loss=0.565 , generator loss=0.829\n",
      "Training progress in epoch #0, step 111, discriminator loss=0.529 , generator loss=0.852\n",
      "Training progress in epoch #0, step 112, discriminator loss=0.547 , generator loss=0.834\n",
      "Training progress in epoch #0, step 113, discriminator loss=0.548 , generator loss=0.823\n",
      "Training progress in epoch #0, step 114, discriminator loss=0.550 , generator loss=0.872\n",
      "Training progress in epoch #0, step 115, discriminator loss=0.556 , generator loss=0.842\n",
      "Training progress in epoch #0, step 116, discriminator loss=0.533 , generator loss=0.847\n",
      "Training progress in epoch #0, step 117, discriminator loss=0.554 , generator loss=0.872\n",
      "Training progress in epoch #0, step 118, discriminator loss=0.548 , generator loss=0.870\n",
      "Training progress in epoch #0, step 119, discriminator loss=0.544 , generator loss=0.877\n",
      "Training progress in epoch #0, step 120, discriminator loss=0.542 , generator loss=0.893\n",
      "Training progress in epoch #0, step 121, discriminator loss=0.543 , generator loss=0.881\n",
      "Training progress in epoch #0, step 122, discriminator loss=0.546 , generator loss=0.876\n",
      "Training progress in epoch #0, step 123, discriminator loss=0.532 , generator loss=0.872\n",
      "Training progress in epoch #0, step 124, discriminator loss=0.569 , generator loss=0.909\n",
      "Training progress in epoch #0, step 125, discriminator loss=0.541 , generator loss=0.868\n",
      "Training progress in epoch #0, step 126, discriminator loss=0.556 , generator loss=0.874\n",
      "Training progress in epoch #0, step 127, discriminator loss=0.556 , generator loss=0.885\n",
      "Training progress in epoch #0, step 128, discriminator loss=0.559 , generator loss=0.906\n",
      "Training progress in epoch #0, step 129, discriminator loss=0.559 , generator loss=0.890\n",
      "Training progress in epoch #0, step 130, discriminator loss=0.545 , generator loss=0.899\n",
      "Training progress in epoch #0, step 131, discriminator loss=0.548 , generator loss=0.890\n",
      "Training progress in epoch #0, step 132, discriminator loss=0.562 , generator loss=0.887\n",
      "Training progress in epoch #0, step 133, discriminator loss=0.548 , generator loss=0.856\n",
      "Training progress in epoch #0, step 134, discriminator loss=0.550 , generator loss=0.871\n",
      "Training progress in epoch #0, step 135, discriminator loss=0.567 , generator loss=0.868\n",
      "Training progress in epoch #0, step 136, discriminator loss=0.577 , generator loss=0.896\n",
      "Training progress in epoch #0, step 137, discriminator loss=0.565 , generator loss=0.930\n",
      "Training progress in epoch #0, step 138, discriminator loss=0.568 , generator loss=0.880\n",
      "Training progress in epoch #0, step 139, discriminator loss=0.559 , generator loss=0.865\n",
      "Training progress in epoch #0, step 140, discriminator loss=0.567 , generator loss=0.895\n",
      "Training progress in epoch #0, step 141, discriminator loss=0.565 , generator loss=0.904\n",
      "Training progress in epoch #0, step 142, discriminator loss=0.557 , generator loss=0.865\n",
      "Training progress in epoch #0, step 143, discriminator loss=0.572 , generator loss=0.878\n",
      "Training progress in epoch #0, step 144, discriminator loss=0.562 , generator loss=0.885\n",
      "Training progress in epoch #0, step 145, discriminator loss=0.559 , generator loss=0.915\n",
      "Training progress in epoch #0, step 146, discriminator loss=0.581 , generator loss=0.902\n",
      "Training progress in epoch #0, step 147, discriminator loss=0.558 , generator loss=0.880\n",
      "Training progress in epoch #0, step 148, discriminator loss=0.564 , generator loss=0.894\n",
      "Training progress in epoch #0, step 149, discriminator loss=0.548 , generator loss=0.864\n",
      "Training progress in epoch #0, step 150, discriminator loss=0.571 , generator loss=0.888\n",
      "Training progress in epoch #0, step 151, discriminator loss=0.570 , generator loss=0.926\n",
      "Training progress in epoch #0, step 152, discriminator loss=0.569 , generator loss=0.870\n",
      "Training progress in epoch #0, step 153, discriminator loss=0.565 , generator loss=0.900\n",
      "Training progress in epoch #0, step 154, discriminator loss=0.559 , generator loss=0.885\n",
      "Training progress in epoch #0, step 155, discriminator loss=0.557 , generator loss=0.913\n",
      "Training progress in epoch #0, step 156, discriminator loss=0.567 , generator loss=0.887\n",
      "Training progress in epoch #0, step 157, discriminator loss=0.568 , generator loss=0.863\n",
      "Training progress in epoch #0, step 158, discriminator loss=0.585 , generator loss=0.859\n",
      "Training progress in epoch #0, step 159, discriminator loss=0.577 , generator loss=0.903\n",
      "Training progress in epoch #0, step 160, discriminator loss=0.588 , generator loss=0.864\n",
      "Training progress in epoch #0, step 161, discriminator loss=0.569 , generator loss=0.860\n",
      "Training progress in epoch #0, step 162, discriminator loss=0.596 , generator loss=0.849\n",
      "Training progress in epoch #0, step 163, discriminator loss=0.595 , generator loss=0.867\n",
      "Training progress in epoch #0, step 164, discriminator loss=0.616 , generator loss=0.870\n",
      "Training progress in epoch #0, step 165, discriminator loss=0.614 , generator loss=0.865\n",
      "Training progress in epoch #0, step 166, discriminator loss=0.606 , generator loss=0.846\n",
      "Training progress in epoch #0, step 167, discriminator loss=0.621 , generator loss=0.794\n",
      "Training progress in epoch #0, step 168, discriminator loss=0.622 , generator loss=0.797\n",
      "Training progress in epoch #0, step 169, discriminator loss=0.621 , generator loss=0.825\n",
      "Training progress in epoch #0, step 170, discriminator loss=0.638 , generator loss=0.831\n",
      "Training progress in epoch #0, step 171, discriminator loss=0.641 , generator loss=0.831\n",
      "Training progress in epoch #0, step 172, discriminator loss=0.631 , generator loss=0.848\n",
      "Training progress in epoch #0, step 173, discriminator loss=0.634 , generator loss=0.812\n",
      "Training progress in epoch #0, step 174, discriminator loss=0.628 , generator loss=0.834\n",
      "Training progress in epoch #0, step 175, discriminator loss=0.619 , generator loss=0.801\n",
      "Training progress in epoch #0, step 176, discriminator loss=0.617 , generator loss=0.825\n",
      "Training progress in epoch #0, step 177, discriminator loss=0.612 , generator loss=0.831\n",
      "Training progress in epoch #0, step 178, discriminator loss=0.593 , generator loss=0.848\n",
      "Training progress in epoch #0, step 179, discriminator loss=0.593 , generator loss=0.854\n",
      "Training progress in epoch #0, step 180, discriminator loss=0.565 , generator loss=0.840\n",
      "Training progress in epoch #0, step 181, discriminator loss=0.584 , generator loss=0.857\n",
      "Training progress in epoch #0, step 182, discriminator loss=0.573 , generator loss=0.860\n",
      "Training progress in epoch #0, step 183, discriminator loss=0.588 , generator loss=0.843\n",
      "Training progress in epoch #0, step 184, discriminator loss=0.588 , generator loss=0.830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #0, step 185, discriminator loss=0.609 , generator loss=0.836\n",
      "Training progress in epoch #0, step 186, discriminator loss=0.637 , generator loss=0.838\n",
      "Training progress in epoch #0, step 187, discriminator loss=0.641 , generator loss=0.849\n",
      "Training progress in epoch #0, step 188, discriminator loss=0.652 , generator loss=0.831\n",
      "Training progress in epoch #0, step 189, discriminator loss=0.669 , generator loss=0.809\n",
      "Training progress in epoch #0, step 190, discriminator loss=0.677 , generator loss=0.777\n",
      "Training progress in epoch #0, step 191, discriminator loss=0.692 , generator loss=0.755\n",
      "Training progress in epoch #0, step 192, discriminator loss=0.677 , generator loss=0.765\n",
      "Training progress in epoch #0, step 193, discriminator loss=0.673 , generator loss=0.759\n",
      "Training progress in epoch #0, step 194, discriminator loss=0.690 , generator loss=0.761\n",
      "Training progress in epoch #0, step 195, discriminator loss=0.681 , generator loss=0.745\n",
      "Training progress in epoch #0, step 196, discriminator loss=0.677 , generator loss=0.741\n",
      "Training progress in epoch #0, step 197, discriminator loss=0.682 , generator loss=0.744\n",
      "Training progress in epoch #0, step 198, discriminator loss=0.680 , generator loss=0.744\n",
      "Training progress in epoch #0, step 199, discriminator loss=0.693 , generator loss=0.752\n",
      "Training progress in epoch #0, step 200, discriminator loss=0.708 , generator loss=0.731\n",
      "Training progress in epoch #0, step 201, discriminator loss=0.715 , generator loss=0.726\n",
      "Training progress in epoch #0, step 202, discriminator loss=0.710 , generator loss=0.713\n",
      "Training progress in epoch #0, step 203, discriminator loss=0.737 , generator loss=0.741\n",
      "Training progress in epoch #0, step 204, discriminator loss=0.743 , generator loss=0.724\n",
      "Training progress in epoch #0, step 205, discriminator loss=0.765 , generator loss=0.722\n",
      "Training progress in epoch #0, step 206, discriminator loss=0.767 , generator loss=0.723\n",
      "Training progress in epoch #0, step 207, discriminator loss=0.776 , generator loss=0.709\n",
      "Training progress in epoch #0, step 208, discriminator loss=0.783 , generator loss=0.689\n",
      "Training progress in epoch #0, step 209, discriminator loss=0.800 , generator loss=0.694\n",
      "Training progress in epoch #0, step 210, discriminator loss=0.795 , generator loss=0.703\n",
      "Training progress in epoch #0, step 211, discriminator loss=0.791 , generator loss=0.726\n",
      "Training progress in epoch #0, step 212, discriminator loss=0.775 , generator loss=0.729\n",
      "Training progress in epoch #0, step 213, discriminator loss=0.768 , generator loss=0.728\n",
      "Training progress in epoch #0, step 214, discriminator loss=0.766 , generator loss=0.690\n",
      "Training progress in epoch #0, step 215, discriminator loss=0.775 , generator loss=0.718\n",
      "Training progress in epoch #0, step 216, discriminator loss=0.749 , generator loss=0.720\n",
      "Training progress in epoch #0, step 217, discriminator loss=0.763 , generator loss=0.741\n",
      "Training progress in epoch #0, step 218, discriminator loss=0.739 , generator loss=0.739\n",
      "Training progress in epoch #0, step 219, discriminator loss=0.733 , generator loss=0.752\n",
      "Training progress in epoch #0, step 220, discriminator loss=0.738 , generator loss=0.760\n",
      "Training progress in epoch #0, step 221, discriminator loss=0.730 , generator loss=0.774\n",
      "Training progress in epoch #0, step 222, discriminator loss=0.736 , generator loss=0.771\n",
      "Training progress in epoch #0, step 223, discriminator loss=0.716 , generator loss=0.763\n",
      "Training progress in epoch #0, step 224, discriminator loss=0.732 , generator loss=0.727\n",
      "Training progress in epoch #0, step 225, discriminator loss=0.749 , generator loss=0.718\n",
      "Training progress in epoch #0, step 226, discriminator loss=0.739 , generator loss=0.735\n",
      "Training progress in epoch #0, step 227, discriminator loss=0.745 , generator loss=0.717\n",
      "Training progress in epoch #0, step 228, discriminator loss=0.765 , generator loss=0.685\n",
      "Training progress in epoch #0, step 229, discriminator loss=0.758 , generator loss=0.671\n",
      "Training progress in epoch #0, step 230, discriminator loss=0.771 , generator loss=0.687\n",
      "Training progress in epoch #0, step 231, discriminator loss=0.764 , generator loss=0.653\n",
      "Training progress in epoch #0, step 232, discriminator loss=0.762 , generator loss=0.641\n",
      "Training progress in epoch #0, step 233, discriminator loss=0.751 , generator loss=0.666\n",
      "Disciminator Accuracy on real images: 23%, on fake images: 44%\n",
      "Training progress in epoch #1, step 0, discriminator loss=0.765 , generator loss=0.666\n",
      "Training progress in epoch #1, step 1, discriminator loss=0.754 , generator loss=0.662\n",
      "Training progress in epoch #1, step 2, discriminator loss=0.752 , generator loss=0.650\n",
      "Training progress in epoch #1, step 3, discriminator loss=0.749 , generator loss=0.641\n",
      "Training progress in epoch #1, step 4, discriminator loss=0.751 , generator loss=0.672\n",
      "Training progress in epoch #1, step 5, discriminator loss=0.739 , generator loss=0.668\n",
      "Training progress in epoch #1, step 6, discriminator loss=0.716 , generator loss=0.689\n",
      "Training progress in epoch #1, step 7, discriminator loss=0.709 , generator loss=0.707\n",
      "Training progress in epoch #1, step 8, discriminator loss=0.708 , generator loss=0.703\n",
      "Training progress in epoch #1, step 9, discriminator loss=0.700 , generator loss=0.725\n",
      "Training progress in epoch #1, step 10, discriminator loss=0.697 , generator loss=0.724\n",
      "Training progress in epoch #1, step 11, discriminator loss=0.696 , generator loss=0.744\n",
      "Training progress in epoch #1, step 12, discriminator loss=0.677 , generator loss=0.737\n",
      "Training progress in epoch #1, step 13, discriminator loss=0.680 , generator loss=0.748\n",
      "Training progress in epoch #1, step 14, discriminator loss=0.672 , generator loss=0.758\n",
      "Training progress in epoch #1, step 15, discriminator loss=0.694 , generator loss=0.752\n",
      "Training progress in epoch #1, step 16, discriminator loss=0.668 , generator loss=0.757\n",
      "Training progress in epoch #1, step 17, discriminator loss=0.677 , generator loss=0.761\n",
      "Training progress in epoch #1, step 18, discriminator loss=0.683 , generator loss=0.800\n",
      "Training progress in epoch #1, step 19, discriminator loss=0.684 , generator loss=0.788\n",
      "Training progress in epoch #1, step 20, discriminator loss=0.683 , generator loss=0.803\n",
      "Training progress in epoch #1, step 21, discriminator loss=0.675 , generator loss=0.808\n",
      "Training progress in epoch #1, step 22, discriminator loss=0.680 , generator loss=0.793\n",
      "Training progress in epoch #1, step 23, discriminator loss=0.684 , generator loss=0.799\n",
      "Training progress in epoch #1, step 24, discriminator loss=0.679 , generator loss=0.812\n",
      "Training progress in epoch #1, step 25, discriminator loss=0.682 , generator loss=0.832\n",
      "Training progress in epoch #1, step 26, discriminator loss=0.683 , generator loss=0.851\n",
      "Training progress in epoch #1, step 27, discriminator loss=0.670 , generator loss=0.824\n",
      "Training progress in epoch #1, step 28, discriminator loss=0.673 , generator loss=0.797\n",
      "Training progress in epoch #1, step 29, discriminator loss=0.681 , generator loss=0.828\n",
      "Training progress in epoch #1, step 30, discriminator loss=0.679 , generator loss=0.809\n",
      "Training progress in epoch #1, step 31, discriminator loss=0.666 , generator loss=0.840\n",
      "Training progress in epoch #1, step 32, discriminator loss=0.675 , generator loss=0.840\n",
      "Training progress in epoch #1, step 33, discriminator loss=0.657 , generator loss=0.831\n",
      "Training progress in epoch #1, step 34, discriminator loss=0.657 , generator loss=0.840\n",
      "Training progress in epoch #1, step 35, discriminator loss=0.670 , generator loss=0.843\n",
      "Training progress in epoch #1, step 36, discriminator loss=0.668 , generator loss=0.831\n",
      "Training progress in epoch #1, step 37, discriminator loss=0.658 , generator loss=0.834\n",
      "Training progress in epoch #1, step 38, discriminator loss=0.652 , generator loss=0.815\n",
      "Training progress in epoch #1, step 39, discriminator loss=0.658 , generator loss=0.806\n",
      "Training progress in epoch #1, step 40, discriminator loss=0.658 , generator loss=0.816\n",
      "Training progress in epoch #1, step 41, discriminator loss=0.652 , generator loss=0.796\n",
      "Training progress in epoch #1, step 42, discriminator loss=0.672 , generator loss=0.802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #1, step 43, discriminator loss=0.668 , generator loss=0.811\n",
      "Training progress in epoch #1, step 44, discriminator loss=0.665 , generator loss=0.797\n",
      "Training progress in epoch #1, step 45, discriminator loss=0.676 , generator loss=0.783\n",
      "Training progress in epoch #1, step 46, discriminator loss=0.681 , generator loss=0.763\n",
      "Training progress in epoch #1, step 47, discriminator loss=0.690 , generator loss=0.781\n",
      "Training progress in epoch #1, step 48, discriminator loss=0.692 , generator loss=0.764\n",
      "Training progress in epoch #1, step 49, discriminator loss=0.692 , generator loss=0.767\n",
      "Training progress in epoch #1, step 50, discriminator loss=0.687 , generator loss=0.742\n",
      "Training progress in epoch #1, step 51, discriminator loss=0.704 , generator loss=0.726\n",
      "Training progress in epoch #1, step 52, discriminator loss=0.709 , generator loss=0.687\n",
      "Training progress in epoch #1, step 53, discriminator loss=0.702 , generator loss=0.693\n",
      "Training progress in epoch #1, step 54, discriminator loss=0.714 , generator loss=0.696\n",
      "Training progress in epoch #1, step 55, discriminator loss=0.708 , generator loss=0.693\n",
      "Training progress in epoch #1, step 56, discriminator loss=0.702 , generator loss=0.695\n",
      "Training progress in epoch #1, step 57, discriminator loss=0.712 , generator loss=0.698\n",
      "Training progress in epoch #1, step 58, discriminator loss=0.720 , generator loss=0.696\n",
      "Training progress in epoch #1, step 59, discriminator loss=0.712 , generator loss=0.690\n",
      "Training progress in epoch #1, step 60, discriminator loss=0.711 , generator loss=0.698\n",
      "Training progress in epoch #1, step 61, discriminator loss=0.718 , generator loss=0.702\n",
      "Training progress in epoch #1, step 62, discriminator loss=0.722 , generator loss=0.681\n",
      "Training progress in epoch #1, step 63, discriminator loss=0.710 , generator loss=0.683\n",
      "Training progress in epoch #1, step 64, discriminator loss=0.722 , generator loss=0.690\n",
      "Training progress in epoch #1, step 65, discriminator loss=0.718 , generator loss=0.694\n",
      "Training progress in epoch #1, step 66, discriminator loss=0.714 , generator loss=0.702\n",
      "Training progress in epoch #1, step 67, discriminator loss=0.718 , generator loss=0.696\n",
      "Training progress in epoch #1, step 68, discriminator loss=0.714 , generator loss=0.685\n",
      "Training progress in epoch #1, step 69, discriminator loss=0.706 , generator loss=0.690\n",
      "Training progress in epoch #1, step 70, discriminator loss=0.699 , generator loss=0.682\n",
      "Training progress in epoch #1, step 71, discriminator loss=0.705 , generator loss=0.697\n",
      "Training progress in epoch #1, step 72, discriminator loss=0.705 , generator loss=0.694\n",
      "Training progress in epoch #1, step 73, discriminator loss=0.697 , generator loss=0.698\n",
      "Training progress in epoch #1, step 74, discriminator loss=0.703 , generator loss=0.713\n",
      "Training progress in epoch #1, step 75, discriminator loss=0.701 , generator loss=0.714\n",
      "Training progress in epoch #1, step 76, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #1, step 77, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #1, step 78, discriminator loss=0.704 , generator loss=0.703\n",
      "Training progress in epoch #1, step 79, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #1, step 80, discriminator loss=0.703 , generator loss=0.724\n",
      "Training progress in epoch #1, step 81, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #1, step 82, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #1, step 83, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #1, step 84, discriminator loss=0.697 , generator loss=0.695\n",
      "Training progress in epoch #1, step 85, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #1, step 86, discriminator loss=0.684 , generator loss=0.702\n",
      "Training progress in epoch #1, step 87, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #1, step 88, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #1, step 89, discriminator loss=0.689 , generator loss=0.737\n",
      "Training progress in epoch #1, step 90, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #1, step 91, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #1, step 92, discriminator loss=0.687 , generator loss=0.738\n",
      "Training progress in epoch #1, step 93, discriminator loss=0.699 , generator loss=0.733\n",
      "Training progress in epoch #1, step 94, discriminator loss=0.693 , generator loss=0.739\n",
      "Training progress in epoch #1, step 95, discriminator loss=0.700 , generator loss=0.724\n",
      "Training progress in epoch #1, step 96, discriminator loss=0.701 , generator loss=0.726\n",
      "Training progress in epoch #1, step 97, discriminator loss=0.696 , generator loss=0.725\n",
      "Training progress in epoch #1, step 98, discriminator loss=0.701 , generator loss=0.729\n",
      "Training progress in epoch #1, step 99, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #1, step 100, discriminator loss=0.698 , generator loss=0.733\n",
      "Training progress in epoch #1, step 101, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #1, step 102, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #1, step 103, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #1, step 104, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #1, step 105, discriminator loss=0.698 , generator loss=0.710\n",
      "Training progress in epoch #1, step 106, discriminator loss=0.703 , generator loss=0.711\n",
      "Training progress in epoch #1, step 107, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #1, step 108, discriminator loss=0.698 , generator loss=0.711\n",
      "Training progress in epoch #1, step 109, discriminator loss=0.700 , generator loss=0.706\n",
      "Training progress in epoch #1, step 110, discriminator loss=0.699 , generator loss=0.694\n",
      "Training progress in epoch #1, step 111, discriminator loss=0.701 , generator loss=0.706\n",
      "Training progress in epoch #1, step 112, discriminator loss=0.705 , generator loss=0.700\n",
      "Training progress in epoch #1, step 113, discriminator loss=0.697 , generator loss=0.707\n",
      "Training progress in epoch #1, step 114, discriminator loss=0.701 , generator loss=0.709\n",
      "Training progress in epoch #1, step 115, discriminator loss=0.706 , generator loss=0.714\n",
      "Training progress in epoch #1, step 116, discriminator loss=0.702 , generator loss=0.712\n",
      "Training progress in epoch #1, step 117, discriminator loss=0.705 , generator loss=0.706\n",
      "Training progress in epoch #1, step 118, discriminator loss=0.700 , generator loss=0.700\n",
      "Training progress in epoch #1, step 119, discriminator loss=0.707 , generator loss=0.684\n",
      "Training progress in epoch #1, step 120, discriminator loss=0.708 , generator loss=0.698\n",
      "Training progress in epoch #1, step 121, discriminator loss=0.701 , generator loss=0.688\n",
      "Training progress in epoch #1, step 122, discriminator loss=0.709 , generator loss=0.684\n",
      "Training progress in epoch #1, step 123, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #1, step 124, discriminator loss=0.702 , generator loss=0.684\n",
      "Training progress in epoch #1, step 125, discriminator loss=0.704 , generator loss=0.690\n",
      "Training progress in epoch #1, step 126, discriminator loss=0.708 , generator loss=0.700\n",
      "Training progress in epoch #1, step 127, discriminator loss=0.703 , generator loss=0.703\n",
      "Training progress in epoch #1, step 128, discriminator loss=0.699 , generator loss=0.705\n",
      "Training progress in epoch #1, step 129, discriminator loss=0.700 , generator loss=0.704\n",
      "Training progress in epoch #1, step 130, discriminator loss=0.708 , generator loss=0.714\n",
      "Training progress in epoch #1, step 131, discriminator loss=0.700 , generator loss=0.692\n",
      "Training progress in epoch #1, step 132, discriminator loss=0.706 , generator loss=0.694\n",
      "Training progress in epoch #1, step 133, discriminator loss=0.704 , generator loss=0.688\n",
      "Training progress in epoch #1, step 134, discriminator loss=0.700 , generator loss=0.682\n",
      "Training progress in epoch #1, step 135, discriminator loss=0.700 , generator loss=0.692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #1, step 136, discriminator loss=0.698 , generator loss=0.699\n",
      "Training progress in epoch #1, step 137, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #1, step 138, discriminator loss=0.699 , generator loss=0.718\n",
      "Training progress in epoch #1, step 139, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #1, step 140, discriminator loss=0.692 , generator loss=0.744\n",
      "Training progress in epoch #1, step 141, discriminator loss=0.693 , generator loss=0.752\n",
      "Training progress in epoch #1, step 142, discriminator loss=0.683 , generator loss=0.719\n",
      "Training progress in epoch #1, step 143, discriminator loss=0.682 , generator loss=0.728\n",
      "Training progress in epoch #1, step 144, discriminator loss=0.680 , generator loss=0.733\n",
      "Training progress in epoch #1, step 145, discriminator loss=0.684 , generator loss=0.721\n",
      "Training progress in epoch #1, step 146, discriminator loss=0.680 , generator loss=0.716\n",
      "Training progress in epoch #1, step 147, discriminator loss=0.677 , generator loss=0.716\n",
      "Training progress in epoch #1, step 148, discriminator loss=0.678 , generator loss=0.737\n",
      "Training progress in epoch #1, step 149, discriminator loss=0.673 , generator loss=0.714\n",
      "Training progress in epoch #1, step 150, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #1, step 151, discriminator loss=0.677 , generator loss=0.752\n",
      "Training progress in epoch #1, step 152, discriminator loss=0.671 , generator loss=0.747\n",
      "Training progress in epoch #1, step 153, discriminator loss=0.664 , generator loss=0.745\n",
      "Training progress in epoch #1, step 154, discriminator loss=0.673 , generator loss=0.740\n",
      "Training progress in epoch #1, step 155, discriminator loss=0.668 , generator loss=0.744\n",
      "Training progress in epoch #1, step 156, discriminator loss=0.666 , generator loss=0.731\n",
      "Training progress in epoch #1, step 157, discriminator loss=0.656 , generator loss=0.748\n",
      "Training progress in epoch #1, step 158, discriminator loss=0.666 , generator loss=0.755\n",
      "Training progress in epoch #1, step 159, discriminator loss=0.660 , generator loss=0.767\n",
      "Training progress in epoch #1, step 160, discriminator loss=0.660 , generator loss=0.756\n",
      "Training progress in epoch #1, step 161, discriminator loss=0.658 , generator loss=0.778\n",
      "Training progress in epoch #1, step 162, discriminator loss=0.657 , generator loss=0.757\n",
      "Training progress in epoch #1, step 163, discriminator loss=0.673 , generator loss=0.757\n",
      "Training progress in epoch #1, step 164, discriminator loss=0.670 , generator loss=0.739\n",
      "Training progress in epoch #1, step 165, discriminator loss=0.667 , generator loss=0.741\n",
      "Training progress in epoch #1, step 166, discriminator loss=0.679 , generator loss=0.740\n",
      "Training progress in epoch #1, step 167, discriminator loss=0.669 , generator loss=0.749\n",
      "Training progress in epoch #1, step 168, discriminator loss=0.677 , generator loss=0.720\n",
      "Training progress in epoch #1, step 169, discriminator loss=0.665 , generator loss=0.756\n",
      "Training progress in epoch #1, step 170, discriminator loss=0.676 , generator loss=0.733\n",
      "Training progress in epoch #1, step 171, discriminator loss=0.676 , generator loss=0.730\n",
      "Training progress in epoch #1, step 172, discriminator loss=0.676 , generator loss=0.750\n",
      "Training progress in epoch #1, step 173, discriminator loss=0.690 , generator loss=0.751\n",
      "Training progress in epoch #1, step 174, discriminator loss=0.681 , generator loss=0.733\n",
      "Training progress in epoch #1, step 175, discriminator loss=0.690 , generator loss=0.751\n",
      "Training progress in epoch #1, step 176, discriminator loss=0.683 , generator loss=0.737\n",
      "Training progress in epoch #1, step 177, discriminator loss=0.688 , generator loss=0.738\n",
      "Training progress in epoch #1, step 178, discriminator loss=0.699 , generator loss=0.716\n",
      "Training progress in epoch #1, step 179, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #1, step 180, discriminator loss=0.692 , generator loss=0.728\n",
      "Training progress in epoch #1, step 181, discriminator loss=0.698 , generator loss=0.724\n",
      "Training progress in epoch #1, step 182, discriminator loss=0.697 , generator loss=0.713\n",
      "Training progress in epoch #1, step 183, discriminator loss=0.703 , generator loss=0.710\n",
      "Training progress in epoch #1, step 184, discriminator loss=0.701 , generator loss=0.705\n",
      "Training progress in epoch #1, step 185, discriminator loss=0.700 , generator loss=0.730\n",
      "Training progress in epoch #1, step 186, discriminator loss=0.698 , generator loss=0.734\n",
      "Training progress in epoch #1, step 187, discriminator loss=0.709 , generator loss=0.720\n",
      "Training progress in epoch #1, step 188, discriminator loss=0.705 , generator loss=0.733\n",
      "Training progress in epoch #1, step 189, discriminator loss=0.701 , generator loss=0.709\n",
      "Training progress in epoch #1, step 190, discriminator loss=0.701 , generator loss=0.719\n",
      "Training progress in epoch #1, step 191, discriminator loss=0.705 , generator loss=0.714\n",
      "Training progress in epoch #1, step 192, discriminator loss=0.705 , generator loss=0.709\n",
      "Training progress in epoch #1, step 193, discriminator loss=0.704 , generator loss=0.697\n",
      "Training progress in epoch #1, step 194, discriminator loss=0.702 , generator loss=0.704\n",
      "Training progress in epoch #1, step 195, discriminator loss=0.701 , generator loss=0.704\n",
      "Training progress in epoch #1, step 196, discriminator loss=0.704 , generator loss=0.710\n",
      "Training progress in epoch #1, step 197, discriminator loss=0.699 , generator loss=0.709\n",
      "Training progress in epoch #1, step 198, discriminator loss=0.704 , generator loss=0.709\n",
      "Training progress in epoch #1, step 199, discriminator loss=0.705 , generator loss=0.712\n",
      "Training progress in epoch #1, step 200, discriminator loss=0.700 , generator loss=0.714\n",
      "Training progress in epoch #1, step 201, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #1, step 202, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #1, step 203, discriminator loss=0.699 , generator loss=0.707\n",
      "Training progress in epoch #1, step 204, discriminator loss=0.698 , generator loss=0.705\n",
      "Training progress in epoch #1, step 205, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #1, step 206, discriminator loss=0.696 , generator loss=0.706\n",
      "Training progress in epoch #1, step 207, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #1, step 208, discriminator loss=0.684 , generator loss=0.714\n",
      "Training progress in epoch #1, step 209, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #1, step 210, discriminator loss=0.686 , generator loss=0.717\n",
      "Training progress in epoch #1, step 211, discriminator loss=0.679 , generator loss=0.722\n",
      "Training progress in epoch #1, step 212, discriminator loss=0.683 , generator loss=0.714\n",
      "Training progress in epoch #1, step 213, discriminator loss=0.684 , generator loss=0.734\n",
      "Training progress in epoch #1, step 214, discriminator loss=0.680 , generator loss=0.738\n",
      "Training progress in epoch #1, step 215, discriminator loss=0.673 , generator loss=0.737\n",
      "Training progress in epoch #1, step 216, discriminator loss=0.671 , generator loss=0.731\n",
      "Training progress in epoch #1, step 217, discriminator loss=0.672 , generator loss=0.741\n",
      "Training progress in epoch #1, step 218, discriminator loss=0.680 , generator loss=0.727\n",
      "Training progress in epoch #1, step 219, discriminator loss=0.681 , generator loss=0.734\n",
      "Training progress in epoch #1, step 220, discriminator loss=0.670 , generator loss=0.725\n",
      "Training progress in epoch #1, step 221, discriminator loss=0.678 , generator loss=0.725\n",
      "Training progress in epoch #1, step 222, discriminator loss=0.682 , generator loss=0.736\n",
      "Training progress in epoch #1, step 223, discriminator loss=0.678 , generator loss=0.745\n",
      "Training progress in epoch #1, step 224, discriminator loss=0.675 , generator loss=0.742\n",
      "Training progress in epoch #1, step 225, discriminator loss=0.673 , generator loss=0.746\n",
      "Training progress in epoch #1, step 226, discriminator loss=0.678 , generator loss=0.752\n",
      "Training progress in epoch #1, step 227, discriminator loss=0.681 , generator loss=0.749\n",
      "Training progress in epoch #1, step 228, discriminator loss=0.674 , generator loss=0.744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #1, step 229, discriminator loss=0.674 , generator loss=0.740\n",
      "Training progress in epoch #1, step 230, discriminator loss=0.683 , generator loss=0.726\n",
      "Training progress in epoch #1, step 231, discriminator loss=0.684 , generator loss=0.732\n",
      "Training progress in epoch #1, step 232, discriminator loss=0.677 , generator loss=0.743\n",
      "Training progress in epoch #1, step 233, discriminator loss=0.685 , generator loss=0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedalhamid/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disciminator Accuracy on real images: 41%, on fake images: 73%\n",
      "Training progress in epoch #2, step 0, discriminator loss=0.682 , generator loss=0.728\n",
      "Training progress in epoch #2, step 1, discriminator loss=0.684 , generator loss=0.726\n",
      "Training progress in epoch #2, step 2, discriminator loss=0.679 , generator loss=0.732\n",
      "Training progress in epoch #2, step 3, discriminator loss=0.682 , generator loss=0.734\n",
      "Training progress in epoch #2, step 4, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #2, step 5, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #2, step 6, discriminator loss=0.681 , generator loss=0.722\n",
      "Training progress in epoch #2, step 7, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #2, step 8, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #2, step 9, discriminator loss=0.697 , generator loss=0.719\n",
      "Training progress in epoch #2, step 10, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #2, step 11, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #2, step 12, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #2, step 13, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #2, step 14, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #2, step 15, discriminator loss=0.706 , generator loss=0.720\n",
      "Training progress in epoch #2, step 16, discriminator loss=0.696 , generator loss=0.723\n",
      "Training progress in epoch #2, step 17, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #2, step 18, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #2, step 19, discriminator loss=0.701 , generator loss=0.719\n",
      "Training progress in epoch #2, step 20, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #2, step 21, discriminator loss=0.696 , generator loss=0.721\n",
      "Training progress in epoch #2, step 22, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #2, step 23, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #2, step 24, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #2, step 25, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #2, step 26, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #2, step 27, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #2, step 28, discriminator loss=0.698 , generator loss=0.725\n",
      "Training progress in epoch #2, step 29, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #2, step 30, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #2, step 31, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #2, step 32, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #2, step 33, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #2, step 34, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #2, step 35, discriminator loss=0.689 , generator loss=0.727\n",
      "Training progress in epoch #2, step 36, discriminator loss=0.679 , generator loss=0.741\n",
      "Training progress in epoch #2, step 37, discriminator loss=0.679 , generator loss=0.734\n",
      "Training progress in epoch #2, step 38, discriminator loss=0.684 , generator loss=0.723\n",
      "Training progress in epoch #2, step 39, discriminator loss=0.676 , generator loss=0.733\n",
      "Training progress in epoch #2, step 40, discriminator loss=0.676 , generator loss=0.741\n",
      "Training progress in epoch #2, step 41, discriminator loss=0.685 , generator loss=0.736\n",
      "Training progress in epoch #2, step 42, discriminator loss=0.680 , generator loss=0.726\n",
      "Training progress in epoch #2, step 43, discriminator loss=0.677 , generator loss=0.747\n",
      "Training progress in epoch #2, step 44, discriminator loss=0.666 , generator loss=0.725\n",
      "Training progress in epoch #2, step 45, discriminator loss=0.668 , generator loss=0.742\n",
      "Training progress in epoch #2, step 46, discriminator loss=0.670 , generator loss=0.753\n",
      "Training progress in epoch #2, step 47, discriminator loss=0.668 , generator loss=0.751\n",
      "Training progress in epoch #2, step 48, discriminator loss=0.659 , generator loss=0.747\n",
      "Training progress in epoch #2, step 49, discriminator loss=0.675 , generator loss=0.743\n",
      "Training progress in epoch #2, step 50, discriminator loss=0.670 , generator loss=0.742\n",
      "Training progress in epoch #2, step 51, discriminator loss=0.670 , generator loss=0.747\n",
      "Training progress in epoch #2, step 52, discriminator loss=0.664 , generator loss=0.752\n",
      "Training progress in epoch #2, step 53, discriminator loss=0.675 , generator loss=0.740\n",
      "Training progress in epoch #2, step 54, discriminator loss=0.661 , generator loss=0.747\n",
      "Training progress in epoch #2, step 55, discriminator loss=0.672 , generator loss=0.749\n",
      "Training progress in epoch #2, step 56, discriminator loss=0.667 , generator loss=0.734\n",
      "Training progress in epoch #2, step 57, discriminator loss=0.674 , generator loss=0.738\n",
      "Training progress in epoch #2, step 58, discriminator loss=0.666 , generator loss=0.722\n",
      "Training progress in epoch #2, step 59, discriminator loss=0.675 , generator loss=0.728\n",
      "Training progress in epoch #2, step 60, discriminator loss=0.675 , generator loss=0.740\n",
      "Training progress in epoch #2, step 61, discriminator loss=0.673 , generator loss=0.709\n",
      "Training progress in epoch #2, step 62, discriminator loss=0.676 , generator loss=0.716\n",
      "Training progress in epoch #2, step 63, discriminator loss=0.678 , generator loss=0.720\n",
      "Training progress in epoch #2, step 64, discriminator loss=0.682 , generator loss=0.709\n",
      "Training progress in epoch #2, step 65, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #2, step 66, discriminator loss=0.680 , generator loss=0.718\n",
      "Training progress in epoch #2, step 67, discriminator loss=0.676 , generator loss=0.731\n",
      "Training progress in epoch #2, step 68, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #2, step 69, discriminator loss=0.681 , generator loss=0.737\n",
      "Training progress in epoch #2, step 70, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #2, step 71, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #2, step 72, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #2, step 73, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #2, step 74, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #2, step 75, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #2, step 76, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #2, step 77, discriminator loss=0.684 , generator loss=0.726\n",
      "Training progress in epoch #2, step 78, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #2, step 79, discriminator loss=0.698 , generator loss=0.705\n",
      "Training progress in epoch #2, step 80, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #2, step 81, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #2, step 82, discriminator loss=0.684 , generator loss=0.716\n",
      "Training progress in epoch #2, step 83, discriminator loss=0.683 , generator loss=0.728\n",
      "Training progress in epoch #2, step 84, discriminator loss=0.686 , generator loss=0.734\n",
      "Training progress in epoch #2, step 85, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #2, step 86, discriminator loss=0.685 , generator loss=0.732\n",
      "Training progress in epoch #2, step 87, discriminator loss=0.683 , generator loss=0.734\n",
      "Training progress in epoch #2, step 88, discriminator loss=0.681 , generator loss=0.723\n",
      "Training progress in epoch #2, step 89, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #2, step 90, discriminator loss=0.680 , generator loss=0.709\n",
      "Training progress in epoch #2, step 91, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #2, step 92, discriminator loss=0.680 , generator loss=0.714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #2, step 93, discriminator loss=0.677 , generator loss=0.727\n",
      "Training progress in epoch #2, step 94, discriminator loss=0.674 , generator loss=0.716\n",
      "Training progress in epoch #2, step 95, discriminator loss=0.676 , generator loss=0.719\n",
      "Training progress in epoch #2, step 96, discriminator loss=0.680 , generator loss=0.728\n",
      "Training progress in epoch #2, step 97, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #2, step 98, discriminator loss=0.674 , generator loss=0.719\n",
      "Training progress in epoch #2, step 99, discriminator loss=0.677 , generator loss=0.741\n",
      "Training progress in epoch #2, step 100, discriminator loss=0.679 , generator loss=0.730\n",
      "Training progress in epoch #2, step 101, discriminator loss=0.686 , generator loss=0.730\n",
      "Training progress in epoch #2, step 102, discriminator loss=0.677 , generator loss=0.728\n",
      "Training progress in epoch #2, step 103, discriminator loss=0.682 , generator loss=0.722\n",
      "Training progress in epoch #2, step 104, discriminator loss=0.680 , generator loss=0.727\n",
      "Training progress in epoch #2, step 105, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #2, step 106, discriminator loss=0.689 , generator loss=0.733\n",
      "Training progress in epoch #2, step 107, discriminator loss=0.683 , generator loss=0.723\n",
      "Training progress in epoch #2, step 108, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #2, step 109, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #2, step 110, discriminator loss=0.679 , generator loss=0.716\n",
      "Training progress in epoch #2, step 111, discriminator loss=0.683 , generator loss=0.721\n",
      "Training progress in epoch #2, step 112, discriminator loss=0.698 , generator loss=0.724\n",
      "Training progress in epoch #2, step 113, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #2, step 114, discriminator loss=0.698 , generator loss=0.716\n",
      "Training progress in epoch #2, step 115, discriminator loss=0.699 , generator loss=0.715\n",
      "Training progress in epoch #2, step 116, discriminator loss=0.700 , generator loss=0.704\n",
      "Training progress in epoch #2, step 117, discriminator loss=0.706 , generator loss=0.692\n",
      "Training progress in epoch #2, step 118, discriminator loss=0.695 , generator loss=0.676\n",
      "Training progress in epoch #2, step 119, discriminator loss=0.703 , generator loss=0.686\n",
      "Training progress in epoch #2, step 120, discriminator loss=0.702 , generator loss=0.675\n",
      "Training progress in epoch #2, step 121, discriminator loss=0.711 , generator loss=0.688\n",
      "Training progress in epoch #2, step 122, discriminator loss=0.713 , generator loss=0.682\n",
      "Training progress in epoch #2, step 123, discriminator loss=0.714 , generator loss=0.678\n",
      "Training progress in epoch #2, step 124, discriminator loss=0.711 , generator loss=0.694\n",
      "Training progress in epoch #2, step 125, discriminator loss=0.715 , generator loss=0.696\n",
      "Training progress in epoch #2, step 126, discriminator loss=0.715 , generator loss=0.684\n",
      "Training progress in epoch #2, step 127, discriminator loss=0.722 , generator loss=0.671\n",
      "Training progress in epoch #2, step 128, discriminator loss=0.717 , generator loss=0.688\n",
      "Training progress in epoch #2, step 129, discriminator loss=0.711 , generator loss=0.678\n",
      "Training progress in epoch #2, step 130, discriminator loss=0.714 , generator loss=0.692\n",
      "Training progress in epoch #2, step 131, discriminator loss=0.718 , generator loss=0.677\n",
      "Training progress in epoch #2, step 132, discriminator loss=0.718 , generator loss=0.681\n",
      "Training progress in epoch #2, step 133, discriminator loss=0.719 , generator loss=0.674\n",
      "Training progress in epoch #2, step 134, discriminator loss=0.711 , generator loss=0.681\n",
      "Training progress in epoch #2, step 135, discriminator loss=0.719 , generator loss=0.685\n",
      "Training progress in epoch #2, step 136, discriminator loss=0.710 , generator loss=0.692\n",
      "Training progress in epoch #2, step 137, discriminator loss=0.709 , generator loss=0.682\n",
      "Training progress in epoch #2, step 138, discriminator loss=0.711 , generator loss=0.683\n",
      "Training progress in epoch #2, step 139, discriminator loss=0.707 , generator loss=0.702\n",
      "Training progress in epoch #2, step 140, discriminator loss=0.704 , generator loss=0.692\n",
      "Training progress in epoch #2, step 141, discriminator loss=0.707 , generator loss=0.706\n",
      "Training progress in epoch #2, step 142, discriminator loss=0.701 , generator loss=0.703\n",
      "Training progress in epoch #2, step 143, discriminator loss=0.707 , generator loss=0.702\n",
      "Training progress in epoch #2, step 144, discriminator loss=0.696 , generator loss=0.707\n",
      "Training progress in epoch #2, step 145, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #2, step 146, discriminator loss=0.697 , generator loss=0.702\n",
      "Training progress in epoch #2, step 147, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #2, step 148, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #2, step 149, discriminator loss=0.685 , generator loss=0.729\n",
      "Training progress in epoch #2, step 150, discriminator loss=0.681 , generator loss=0.744\n",
      "Training progress in epoch #2, step 151, discriminator loss=0.686 , generator loss=0.739\n",
      "Training progress in epoch #2, step 152, discriminator loss=0.680 , generator loss=0.731\n",
      "Training progress in epoch #2, step 153, discriminator loss=0.682 , generator loss=0.721\n",
      "Training progress in epoch #2, step 154, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #2, step 155, discriminator loss=0.683 , generator loss=0.726\n",
      "Training progress in epoch #2, step 156, discriminator loss=0.683 , generator loss=0.732\n",
      "Training progress in epoch #2, step 157, discriminator loss=0.679 , generator loss=0.732\n",
      "Training progress in epoch #2, step 158, discriminator loss=0.685 , generator loss=0.726\n",
      "Training progress in epoch #2, step 159, discriminator loss=0.674 , generator loss=0.726\n",
      "Training progress in epoch #2, step 160, discriminator loss=0.678 , generator loss=0.726\n",
      "Training progress in epoch #2, step 161, discriminator loss=0.683 , generator loss=0.733\n",
      "Training progress in epoch #2, step 162, discriminator loss=0.683 , generator loss=0.727\n",
      "Training progress in epoch #2, step 163, discriminator loss=0.684 , generator loss=0.724\n",
      "Training progress in epoch #2, step 164, discriminator loss=0.682 , generator loss=0.727\n",
      "Training progress in epoch #2, step 165, discriminator loss=0.676 , generator loss=0.727\n",
      "Training progress in epoch #2, step 166, discriminator loss=0.676 , generator loss=0.723\n",
      "Training progress in epoch #2, step 167, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #2, step 168, discriminator loss=0.678 , generator loss=0.716\n",
      "Training progress in epoch #2, step 169, discriminator loss=0.684 , generator loss=0.722\n",
      "Training progress in epoch #2, step 170, discriminator loss=0.682 , generator loss=0.728\n",
      "Training progress in epoch #2, step 171, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #2, step 172, discriminator loss=0.675 , generator loss=0.741\n",
      "Training progress in epoch #2, step 173, discriminator loss=0.674 , generator loss=0.735\n",
      "Training progress in epoch #2, step 174, discriminator loss=0.687 , generator loss=0.740\n",
      "Training progress in epoch #2, step 175, discriminator loss=0.678 , generator loss=0.736\n",
      "Training progress in epoch #2, step 176, discriminator loss=0.683 , generator loss=0.742\n",
      "Training progress in epoch #2, step 177, discriminator loss=0.682 , generator loss=0.740\n",
      "Training progress in epoch #2, step 178, discriminator loss=0.677 , generator loss=0.738\n",
      "Training progress in epoch #2, step 179, discriminator loss=0.679 , generator loss=0.732\n",
      "Training progress in epoch #2, step 180, discriminator loss=0.680 , generator loss=0.741\n",
      "Training progress in epoch #2, step 181, discriminator loss=0.683 , generator loss=0.736\n",
      "Training progress in epoch #2, step 182, discriminator loss=0.678 , generator loss=0.751\n",
      "Training progress in epoch #2, step 183, discriminator loss=0.687 , generator loss=0.744\n",
      "Training progress in epoch #2, step 184, discriminator loss=0.684 , generator loss=0.745\n",
      "Training progress in epoch #2, step 185, discriminator loss=0.688 , generator loss=0.749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #2, step 186, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #2, step 187, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #2, step 188, discriminator loss=0.682 , generator loss=0.710\n",
      "Training progress in epoch #2, step 189, discriminator loss=0.684 , generator loss=0.701\n",
      "Training progress in epoch #2, step 190, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #2, step 191, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #2, step 192, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #2, step 193, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #2, step 194, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #2, step 195, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #2, step 196, discriminator loss=0.683 , generator loss=0.696\n",
      "Training progress in epoch #2, step 197, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #2, step 198, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #2, step 199, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #2, step 200, discriminator loss=0.701 , generator loss=0.719\n",
      "Training progress in epoch #2, step 201, discriminator loss=0.699 , generator loss=0.707\n",
      "Training progress in epoch #2, step 202, discriminator loss=0.697 , generator loss=0.710\n",
      "Training progress in epoch #2, step 203, discriminator loss=0.702 , generator loss=0.706\n",
      "Training progress in epoch #2, step 204, discriminator loss=0.708 , generator loss=0.714\n",
      "Training progress in epoch #2, step 205, discriminator loss=0.702 , generator loss=0.697\n",
      "Training progress in epoch #2, step 206, discriminator loss=0.706 , generator loss=0.709\n",
      "Training progress in epoch #2, step 207, discriminator loss=0.702 , generator loss=0.707\n",
      "Training progress in epoch #2, step 208, discriminator loss=0.704 , generator loss=0.698\n",
      "Training progress in epoch #2, step 209, discriminator loss=0.701 , generator loss=0.689\n",
      "Training progress in epoch #2, step 210, discriminator loss=0.709 , generator loss=0.687\n",
      "Training progress in epoch #2, step 211, discriminator loss=0.705 , generator loss=0.687\n",
      "Training progress in epoch #2, step 212, discriminator loss=0.697 , generator loss=0.680\n",
      "Training progress in epoch #2, step 213, discriminator loss=0.706 , generator loss=0.680\n",
      "Training progress in epoch #2, step 214, discriminator loss=0.700 , generator loss=0.685\n",
      "Training progress in epoch #2, step 215, discriminator loss=0.699 , generator loss=0.678\n",
      "Training progress in epoch #2, step 216, discriminator loss=0.700 , generator loss=0.675\n",
      "Training progress in epoch #2, step 217, discriminator loss=0.701 , generator loss=0.672\n",
      "Training progress in epoch #2, step 218, discriminator loss=0.703 , generator loss=0.691\n",
      "Training progress in epoch #2, step 219, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #2, step 220, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #2, step 221, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #2, step 222, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #2, step 223, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #2, step 224, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #2, step 225, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #2, step 226, discriminator loss=0.684 , generator loss=0.714\n",
      "Training progress in epoch #2, step 227, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #2, step 228, discriminator loss=0.677 , generator loss=0.717\n",
      "Training progress in epoch #2, step 229, discriminator loss=0.678 , generator loss=0.716\n",
      "Training progress in epoch #2, step 230, discriminator loss=0.682 , generator loss=0.731\n",
      "Training progress in epoch #2, step 231, discriminator loss=0.683 , generator loss=0.732\n",
      "Training progress in epoch #2, step 232, discriminator loss=0.676 , generator loss=0.746\n",
      "Training progress in epoch #2, step 233, discriminator loss=0.668 , generator loss=0.737\n",
      "Disciminator Accuracy on real images: 63%, on fake images: 61%\n",
      "Training progress in epoch #3, step 0, discriminator loss=0.671 , generator loss=0.746\n",
      "Training progress in epoch #3, step 1, discriminator loss=0.672 , generator loss=0.730\n",
      "Training progress in epoch #3, step 2, discriminator loss=0.675 , generator loss=0.728\n",
      "Training progress in epoch #3, step 3, discriminator loss=0.680 , generator loss=0.742\n",
      "Training progress in epoch #3, step 4, discriminator loss=0.671 , generator loss=0.737\n",
      "Training progress in epoch #3, step 5, discriminator loss=0.678 , generator loss=0.725\n",
      "Training progress in epoch #3, step 6, discriminator loss=0.673 , generator loss=0.715\n",
      "Training progress in epoch #3, step 7, discriminator loss=0.673 , generator loss=0.715\n",
      "Training progress in epoch #3, step 8, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #3, step 9, discriminator loss=0.681 , generator loss=0.709\n",
      "Training progress in epoch #3, step 10, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #3, step 11, discriminator loss=0.680 , generator loss=0.709\n",
      "Training progress in epoch #3, step 12, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #3, step 13, discriminator loss=0.683 , generator loss=0.702\n",
      "Training progress in epoch #3, step 14, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #3, step 15, discriminator loss=0.683 , generator loss=0.682\n",
      "Training progress in epoch #3, step 16, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #3, step 17, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #3, step 18, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #3, step 19, discriminator loss=0.683 , generator loss=0.695\n",
      "Training progress in epoch #3, step 20, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #3, step 21, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #3, step 22, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #3, step 23, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #3, step 24, discriminator loss=0.692 , generator loss=0.728\n",
      "Training progress in epoch #3, step 25, discriminator loss=0.697 , generator loss=0.711\n",
      "Training progress in epoch #3, step 26, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #3, step 27, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #3, step 28, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #3, step 29, discriminator loss=0.700 , generator loss=0.713\n",
      "Training progress in epoch #3, step 30, discriminator loss=0.699 , generator loss=0.714\n",
      "Training progress in epoch #3, step 31, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #3, step 32, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #3, step 33, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #3, step 34, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #3, step 35, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #3, step 36, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #3, step 37, discriminator loss=0.701 , generator loss=0.702\n",
      "Training progress in epoch #3, step 38, discriminator loss=0.701 , generator loss=0.715\n",
      "Training progress in epoch #3, step 39, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #3, step 40, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #3, step 41, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #3, step 42, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #3, step 43, discriminator loss=0.703 , generator loss=0.684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #3, step 44, discriminator loss=0.704 , generator loss=0.684\n",
      "Training progress in epoch #3, step 45, discriminator loss=0.707 , generator loss=0.675\n",
      "Training progress in epoch #3, step 46, discriminator loss=0.705 , generator loss=0.669\n",
      "Training progress in epoch #3, step 47, discriminator loss=0.700 , generator loss=0.665\n",
      "Training progress in epoch #3, step 48, discriminator loss=0.699 , generator loss=0.665\n",
      "Training progress in epoch #3, step 49, discriminator loss=0.705 , generator loss=0.669\n",
      "Training progress in epoch #3, step 50, discriminator loss=0.702 , generator loss=0.678\n",
      "Training progress in epoch #3, step 51, discriminator loss=0.698 , generator loss=0.681\n",
      "Training progress in epoch #3, step 52, discriminator loss=0.701 , generator loss=0.681\n",
      "Training progress in epoch #3, step 53, discriminator loss=0.700 , generator loss=0.683\n",
      "Training progress in epoch #3, step 54, discriminator loss=0.708 , generator loss=0.685\n",
      "Training progress in epoch #3, step 55, discriminator loss=0.702 , generator loss=0.692\n",
      "Training progress in epoch #3, step 56, discriminator loss=0.702 , generator loss=0.703\n",
      "Training progress in epoch #3, step 57, discriminator loss=0.698 , generator loss=0.704\n",
      "Training progress in epoch #3, step 58, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #3, step 59, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #3, step 60, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #3, step 61, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #3, step 62, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #3, step 63, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #3, step 64, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #3, step 65, discriminator loss=0.683 , generator loss=0.708\n",
      "Training progress in epoch #3, step 66, discriminator loss=0.677 , generator loss=0.711\n",
      "Training progress in epoch #3, step 67, discriminator loss=0.680 , generator loss=0.718\n",
      "Training progress in epoch #3, step 68, discriminator loss=0.672 , generator loss=0.725\n",
      "Training progress in epoch #3, step 69, discriminator loss=0.676 , generator loss=0.734\n",
      "Training progress in epoch #3, step 70, discriminator loss=0.677 , generator loss=0.731\n",
      "Training progress in epoch #3, step 71, discriminator loss=0.686 , generator loss=0.730\n",
      "Training progress in epoch #3, step 72, discriminator loss=0.679 , generator loss=0.729\n",
      "Training progress in epoch #3, step 73, discriminator loss=0.677 , generator loss=0.720\n",
      "Training progress in epoch #3, step 74, discriminator loss=0.678 , generator loss=0.711\n",
      "Training progress in epoch #3, step 75, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #3, step 76, discriminator loss=0.684 , generator loss=0.710\n",
      "Training progress in epoch #3, step 77, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #3, step 78, discriminator loss=0.684 , generator loss=0.700\n",
      "Training progress in epoch #3, step 79, discriminator loss=0.682 , generator loss=0.714\n",
      "Training progress in epoch #3, step 80, discriminator loss=0.685 , generator loss=0.711\n",
      "Training progress in epoch #3, step 81, discriminator loss=0.680 , generator loss=0.705\n",
      "Training progress in epoch #3, step 82, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #3, step 83, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #3, step 84, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #3, step 85, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #3, step 86, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #3, step 87, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #3, step 88, discriminator loss=0.701 , generator loss=0.687\n",
      "Training progress in epoch #3, step 89, discriminator loss=0.702 , generator loss=0.676\n",
      "Training progress in epoch #3, step 90, discriminator loss=0.697 , generator loss=0.682\n",
      "Training progress in epoch #3, step 91, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #3, step 92, discriminator loss=0.704 , generator loss=0.696\n",
      "Training progress in epoch #3, step 93, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #3, step 94, discriminator loss=0.698 , generator loss=0.697\n",
      "Training progress in epoch #3, step 95, discriminator loss=0.697 , generator loss=0.698\n",
      "Training progress in epoch #3, step 96, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #3, step 97, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #3, step 98, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #3, step 99, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #3, step 100, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #3, step 101, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #3, step 102, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #3, step 103, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #3, step 104, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #3, step 105, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #3, step 106, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #3, step 107, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #3, step 108, discriminator loss=0.682 , generator loss=0.704\n",
      "Training progress in epoch #3, step 109, discriminator loss=0.678 , generator loss=0.699\n",
      "Training progress in epoch #3, step 110, discriminator loss=0.683 , generator loss=0.703\n",
      "Training progress in epoch #3, step 111, discriminator loss=0.679 , generator loss=0.707\n",
      "Training progress in epoch #3, step 112, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #3, step 113, discriminator loss=0.683 , generator loss=0.701\n",
      "Training progress in epoch #3, step 114, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #3, step 115, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #3, step 116, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #3, step 117, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #3, step 118, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #3, step 119, discriminator loss=0.697 , generator loss=0.701\n",
      "Training progress in epoch #3, step 120, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #3, step 121, discriminator loss=0.699 , generator loss=0.692\n",
      "Training progress in epoch #3, step 122, discriminator loss=0.700 , generator loss=0.689\n",
      "Training progress in epoch #3, step 123, discriminator loss=0.692 , generator loss=0.675\n",
      "Training progress in epoch #3, step 124, discriminator loss=0.699 , generator loss=0.664\n",
      "Training progress in epoch #3, step 125, discriminator loss=0.697 , generator loss=0.672\n",
      "Training progress in epoch #3, step 126, discriminator loss=0.694 , generator loss=0.673\n",
      "Training progress in epoch #3, step 127, discriminator loss=0.692 , generator loss=0.670\n",
      "Training progress in epoch #3, step 128, discriminator loss=0.696 , generator loss=0.684\n",
      "Training progress in epoch #3, step 129, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #3, step 130, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #3, step 131, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #3, step 132, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #3, step 133, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #3, step 134, discriminator loss=0.685 , generator loss=0.701\n",
      "Training progress in epoch #3, step 135, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #3, step 136, discriminator loss=0.689 , generator loss=0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #3, step 137, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #3, step 138, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #3, step 139, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #3, step 140, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #3, step 141, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #3, step 142, discriminator loss=0.683 , generator loss=0.710\n",
      "Training progress in epoch #3, step 143, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #3, step 144, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #3, step 145, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #3, step 146, discriminator loss=0.681 , generator loss=0.711\n",
      "Training progress in epoch #3, step 147, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #3, step 148, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #3, step 149, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #3, step 150, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #3, step 151, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #3, step 152, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #3, step 153, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #3, step 154, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #3, step 155, discriminator loss=0.697 , generator loss=0.703\n",
      "Training progress in epoch #3, step 156, discriminator loss=0.698 , generator loss=0.704\n",
      "Training progress in epoch #3, step 157, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #3, step 158, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #3, step 159, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #3, step 160, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #3, step 161, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #3, step 162, discriminator loss=0.698 , generator loss=0.698\n",
      "Training progress in epoch #3, step 163, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #3, step 164, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #3, step 165, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #3, step 166, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #3, step 167, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #3, step 168, discriminator loss=0.698 , generator loss=0.685\n",
      "Training progress in epoch #3, step 169, discriminator loss=0.695 , generator loss=0.673\n",
      "Training progress in epoch #3, step 170, discriminator loss=0.690 , generator loss=0.675\n",
      "Training progress in epoch #3, step 171, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #3, step 172, discriminator loss=0.701 , generator loss=0.678\n",
      "Training progress in epoch #3, step 173, discriminator loss=0.696 , generator loss=0.682\n",
      "Training progress in epoch #3, step 174, discriminator loss=0.700 , generator loss=0.688\n",
      "Training progress in epoch #3, step 175, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #3, step 176, discriminator loss=0.698 , generator loss=0.693\n",
      "Training progress in epoch #3, step 177, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #3, step 178, discriminator loss=0.697 , generator loss=0.684\n",
      "Training progress in epoch #3, step 179, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #3, step 180, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #3, step 181, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #3, step 182, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #3, step 183, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #3, step 184, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #3, step 185, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #3, step 186, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #3, step 187, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #3, step 188, discriminator loss=0.682 , generator loss=0.692\n",
      "Training progress in epoch #3, step 189, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #3, step 190, discriminator loss=0.681 , generator loss=0.696\n",
      "Training progress in epoch #3, step 191, discriminator loss=0.684 , generator loss=0.695\n",
      "Training progress in epoch #3, step 192, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #3, step 193, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #3, step 194, discriminator loss=0.682 , generator loss=0.718\n",
      "Training progress in epoch #3, step 195, discriminator loss=0.683 , generator loss=0.722\n",
      "Training progress in epoch #3, step 196, discriminator loss=0.686 , generator loss=0.736\n",
      "Training progress in epoch #3, step 197, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #3, step 198, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #3, step 199, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #3, step 200, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #3, step 201, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #3, step 202, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #3, step 203, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #3, step 204, discriminator loss=0.685 , generator loss=0.686\n",
      "Training progress in epoch #3, step 205, discriminator loss=0.683 , generator loss=0.694\n",
      "Training progress in epoch #3, step 206, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #3, step 207, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #3, step 208, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #3, step 209, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #3, step 210, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #3, step 211, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #3, step 212, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #3, step 213, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #3, step 214, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #3, step 215, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #3, step 216, discriminator loss=0.684 , generator loss=0.689\n",
      "Training progress in epoch #3, step 217, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #3, step 218, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #3, step 219, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #3, step 220, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #3, step 221, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #3, step 222, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #3, step 223, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #3, step 224, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #3, step 225, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #3, step 226, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #3, step 227, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #3, step 228, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #3, step 229, discriminator loss=0.692 , generator loss=0.688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #3, step 230, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #3, step 231, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #3, step 232, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #3, step 233, discriminator loss=0.691 , generator loss=0.694\n",
      "Disciminator Accuracy on real images: 67%, on fake images: 59%\n",
      "Training progress in epoch #4, step 0, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #4, step 1, discriminator loss=0.683 , generator loss=0.701\n",
      "Training progress in epoch #4, step 2, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #4, step 3, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #4, step 4, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #4, step 5, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #4, step 6, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #4, step 7, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #4, step 8, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #4, step 9, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #4, step 10, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #4, step 11, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #4, step 12, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #4, step 13, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #4, step 14, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #4, step 15, discriminator loss=0.684 , generator loss=0.703\n",
      "Training progress in epoch #4, step 16, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #4, step 17, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #4, step 18, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #4, step 19, discriminator loss=0.684 , generator loss=0.695\n",
      "Training progress in epoch #4, step 20, discriminator loss=0.682 , generator loss=0.688\n",
      "Training progress in epoch #4, step 21, discriminator loss=0.686 , generator loss=0.681\n",
      "Training progress in epoch #4, step 22, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #4, step 23, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #4, step 24, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #4, step 25, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #4, step 26, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #4, step 27, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #4, step 28, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #4, step 29, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #4, step 30, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #4, step 31, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #4, step 32, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #4, step 33, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #4, step 34, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #4, step 35, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #4, step 36, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #4, step 37, discriminator loss=0.684 , generator loss=0.712\n",
      "Training progress in epoch #4, step 38, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #4, step 39, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #4, step 40, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #4, step 41, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #4, step 42, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #4, step 43, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #4, step 44, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #4, step 45, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #4, step 46, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #4, step 47, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #4, step 48, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #4, step 49, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #4, step 50, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #4, step 51, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #4, step 52, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #4, step 53, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #4, step 54, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #4, step 55, discriminator loss=0.683 , generator loss=0.711\n",
      "Training progress in epoch #4, step 56, discriminator loss=0.683 , generator loss=0.707\n",
      "Training progress in epoch #4, step 57, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #4, step 58, discriminator loss=0.684 , generator loss=0.713\n",
      "Training progress in epoch #4, step 59, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #4, step 60, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #4, step 61, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #4, step 62, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #4, step 63, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #4, step 64, discriminator loss=0.683 , generator loss=0.699\n",
      "Training progress in epoch #4, step 65, discriminator loss=0.685 , generator loss=0.698\n",
      "Training progress in epoch #4, step 66, discriminator loss=0.684 , generator loss=0.692\n",
      "Training progress in epoch #4, step 67, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #4, step 68, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #4, step 69, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #4, step 70, discriminator loss=0.692 , generator loss=0.732\n",
      "Training progress in epoch #4, step 71, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #4, step 72, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #4, step 73, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #4, step 74, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #4, step 75, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #4, step 76, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #4, step 77, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #4, step 78, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #4, step 79, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #4, step 80, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #4, step 81, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #4, step 82, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #4, step 83, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #4, step 84, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #4, step 85, discriminator loss=0.682 , generator loss=0.718\n",
      "Training progress in epoch #4, step 86, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #4, step 87, discriminator loss=0.682 , generator loss=0.730\n",
      "Training progress in epoch #4, step 88, discriminator loss=0.682 , generator loss=0.722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #4, step 89, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #4, step 90, discriminator loss=0.683 , generator loss=0.710\n",
      "Training progress in epoch #4, step 91, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #4, step 92, discriminator loss=0.683 , generator loss=0.695\n",
      "Training progress in epoch #4, step 93, discriminator loss=0.682 , generator loss=0.696\n",
      "Training progress in epoch #4, step 94, discriminator loss=0.683 , generator loss=0.697\n",
      "Training progress in epoch #4, step 95, discriminator loss=0.683 , generator loss=0.701\n",
      "Training progress in epoch #4, step 96, discriminator loss=0.679 , generator loss=0.714\n",
      "Training progress in epoch #4, step 97, discriminator loss=0.682 , generator loss=0.717\n",
      "Training progress in epoch #4, step 98, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #4, step 99, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #4, step 100, discriminator loss=0.683 , generator loss=0.703\n",
      "Training progress in epoch #4, step 101, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #4, step 102, discriminator loss=0.684 , generator loss=0.698\n",
      "Training progress in epoch #4, step 103, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #4, step 104, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #4, step 105, discriminator loss=0.681 , generator loss=0.707\n",
      "Training progress in epoch #4, step 106, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #4, step 107, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #4, step 108, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #4, step 109, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #4, step 110, discriminator loss=0.685 , generator loss=0.715\n",
      "Training progress in epoch #4, step 111, discriminator loss=0.684 , generator loss=0.709\n",
      "Training progress in epoch #4, step 112, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #4, step 113, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #4, step 114, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #4, step 115, discriminator loss=0.682 , generator loss=0.716\n",
      "Training progress in epoch #4, step 116, discriminator loss=0.681 , generator loss=0.711\n",
      "Training progress in epoch #4, step 117, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #4, step 118, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #4, step 119, discriminator loss=0.684 , generator loss=0.694\n",
      "Training progress in epoch #4, step 120, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #4, step 121, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #4, step 122, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #4, step 123, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #4, step 124, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #4, step 125, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #4, step 126, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #4, step 127, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #4, step 128, discriminator loss=0.684 , generator loss=0.727\n",
      "Training progress in epoch #4, step 129, discriminator loss=0.688 , generator loss=0.731\n",
      "Training progress in epoch #4, step 130, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #4, step 131, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #4, step 132, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #4, step 133, discriminator loss=0.681 , generator loss=0.701\n",
      "Training progress in epoch #4, step 134, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #4, step 135, discriminator loss=0.683 , generator loss=0.701\n",
      "Training progress in epoch #4, step 136, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #4, step 137, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #4, step 138, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #4, step 139, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #4, step 140, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #4, step 141, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #4, step 142, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #4, step 143, discriminator loss=0.682 , generator loss=0.714\n",
      "Training progress in epoch #4, step 144, discriminator loss=0.679 , generator loss=0.711\n",
      "Training progress in epoch #4, step 145, discriminator loss=0.680 , generator loss=0.714\n",
      "Training progress in epoch #4, step 146, discriminator loss=0.681 , generator loss=0.724\n",
      "Training progress in epoch #4, step 147, discriminator loss=0.678 , generator loss=0.737\n",
      "Training progress in epoch #4, step 148, discriminator loss=0.677 , generator loss=0.727\n",
      "Training progress in epoch #4, step 149, discriminator loss=0.678 , generator loss=0.726\n",
      "Training progress in epoch #4, step 150, discriminator loss=0.680 , generator loss=0.721\n",
      "Training progress in epoch #4, step 151, discriminator loss=0.678 , generator loss=0.720\n",
      "Training progress in epoch #4, step 152, discriminator loss=0.676 , generator loss=0.722\n",
      "Training progress in epoch #4, step 153, discriminator loss=0.678 , generator loss=0.715\n",
      "Training progress in epoch #4, step 154, discriminator loss=0.682 , generator loss=0.717\n",
      "Training progress in epoch #4, step 155, discriminator loss=0.674 , generator loss=0.695\n",
      "Training progress in epoch #4, step 156, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #4, step 157, discriminator loss=0.683 , generator loss=0.696\n",
      "Training progress in epoch #4, step 158, discriminator loss=0.682 , generator loss=0.707\n",
      "Training progress in epoch #4, step 159, discriminator loss=0.677 , generator loss=0.704\n",
      "Training progress in epoch #4, step 160, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #4, step 161, discriminator loss=0.685 , generator loss=0.709\n",
      "Training progress in epoch #4, step 162, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #4, step 163, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #4, step 164, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #4, step 165, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #4, step 166, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #4, step 167, discriminator loss=0.690 , generator loss=0.727\n",
      "Training progress in epoch #4, step 168, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #4, step 169, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #4, step 170, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #4, step 171, discriminator loss=0.684 , generator loss=0.702\n",
      "Training progress in epoch #4, step 172, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #4, step 173, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #4, step 174, discriminator loss=0.681 , generator loss=0.704\n",
      "Training progress in epoch #4, step 175, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #4, step 176, discriminator loss=0.679 , generator loss=0.710\n",
      "Training progress in epoch #4, step 177, discriminator loss=0.675 , generator loss=0.721\n",
      "Training progress in epoch #4, step 178, discriminator loss=0.677 , generator loss=0.718\n",
      "Training progress in epoch #4, step 179, discriminator loss=0.679 , generator loss=0.720\n",
      "Training progress in epoch #4, step 180, discriminator loss=0.682 , generator loss=0.718\n",
      "Training progress in epoch #4, step 181, discriminator loss=0.684 , generator loss=0.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #4, step 182, discriminator loss=0.684 , generator loss=0.689\n",
      "Training progress in epoch #4, step 183, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #4, step 184, discriminator loss=0.684 , generator loss=0.691\n",
      "Training progress in epoch #4, step 185, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #4, step 186, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #4, step 187, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #4, step 188, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #4, step 189, discriminator loss=0.698 , generator loss=0.702\n",
      "Training progress in epoch #4, step 190, discriminator loss=0.697 , generator loss=0.696\n",
      "Training progress in epoch #4, step 191, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #4, step 192, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #4, step 193, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #4, step 194, discriminator loss=0.697 , generator loss=0.692\n",
      "Training progress in epoch #4, step 195, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #4, step 196, discriminator loss=0.698 , generator loss=0.698\n",
      "Training progress in epoch #4, step 197, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #4, step 198, discriminator loss=0.697 , generator loss=0.701\n",
      "Training progress in epoch #4, step 199, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #4, step 200, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #4, step 201, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #4, step 202, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #4, step 203, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #4, step 204, discriminator loss=0.682 , generator loss=0.699\n",
      "Training progress in epoch #4, step 205, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #4, step 206, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #4, step 207, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #4, step 208, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #4, step 209, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #4, step 210, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #4, step 211, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #4, step 212, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #4, step 213, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #4, step 214, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #4, step 215, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #4, step 216, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #4, step 217, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #4, step 218, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #4, step 219, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #4, step 220, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #4, step 221, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #4, step 222, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #4, step 223, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #4, step 224, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #4, step 225, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #4, step 226, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #4, step 227, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #4, step 228, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #4, step 229, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #4, step 230, discriminator loss=0.682 , generator loss=0.726\n",
      "Training progress in epoch #4, step 231, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #4, step 232, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #4, step 233, discriminator loss=0.683 , generator loss=0.718\n",
      "Disciminator Accuracy on real images: 63%, on fake images: 71%\n",
      "Training progress in epoch #5, step 0, discriminator loss=0.683 , generator loss=0.729\n",
      "Training progress in epoch #5, step 1, discriminator loss=0.677 , generator loss=0.740\n",
      "Training progress in epoch #5, step 2, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #5, step 3, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #5, step 4, discriminator loss=0.676 , generator loss=0.709\n",
      "Training progress in epoch #5, step 5, discriminator loss=0.680 , generator loss=0.699\n",
      "Training progress in epoch #5, step 6, discriminator loss=0.678 , generator loss=0.701\n",
      "Training progress in epoch #5, step 7, discriminator loss=0.676 , generator loss=0.696\n",
      "Training progress in epoch #5, step 8, discriminator loss=0.677 , generator loss=0.689\n",
      "Training progress in epoch #5, step 9, discriminator loss=0.671 , generator loss=0.691\n",
      "Training progress in epoch #5, step 10, discriminator loss=0.674 , generator loss=0.699\n",
      "Training progress in epoch #5, step 11, discriminator loss=0.682 , generator loss=0.705\n",
      "Training progress in epoch #5, step 12, discriminator loss=0.674 , generator loss=0.726\n",
      "Training progress in epoch #5, step 13, discriminator loss=0.680 , generator loss=0.731\n",
      "Training progress in epoch #5, step 14, discriminator loss=0.673 , generator loss=0.726\n",
      "Training progress in epoch #5, step 15, discriminator loss=0.670 , generator loss=0.725\n",
      "Training progress in epoch #5, step 16, discriminator loss=0.679 , generator loss=0.738\n",
      "Training progress in epoch #5, step 17, discriminator loss=0.675 , generator loss=0.743\n",
      "Training progress in epoch #5, step 18, discriminator loss=0.680 , generator loss=0.735\n",
      "Training progress in epoch #5, step 19, discriminator loss=0.678 , generator loss=0.737\n",
      "Training progress in epoch #5, step 20, discriminator loss=0.684 , generator loss=0.742\n",
      "Training progress in epoch #5, step 21, discriminator loss=0.688 , generator loss=0.737\n",
      "Training progress in epoch #5, step 22, discriminator loss=0.682 , generator loss=0.711\n",
      "Training progress in epoch #5, step 23, discriminator loss=0.681 , generator loss=0.705\n",
      "Training progress in epoch #5, step 24, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #5, step 25, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #5, step 26, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #5, step 27, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #5, step 28, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #5, step 29, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #5, step 30, discriminator loss=0.685 , generator loss=0.710\n",
      "Training progress in epoch #5, step 31, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #5, step 32, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #5, step 33, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #5, step 34, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #5, step 35, discriminator loss=0.678 , generator loss=0.740\n",
      "Training progress in epoch #5, step 36, discriminator loss=0.687 , generator loss=0.743\n",
      "Training progress in epoch #5, step 37, discriminator loss=0.682 , generator loss=0.746\n",
      "Training progress in epoch #5, step 38, discriminator loss=0.675 , generator loss=0.759\n",
      "Training progress in epoch #5, step 39, discriminator loss=0.671 , generator loss=0.746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #5, step 40, discriminator loss=0.677 , generator loss=0.742\n",
      "Training progress in epoch #5, step 41, discriminator loss=0.674 , generator loss=0.733\n",
      "Training progress in epoch #5, step 42, discriminator loss=0.675 , generator loss=0.734\n",
      "Training progress in epoch #5, step 43, discriminator loss=0.670 , generator loss=0.717\n",
      "Training progress in epoch #5, step 44, discriminator loss=0.671 , generator loss=0.719\n",
      "Training progress in epoch #5, step 45, discriminator loss=0.672 , generator loss=0.714\n",
      "Training progress in epoch #5, step 46, discriminator loss=0.674 , generator loss=0.712\n",
      "Training progress in epoch #5, step 47, discriminator loss=0.670 , generator loss=0.711\n",
      "Training progress in epoch #5, step 48, discriminator loss=0.673 , generator loss=0.697\n",
      "Training progress in epoch #5, step 49, discriminator loss=0.675 , generator loss=0.703\n",
      "Training progress in epoch #5, step 50, discriminator loss=0.679 , generator loss=0.689\n",
      "Training progress in epoch #5, step 51, discriminator loss=0.683 , generator loss=0.684\n",
      "Training progress in epoch #5, step 52, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #5, step 53, discriminator loss=0.680 , generator loss=0.692\n",
      "Training progress in epoch #5, step 54, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #5, step 55, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #5, step 56, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #5, step 57, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #5, step 58, discriminator loss=0.700 , generator loss=0.725\n",
      "Training progress in epoch #5, step 59, discriminator loss=0.692 , generator loss=0.732\n",
      "Training progress in epoch #5, step 60, discriminator loss=0.696 , generator loss=0.733\n",
      "Training progress in epoch #5, step 61, discriminator loss=0.693 , generator loss=0.729\n",
      "Training progress in epoch #5, step 62, discriminator loss=0.685 , generator loss=0.721\n",
      "Training progress in epoch #5, step 63, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #5, step 64, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #5, step 65, discriminator loss=0.684 , generator loss=0.692\n",
      "Training progress in epoch #5, step 66, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #5, step 67, discriminator loss=0.681 , generator loss=0.693\n",
      "Training progress in epoch #5, step 68, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #5, step 69, discriminator loss=0.675 , generator loss=0.701\n",
      "Training progress in epoch #5, step 70, discriminator loss=0.672 , generator loss=0.709\n",
      "Training progress in epoch #5, step 71, discriminator loss=0.668 , generator loss=0.723\n",
      "Training progress in epoch #5, step 72, discriminator loss=0.669 , generator loss=0.711\n",
      "Training progress in epoch #5, step 73, discriminator loss=0.674 , generator loss=0.715\n",
      "Training progress in epoch #5, step 74, discriminator loss=0.675 , generator loss=0.735\n",
      "Training progress in epoch #5, step 75, discriminator loss=0.680 , generator loss=0.742\n",
      "Training progress in epoch #5, step 76, discriminator loss=0.674 , generator loss=0.765\n",
      "Training progress in epoch #5, step 77, discriminator loss=0.672 , generator loss=0.781\n",
      "Training progress in epoch #5, step 78, discriminator loss=0.679 , generator loss=0.757\n",
      "Training progress in epoch #5, step 79, discriminator loss=0.678 , generator loss=0.749\n",
      "Training progress in epoch #5, step 80, discriminator loss=0.676 , generator loss=0.758\n",
      "Training progress in epoch #5, step 81, discriminator loss=0.682 , generator loss=0.743\n",
      "Training progress in epoch #5, step 82, discriminator loss=0.687 , generator loss=0.744\n",
      "Training progress in epoch #5, step 83, discriminator loss=0.683 , generator loss=0.732\n",
      "Training progress in epoch #5, step 84, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #5, step 85, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #5, step 86, discriminator loss=0.698 , generator loss=0.695\n",
      "Training progress in epoch #5, step 87, discriminator loss=0.700 , generator loss=0.679\n",
      "Training progress in epoch #5, step 88, discriminator loss=0.698 , generator loss=0.673\n",
      "Training progress in epoch #5, step 89, discriminator loss=0.695 , generator loss=0.677\n",
      "Training progress in epoch #5, step 90, discriminator loss=0.699 , generator loss=0.683\n",
      "Training progress in epoch #5, step 91, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #5, step 92, discriminator loss=0.700 , generator loss=0.702\n",
      "Training progress in epoch #5, step 93, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #5, step 94, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #5, step 95, discriminator loss=0.681 , generator loss=0.713\n",
      "Training progress in epoch #5, step 96, discriminator loss=0.685 , generator loss=0.709\n",
      "Training progress in epoch #5, step 97, discriminator loss=0.677 , generator loss=0.726\n",
      "Training progress in epoch #5, step 98, discriminator loss=0.675 , generator loss=0.748\n",
      "Training progress in epoch #5, step 99, discriminator loss=0.673 , generator loss=0.752\n",
      "Training progress in epoch #5, step 100, discriminator loss=0.672 , generator loss=0.754\n",
      "Training progress in epoch #5, step 101, discriminator loss=0.667 , generator loss=0.759\n",
      "Training progress in epoch #5, step 102, discriminator loss=0.663 , generator loss=0.748\n",
      "Training progress in epoch #5, step 103, discriminator loss=0.670 , generator loss=0.742\n",
      "Training progress in epoch #5, step 104, discriminator loss=0.669 , generator loss=0.750\n",
      "Training progress in epoch #5, step 105, discriminator loss=0.665 , generator loss=0.740\n",
      "Training progress in epoch #5, step 106, discriminator loss=0.673 , generator loss=0.725\n",
      "Training progress in epoch #5, step 107, discriminator loss=0.675 , generator loss=0.707\n",
      "Training progress in epoch #5, step 108, discriminator loss=0.667 , generator loss=0.708\n",
      "Training progress in epoch #5, step 109, discriminator loss=0.670 , generator loss=0.694\n",
      "Training progress in epoch #5, step 110, discriminator loss=0.673 , generator loss=0.697\n",
      "Training progress in epoch #5, step 111, discriminator loss=0.677 , generator loss=0.703\n",
      "Training progress in epoch #5, step 112, discriminator loss=0.679 , generator loss=0.710\n",
      "Training progress in epoch #5, step 113, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #5, step 114, discriminator loss=0.679 , generator loss=0.712\n",
      "Training progress in epoch #5, step 115, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #5, step 116, discriminator loss=0.700 , generator loss=0.706\n",
      "Training progress in epoch #5, step 117, discriminator loss=0.701 , generator loss=0.715\n",
      "Training progress in epoch #5, step 118, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #5, step 119, discriminator loss=0.703 , generator loss=0.699\n",
      "Training progress in epoch #5, step 120, discriminator loss=0.703 , generator loss=0.701\n",
      "Training progress in epoch #5, step 121, discriminator loss=0.705 , generator loss=0.725\n",
      "Training progress in epoch #5, step 122, discriminator loss=0.700 , generator loss=0.734\n",
      "Training progress in epoch #5, step 123, discriminator loss=0.704 , generator loss=0.720\n",
      "Training progress in epoch #5, step 124, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #5, step 125, discriminator loss=0.698 , generator loss=0.710\n",
      "Training progress in epoch #5, step 126, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #5, step 127, discriminator loss=0.685 , generator loss=0.718\n",
      "Training progress in epoch #5, step 128, discriminator loss=0.674 , generator loss=0.703\n",
      "Training progress in epoch #5, step 129, discriminator loss=0.665 , generator loss=0.709\n",
      "Training progress in epoch #5, step 130, discriminator loss=0.663 , generator loss=0.710\n",
      "Training progress in epoch #5, step 131, discriminator loss=0.665 , generator loss=0.712\n",
      "Training progress in epoch #5, step 132, discriminator loss=0.664 , generator loss=0.726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #5, step 133, discriminator loss=0.659 , generator loss=0.743\n",
      "Training progress in epoch #5, step 134, discriminator loss=0.662 , generator loss=0.738\n",
      "Training progress in epoch #5, step 135, discriminator loss=0.657 , generator loss=0.742\n",
      "Training progress in epoch #5, step 136, discriminator loss=0.659 , generator loss=0.748\n",
      "Training progress in epoch #5, step 137, discriminator loss=0.663 , generator loss=0.733\n",
      "Training progress in epoch #5, step 138, discriminator loss=0.673 , generator loss=0.733\n",
      "Training progress in epoch #5, step 139, discriminator loss=0.674 , generator loss=0.739\n",
      "Training progress in epoch #5, step 140, discriminator loss=0.685 , generator loss=0.728\n",
      "Training progress in epoch #5, step 141, discriminator loss=0.678 , generator loss=0.735\n",
      "Training progress in epoch #5, step 142, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #5, step 143, discriminator loss=0.700 , generator loss=0.694\n",
      "Training progress in epoch #5, step 144, discriminator loss=0.703 , generator loss=0.688\n",
      "Training progress in epoch #5, step 145, discriminator loss=0.707 , generator loss=0.688\n",
      "Training progress in epoch #5, step 146, discriminator loss=0.712 , generator loss=0.693\n",
      "Training progress in epoch #5, step 147, discriminator loss=0.707 , generator loss=0.702\n",
      "Training progress in epoch #5, step 148, discriminator loss=0.713 , generator loss=0.687\n",
      "Training progress in epoch #5, step 149, discriminator loss=0.706 , generator loss=0.685\n",
      "Training progress in epoch #5, step 150, discriminator loss=0.706 , generator loss=0.693\n",
      "Training progress in epoch #5, step 151, discriminator loss=0.703 , generator loss=0.699\n",
      "Training progress in epoch #5, step 152, discriminator loss=0.702 , generator loss=0.704\n",
      "Training progress in epoch #5, step 153, discriminator loss=0.697 , generator loss=0.714\n",
      "Training progress in epoch #5, step 154, discriminator loss=0.685 , generator loss=0.732\n",
      "Training progress in epoch #5, step 155, discriminator loss=0.686 , generator loss=0.734\n",
      "Training progress in epoch #5, step 156, discriminator loss=0.672 , generator loss=0.751\n",
      "Training progress in epoch #5, step 157, discriminator loss=0.669 , generator loss=0.764\n",
      "Training progress in epoch #5, step 158, discriminator loss=0.671 , generator loss=0.758\n",
      "Training progress in epoch #5, step 159, discriminator loss=0.665 , generator loss=0.763\n",
      "Training progress in epoch #5, step 160, discriminator loss=0.663 , generator loss=0.753\n",
      "Training progress in epoch #5, step 161, discriminator loss=0.664 , generator loss=0.754\n",
      "Training progress in epoch #5, step 162, discriminator loss=0.665 , generator loss=0.748\n",
      "Training progress in epoch #5, step 163, discriminator loss=0.667 , generator loss=0.743\n",
      "Training progress in epoch #5, step 164, discriminator loss=0.661 , generator loss=0.739\n",
      "Training progress in epoch #5, step 165, discriminator loss=0.665 , generator loss=0.737\n",
      "Training progress in epoch #5, step 166, discriminator loss=0.667 , generator loss=0.729\n",
      "Training progress in epoch #5, step 167, discriminator loss=0.677 , generator loss=0.715\n",
      "Training progress in epoch #5, step 168, discriminator loss=0.680 , generator loss=0.695\n",
      "Training progress in epoch #5, step 169, discriminator loss=0.685 , generator loss=0.677\n",
      "Training progress in epoch #5, step 170, discriminator loss=0.691 , generator loss=0.660\n",
      "Training progress in epoch #5, step 171, discriminator loss=0.692 , generator loss=0.668\n",
      "Training progress in epoch #5, step 172, discriminator loss=0.703 , generator loss=0.666\n",
      "Training progress in epoch #5, step 173, discriminator loss=0.700 , generator loss=0.664\n",
      "Training progress in epoch #5, step 174, discriminator loss=0.707 , generator loss=0.677\n",
      "Training progress in epoch #5, step 175, discriminator loss=0.709 , generator loss=0.694\n",
      "Training progress in epoch #5, step 176, discriminator loss=0.706 , generator loss=0.711\n",
      "Training progress in epoch #5, step 177, discriminator loss=0.704 , generator loss=0.720\n",
      "Training progress in epoch #5, step 178, discriminator loss=0.706 , generator loss=0.718\n",
      "Training progress in epoch #5, step 179, discriminator loss=0.704 , generator loss=0.714\n",
      "Training progress in epoch #5, step 180, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #5, step 181, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #5, step 182, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #5, step 183, discriminator loss=0.680 , generator loss=0.729\n",
      "Training progress in epoch #5, step 184, discriminator loss=0.671 , generator loss=0.728\n",
      "Training progress in epoch #5, step 185, discriminator loss=0.672 , generator loss=0.733\n",
      "Training progress in epoch #5, step 186, discriminator loss=0.676 , generator loss=0.713\n",
      "Training progress in epoch #5, step 187, discriminator loss=0.668 , generator loss=0.721\n",
      "Training progress in epoch #5, step 188, discriminator loss=0.670 , generator loss=0.720\n",
      "Training progress in epoch #5, step 189, discriminator loss=0.661 , generator loss=0.721\n",
      "Training progress in epoch #5, step 190, discriminator loss=0.667 , generator loss=0.732\n",
      "Training progress in epoch #5, step 191, discriminator loss=0.666 , generator loss=0.729\n",
      "Training progress in epoch #5, step 192, discriminator loss=0.666 , generator loss=0.713\n",
      "Training progress in epoch #5, step 193, discriminator loss=0.669 , generator loss=0.711\n",
      "Training progress in epoch #5, step 194, discriminator loss=0.671 , generator loss=0.713\n",
      "Training progress in epoch #5, step 195, discriminator loss=0.668 , generator loss=0.720\n",
      "Training progress in epoch #5, step 196, discriminator loss=0.677 , generator loss=0.712\n",
      "Training progress in epoch #5, step 197, discriminator loss=0.681 , generator loss=0.725\n",
      "Training progress in epoch #5, step 198, discriminator loss=0.678 , generator loss=0.739\n",
      "Training progress in epoch #5, step 199, discriminator loss=0.676 , generator loss=0.742\n",
      "Training progress in epoch #5, step 200, discriminator loss=0.677 , generator loss=0.732\n",
      "Training progress in epoch #5, step 201, discriminator loss=0.685 , generator loss=0.733\n",
      "Training progress in epoch #5, step 202, discriminator loss=0.681 , generator loss=0.731\n",
      "Training progress in epoch #5, step 203, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #5, step 204, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #5, step 205, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #5, step 206, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #5, step 207, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #5, step 208, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #5, step 209, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #5, step 210, discriminator loss=0.678 , generator loss=0.700\n",
      "Training progress in epoch #5, step 211, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #5, step 212, discriminator loss=0.685 , generator loss=0.723\n",
      "Training progress in epoch #5, step 213, discriminator loss=0.681 , generator loss=0.710\n",
      "Training progress in epoch #5, step 214, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #5, step 215, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #5, step 216, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #5, step 217, discriminator loss=0.680 , generator loss=0.725\n",
      "Training progress in epoch #5, step 218, discriminator loss=0.675 , generator loss=0.734\n",
      "Training progress in epoch #5, step 219, discriminator loss=0.676 , generator loss=0.729\n",
      "Training progress in epoch #5, step 220, discriminator loss=0.668 , generator loss=0.736\n",
      "Training progress in epoch #5, step 221, discriminator loss=0.677 , generator loss=0.743\n",
      "Training progress in epoch #5, step 222, discriminator loss=0.676 , generator loss=0.747\n",
      "Training progress in epoch #5, step 223, discriminator loss=0.674 , generator loss=0.751\n",
      "Training progress in epoch #5, step 224, discriminator loss=0.670 , generator loss=0.742\n",
      "Training progress in epoch #5, step 225, discriminator loss=0.674 , generator loss=0.731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #5, step 226, discriminator loss=0.673 , generator loss=0.739\n",
      "Training progress in epoch #5, step 227, discriminator loss=0.673 , generator loss=0.733\n",
      "Training progress in epoch #5, step 228, discriminator loss=0.678 , generator loss=0.735\n",
      "Training progress in epoch #5, step 229, discriminator loss=0.677 , generator loss=0.735\n",
      "Training progress in epoch #5, step 230, discriminator loss=0.674 , generator loss=0.722\n",
      "Training progress in epoch #5, step 231, discriminator loss=0.675 , generator loss=0.722\n",
      "Training progress in epoch #5, step 232, discriminator loss=0.666 , generator loss=0.713\n",
      "Training progress in epoch #5, step 233, discriminator loss=0.670 , generator loss=0.697\n",
      "Disciminator Accuracy on real images: 70%, on fake images: 50%\n",
      "Training progress in epoch #6, step 0, discriminator loss=0.674 , generator loss=0.692\n",
      "Training progress in epoch #6, step 1, discriminator loss=0.680 , generator loss=0.698\n",
      "Training progress in epoch #6, step 2, discriminator loss=0.674 , generator loss=0.711\n",
      "Training progress in epoch #6, step 3, discriminator loss=0.682 , generator loss=0.710\n",
      "Training progress in epoch #6, step 4, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #6, step 5, discriminator loss=0.683 , generator loss=0.729\n",
      "Training progress in epoch #6, step 6, discriminator loss=0.686 , generator loss=0.729\n",
      "Training progress in epoch #6, step 7, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #6, step 8, discriminator loss=0.695 , generator loss=0.728\n",
      "Training progress in epoch #6, step 9, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #6, step 10, discriminator loss=0.694 , generator loss=0.730\n",
      "Training progress in epoch #6, step 11, discriminator loss=0.699 , generator loss=0.715\n",
      "Training progress in epoch #6, step 12, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #6, step 13, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #6, step 14, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #6, step 15, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #6, step 16, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #6, step 17, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #6, step 18, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #6, step 19, discriminator loss=0.679 , generator loss=0.713\n",
      "Training progress in epoch #6, step 20, discriminator loss=0.677 , generator loss=0.731\n",
      "Training progress in epoch #6, step 21, discriminator loss=0.677 , generator loss=0.732\n",
      "Training progress in epoch #6, step 22, discriminator loss=0.673 , generator loss=0.740\n",
      "Training progress in epoch #6, step 23, discriminator loss=0.666 , generator loss=0.728\n",
      "Training progress in epoch #6, step 24, discriminator loss=0.667 , generator loss=0.759\n",
      "Training progress in epoch #6, step 25, discriminator loss=0.662 , generator loss=0.767\n",
      "Training progress in epoch #6, step 26, discriminator loss=0.669 , generator loss=0.763\n",
      "Training progress in epoch #6, step 27, discriminator loss=0.662 , generator loss=0.749\n",
      "Training progress in epoch #6, step 28, discriminator loss=0.668 , generator loss=0.764\n",
      "Training progress in epoch #6, step 29, discriminator loss=0.670 , generator loss=0.751\n",
      "Training progress in epoch #6, step 30, discriminator loss=0.672 , generator loss=0.737\n",
      "Training progress in epoch #6, step 31, discriminator loss=0.682 , generator loss=0.747\n",
      "Training progress in epoch #6, step 32, discriminator loss=0.685 , generator loss=0.733\n",
      "Training progress in epoch #6, step 33, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #6, step 34, discriminator loss=0.700 , generator loss=0.697\n",
      "Training progress in epoch #6, step 35, discriminator loss=0.704 , generator loss=0.675\n",
      "Training progress in epoch #6, step 36, discriminator loss=0.701 , generator loss=0.679\n",
      "Training progress in epoch #6, step 37, discriminator loss=0.708 , generator loss=0.680\n",
      "Training progress in epoch #6, step 38, discriminator loss=0.711 , generator loss=0.678\n",
      "Training progress in epoch #6, step 39, discriminator loss=0.711 , generator loss=0.671\n",
      "Training progress in epoch #6, step 40, discriminator loss=0.714 , generator loss=0.671\n",
      "Training progress in epoch #6, step 41, discriminator loss=0.715 , generator loss=0.678\n",
      "Training progress in epoch #6, step 42, discriminator loss=0.710 , generator loss=0.685\n",
      "Training progress in epoch #6, step 43, discriminator loss=0.704 , generator loss=0.709\n",
      "Training progress in epoch #6, step 44, discriminator loss=0.702 , generator loss=0.722\n",
      "Training progress in epoch #6, step 45, discriminator loss=0.704 , generator loss=0.713\n",
      "Training progress in epoch #6, step 46, discriminator loss=0.699 , generator loss=0.706\n",
      "Training progress in epoch #6, step 47, discriminator loss=0.692 , generator loss=0.728\n",
      "Training progress in epoch #6, step 48, discriminator loss=0.683 , generator loss=0.737\n",
      "Training progress in epoch #6, step 49, discriminator loss=0.681 , generator loss=0.759\n",
      "Training progress in epoch #6, step 50, discriminator loss=0.666 , generator loss=0.755\n",
      "Training progress in epoch #6, step 51, discriminator loss=0.661 , generator loss=0.766\n",
      "Training progress in epoch #6, step 52, discriminator loss=0.665 , generator loss=0.770\n",
      "Training progress in epoch #6, step 53, discriminator loss=0.657 , generator loss=0.769\n",
      "Training progress in epoch #6, step 54, discriminator loss=0.658 , generator loss=0.755\n",
      "Training progress in epoch #6, step 55, discriminator loss=0.654 , generator loss=0.730\n",
      "Training progress in epoch #6, step 56, discriminator loss=0.663 , generator loss=0.731\n",
      "Training progress in epoch #6, step 57, discriminator loss=0.663 , generator loss=0.713\n",
      "Training progress in epoch #6, step 58, discriminator loss=0.662 , generator loss=0.701\n",
      "Training progress in epoch #6, step 59, discriminator loss=0.664 , generator loss=0.704\n",
      "Training progress in epoch #6, step 60, discriminator loss=0.674 , generator loss=0.712\n",
      "Training progress in epoch #6, step 61, discriminator loss=0.677 , generator loss=0.700\n",
      "Training progress in epoch #6, step 62, discriminator loss=0.678 , generator loss=0.704\n",
      "Training progress in epoch #6, step 63, discriminator loss=0.680 , generator loss=0.709\n",
      "Training progress in epoch #6, step 64, discriminator loss=0.677 , generator loss=0.710\n",
      "Training progress in epoch #6, step 65, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #6, step 66, discriminator loss=0.698 , generator loss=0.694\n",
      "Training progress in epoch #6, step 67, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #6, step 68, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #6, step 69, discriminator loss=0.712 , generator loss=0.714\n",
      "Training progress in epoch #6, step 70, discriminator loss=0.710 , generator loss=0.724\n",
      "Training progress in epoch #6, step 71, discriminator loss=0.711 , generator loss=0.731\n",
      "Training progress in epoch #6, step 72, discriminator loss=0.712 , generator loss=0.726\n",
      "Training progress in epoch #6, step 73, discriminator loss=0.707 , generator loss=0.716\n",
      "Training progress in epoch #6, step 74, discriminator loss=0.707 , generator loss=0.716\n",
      "Training progress in epoch #6, step 75, discriminator loss=0.704 , generator loss=0.708\n",
      "Training progress in epoch #6, step 76, discriminator loss=0.703 , generator loss=0.699\n",
      "Training progress in epoch #6, step 77, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #6, step 78, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #6, step 79, discriminator loss=0.686 , generator loss=0.730\n",
      "Training progress in epoch #6, step 80, discriminator loss=0.675 , generator loss=0.731\n",
      "Training progress in epoch #6, step 81, discriminator loss=0.672 , generator loss=0.727\n",
      "Training progress in epoch #6, step 82, discriminator loss=0.666 , generator loss=0.724\n",
      "Training progress in epoch #6, step 83, discriminator loss=0.658 , generator loss=0.724\n",
      "Training progress in epoch #6, step 84, discriminator loss=0.657 , generator loss=0.742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #6, step 85, discriminator loss=0.652 , generator loss=0.737\n",
      "Training progress in epoch #6, step 86, discriminator loss=0.651 , generator loss=0.755\n",
      "Training progress in epoch #6, step 87, discriminator loss=0.653 , generator loss=0.754\n",
      "Training progress in epoch #6, step 88, discriminator loss=0.648 , generator loss=0.757\n",
      "Training progress in epoch #6, step 89, discriminator loss=0.646 , generator loss=0.769\n",
      "Training progress in epoch #6, step 90, discriminator loss=0.658 , generator loss=0.778\n",
      "Training progress in epoch #6, step 91, discriminator loss=0.658 , generator loss=0.793\n",
      "Training progress in epoch #6, step 92, discriminator loss=0.665 , generator loss=0.785\n",
      "Training progress in epoch #6, step 93, discriminator loss=0.663 , generator loss=0.787\n",
      "Training progress in epoch #6, step 94, discriminator loss=0.666 , generator loss=0.764\n",
      "Training progress in epoch #6, step 95, discriminator loss=0.665 , generator loss=0.729\n",
      "Training progress in epoch #6, step 96, discriminator loss=0.682 , generator loss=0.735\n",
      "Training progress in epoch #6, step 97, discriminator loss=0.689 , generator loss=0.733\n",
      "Training progress in epoch #6, step 98, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #6, step 99, discriminator loss=0.700 , generator loss=0.699\n",
      "Training progress in epoch #6, step 100, discriminator loss=0.699 , generator loss=0.666\n",
      "Training progress in epoch #6, step 101, discriminator loss=0.706 , generator loss=0.679\n",
      "Training progress in epoch #6, step 102, discriminator loss=0.699 , generator loss=0.685\n",
      "Training progress in epoch #6, step 103, discriminator loss=0.707 , generator loss=0.674\n",
      "Training progress in epoch #6, step 104, discriminator loss=0.707 , generator loss=0.670\n",
      "Training progress in epoch #6, step 105, discriminator loss=0.710 , generator loss=0.672\n",
      "Training progress in epoch #6, step 106, discriminator loss=0.700 , generator loss=0.693\n",
      "Training progress in epoch #6, step 107, discriminator loss=0.702 , generator loss=0.687\n",
      "Training progress in epoch #6, step 108, discriminator loss=0.700 , generator loss=0.707\n",
      "Training progress in epoch #6, step 109, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #6, step 110, discriminator loss=0.695 , generator loss=0.734\n",
      "Training progress in epoch #6, step 111, discriminator loss=0.688 , generator loss=0.746\n",
      "Training progress in epoch #6, step 112, discriminator loss=0.685 , generator loss=0.741\n",
      "Training progress in epoch #6, step 113, discriminator loss=0.687 , generator loss=0.745\n",
      "Training progress in epoch #6, step 114, discriminator loss=0.667 , generator loss=0.755\n",
      "Training progress in epoch #6, step 115, discriminator loss=0.661 , generator loss=0.755\n",
      "Training progress in epoch #6, step 116, discriminator loss=0.654 , generator loss=0.762\n",
      "Training progress in epoch #6, step 117, discriminator loss=0.662 , generator loss=0.763\n",
      "Training progress in epoch #6, step 118, discriminator loss=0.647 , generator loss=0.761\n",
      "Training progress in epoch #6, step 119, discriminator loss=0.656 , generator loss=0.740\n",
      "Training progress in epoch #6, step 120, discriminator loss=0.653 , generator loss=0.721\n",
      "Training progress in epoch #6, step 121, discriminator loss=0.656 , generator loss=0.721\n",
      "Training progress in epoch #6, step 122, discriminator loss=0.651 , generator loss=0.711\n",
      "Training progress in epoch #6, step 123, discriminator loss=0.659 , generator loss=0.703\n",
      "Training progress in epoch #6, step 124, discriminator loss=0.665 , generator loss=0.718\n",
      "Training progress in epoch #6, step 125, discriminator loss=0.672 , generator loss=0.723\n",
      "Training progress in epoch #6, step 126, discriminator loss=0.674 , generator loss=0.709\n",
      "Training progress in epoch #6, step 127, discriminator loss=0.672 , generator loss=0.711\n",
      "Training progress in epoch #6, step 128, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #6, step 129, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #6, step 130, discriminator loss=0.699 , generator loss=0.700\n",
      "Training progress in epoch #6, step 131, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #6, step 132, discriminator loss=0.700 , generator loss=0.713\n",
      "Training progress in epoch #6, step 133, discriminator loss=0.705 , generator loss=0.733\n",
      "Training progress in epoch #6, step 134, discriminator loss=0.705 , generator loss=0.734\n",
      "Training progress in epoch #6, step 135, discriminator loss=0.713 , generator loss=0.734\n",
      "Training progress in epoch #6, step 136, discriminator loss=0.704 , generator loss=0.727\n",
      "Training progress in epoch #6, step 137, discriminator loss=0.714 , generator loss=0.697\n",
      "Training progress in epoch #6, step 138, discriminator loss=0.701 , generator loss=0.705\n",
      "Training progress in epoch #6, step 139, discriminator loss=0.699 , generator loss=0.702\n",
      "Training progress in epoch #6, step 140, discriminator loss=0.699 , generator loss=0.706\n",
      "Training progress in epoch #6, step 141, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #6, step 142, discriminator loss=0.676 , generator loss=0.702\n",
      "Training progress in epoch #6, step 143, discriminator loss=0.675 , generator loss=0.717\n",
      "Training progress in epoch #6, step 144, discriminator loss=0.674 , generator loss=0.723\n",
      "Training progress in epoch #6, step 145, discriminator loss=0.654 , generator loss=0.721\n",
      "Training progress in epoch #6, step 146, discriminator loss=0.657 , generator loss=0.732\n",
      "Training progress in epoch #6, step 147, discriminator loss=0.654 , generator loss=0.746\n",
      "Training progress in epoch #6, step 148, discriminator loss=0.644 , generator loss=0.753\n",
      "Training progress in epoch #6, step 149, discriminator loss=0.650 , generator loss=0.765\n",
      "Training progress in epoch #6, step 150, discriminator loss=0.647 , generator loss=0.771\n",
      "Training progress in epoch #6, step 151, discriminator loss=0.650 , generator loss=0.807\n",
      "Training progress in epoch #6, step 152, discriminator loss=0.652 , generator loss=0.796\n",
      "Training progress in epoch #6, step 153, discriminator loss=0.653 , generator loss=0.798\n",
      "Training progress in epoch #6, step 154, discriminator loss=0.662 , generator loss=0.785\n",
      "Training progress in epoch #6, step 155, discriminator loss=0.665 , generator loss=0.767\n",
      "Training progress in epoch #6, step 156, discriminator loss=0.671 , generator loss=0.735\n",
      "Training progress in epoch #6, step 157, discriminator loss=0.674 , generator loss=0.724\n",
      "Training progress in epoch #6, step 158, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #6, step 159, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #6, step 160, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #6, step 161, discriminator loss=0.707 , generator loss=0.679\n",
      "Training progress in epoch #6, step 162, discriminator loss=0.710 , generator loss=0.670\n",
      "Training progress in epoch #6, step 163, discriminator loss=0.714 , generator loss=0.657\n",
      "Training progress in epoch #6, step 164, discriminator loss=0.716 , generator loss=0.654\n",
      "Training progress in epoch #6, step 165, discriminator loss=0.712 , generator loss=0.668\n",
      "Training progress in epoch #6, step 166, discriminator loss=0.708 , generator loss=0.692\n",
      "Training progress in epoch #6, step 167, discriminator loss=0.711 , generator loss=0.688\n",
      "Training progress in epoch #6, step 168, discriminator loss=0.703 , generator loss=0.703\n",
      "Training progress in epoch #6, step 169, discriminator loss=0.704 , generator loss=0.726\n",
      "Training progress in epoch #6, step 170, discriminator loss=0.697 , generator loss=0.720\n",
      "Training progress in epoch #6, step 171, discriminator loss=0.696 , generator loss=0.729\n",
      "Training progress in epoch #6, step 172, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #6, step 173, discriminator loss=0.676 , generator loss=0.725\n",
      "Training progress in epoch #6, step 174, discriminator loss=0.674 , generator loss=0.734\n",
      "Training progress in epoch #6, step 175, discriminator loss=0.666 , generator loss=0.737\n",
      "Training progress in epoch #6, step 176, discriminator loss=0.665 , generator loss=0.741\n",
      "Training progress in epoch #6, step 177, discriminator loss=0.660 , generator loss=0.744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #6, step 178, discriminator loss=0.661 , generator loss=0.738\n",
      "Training progress in epoch #6, step 179, discriminator loss=0.659 , generator loss=0.747\n",
      "Training progress in epoch #6, step 180, discriminator loss=0.650 , generator loss=0.735\n",
      "Training progress in epoch #6, step 181, discriminator loss=0.661 , generator loss=0.733\n",
      "Training progress in epoch #6, step 182, discriminator loss=0.656 , generator loss=0.710\n",
      "Training progress in epoch #6, step 183, discriminator loss=0.665 , generator loss=0.720\n",
      "Training progress in epoch #6, step 184, discriminator loss=0.659 , generator loss=0.732\n",
      "Training progress in epoch #6, step 185, discriminator loss=0.660 , generator loss=0.740\n",
      "Training progress in epoch #6, step 186, discriminator loss=0.665 , generator loss=0.715\n",
      "Training progress in epoch #6, step 187, discriminator loss=0.669 , generator loss=0.721\n",
      "Training progress in epoch #6, step 188, discriminator loss=0.679 , generator loss=0.747\n",
      "Training progress in epoch #6, step 189, discriminator loss=0.679 , generator loss=0.749\n",
      "Training progress in epoch #6, step 190, discriminator loss=0.681 , generator loss=0.748\n",
      "Training progress in epoch #6, step 191, discriminator loss=0.683 , generator loss=0.738\n",
      "Training progress in epoch #6, step 192, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #6, step 193, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #6, step 194, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #6, step 195, discriminator loss=0.699 , generator loss=0.720\n",
      "Training progress in epoch #6, step 196, discriminator loss=0.703 , generator loss=0.709\n",
      "Training progress in epoch #6, step 197, discriminator loss=0.697 , generator loss=0.724\n",
      "Training progress in epoch #6, step 198, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #6, step 199, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #6, step 200, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #6, step 201, discriminator loss=0.703 , generator loss=0.668\n",
      "Training progress in epoch #6, step 202, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #6, step 203, discriminator loss=0.700 , generator loss=0.710\n",
      "Training progress in epoch #6, step 204, discriminator loss=0.699 , generator loss=0.714\n",
      "Training progress in epoch #6, step 205, discriminator loss=0.683 , generator loss=0.720\n",
      "Training progress in epoch #6, step 206, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #6, step 207, discriminator loss=0.688 , generator loss=0.731\n",
      "Training progress in epoch #6, step 208, discriminator loss=0.684 , generator loss=0.733\n",
      "Training progress in epoch #6, step 209, discriminator loss=0.687 , generator loss=0.742\n",
      "Training progress in epoch #6, step 210, discriminator loss=0.675 , generator loss=0.752\n",
      "Training progress in epoch #6, step 211, discriminator loss=0.681 , generator loss=0.744\n",
      "Training progress in epoch #6, step 212, discriminator loss=0.681 , generator loss=0.737\n",
      "Training progress in epoch #6, step 213, discriminator loss=0.681 , generator loss=0.738\n",
      "Training progress in epoch #6, step 214, discriminator loss=0.675 , generator loss=0.737\n",
      "Training progress in epoch #6, step 215, discriminator loss=0.676 , generator loss=0.736\n",
      "Training progress in epoch #6, step 216, discriminator loss=0.668 , generator loss=0.730\n",
      "Training progress in epoch #6, step 217, discriminator loss=0.674 , generator loss=0.704\n",
      "Training progress in epoch #6, step 218, discriminator loss=0.674 , generator loss=0.724\n",
      "Training progress in epoch #6, step 219, discriminator loss=0.669 , generator loss=0.730\n",
      "Training progress in epoch #6, step 220, discriminator loss=0.664 , generator loss=0.752\n",
      "Training progress in epoch #6, step 221, discriminator loss=0.663 , generator loss=0.735\n",
      "Training progress in epoch #6, step 222, discriminator loss=0.666 , generator loss=0.728\n",
      "Training progress in epoch #6, step 223, discriminator loss=0.665 , generator loss=0.722\n",
      "Training progress in epoch #6, step 224, discriminator loss=0.673 , generator loss=0.728\n",
      "Training progress in epoch #6, step 225, discriminator loss=0.675 , generator loss=0.744\n",
      "Training progress in epoch #6, step 226, discriminator loss=0.668 , generator loss=0.745\n",
      "Training progress in epoch #6, step 227, discriminator loss=0.680 , generator loss=0.725\n",
      "Training progress in epoch #6, step 228, discriminator loss=0.678 , generator loss=0.720\n",
      "Training progress in epoch #6, step 229, discriminator loss=0.678 , generator loss=0.734\n",
      "Training progress in epoch #6, step 230, discriminator loss=0.682 , generator loss=0.719\n",
      "Training progress in epoch #6, step 231, discriminator loss=0.679 , generator loss=0.715\n",
      "Training progress in epoch #6, step 232, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #6, step 233, discriminator loss=0.687 , generator loss=0.717\n",
      "Disciminator Accuracy on real images: 58%, on fake images: 56%\n",
      "Training progress in epoch #7, step 0, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #7, step 1, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #7, step 2, discriminator loss=0.685 , generator loss=0.684\n",
      "Training progress in epoch #7, step 3, discriminator loss=0.682 , generator loss=0.677\n",
      "Training progress in epoch #7, step 4, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #7, step 5, discriminator loss=0.678 , generator loss=0.715\n",
      "Training progress in epoch #7, step 6, discriminator loss=0.679 , generator loss=0.718\n",
      "Training progress in epoch #7, step 7, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #7, step 8, discriminator loss=0.678 , generator loss=0.713\n",
      "Training progress in epoch #7, step 9, discriminator loss=0.676 , generator loss=0.720\n",
      "Training progress in epoch #7, step 10, discriminator loss=0.677 , generator loss=0.735\n",
      "Training progress in epoch #7, step 11, discriminator loss=0.677 , generator loss=0.719\n",
      "Training progress in epoch #7, step 12, discriminator loss=0.671 , generator loss=0.733\n",
      "Training progress in epoch #7, step 13, discriminator loss=0.672 , generator loss=0.744\n",
      "Training progress in epoch #7, step 14, discriminator loss=0.672 , generator loss=0.736\n",
      "Training progress in epoch #7, step 15, discriminator loss=0.677 , generator loss=0.734\n",
      "Training progress in epoch #7, step 16, discriminator loss=0.675 , generator loss=0.740\n",
      "Training progress in epoch #7, step 17, discriminator loss=0.670 , generator loss=0.748\n",
      "Training progress in epoch #7, step 18, discriminator loss=0.664 , generator loss=0.748\n",
      "Training progress in epoch #7, step 19, discriminator loss=0.672 , generator loss=0.754\n",
      "Training progress in epoch #7, step 20, discriminator loss=0.668 , generator loss=0.747\n",
      "Training progress in epoch #7, step 21, discriminator loss=0.661 , generator loss=0.748\n",
      "Training progress in epoch #7, step 22, discriminator loss=0.662 , generator loss=0.767\n",
      "Training progress in epoch #7, step 23, discriminator loss=0.666 , generator loss=0.749\n",
      "Training progress in epoch #7, step 24, discriminator loss=0.668 , generator loss=0.750\n",
      "Training progress in epoch #7, step 25, discriminator loss=0.664 , generator loss=0.733\n",
      "Training progress in epoch #7, step 26, discriminator loss=0.672 , generator loss=0.726\n",
      "Training progress in epoch #7, step 27, discriminator loss=0.671 , generator loss=0.716\n",
      "Training progress in epoch #7, step 28, discriminator loss=0.675 , generator loss=0.704\n",
      "Training progress in epoch #7, step 29, discriminator loss=0.680 , generator loss=0.706\n",
      "Training progress in epoch #7, step 30, discriminator loss=0.685 , generator loss=0.691\n",
      "Training progress in epoch #7, step 31, discriminator loss=0.684 , generator loss=0.676\n",
      "Training progress in epoch #7, step 32, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #7, step 33, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #7, step 34, discriminator loss=0.701 , generator loss=0.697\n",
      "Training progress in epoch #7, step 35, discriminator loss=0.696 , generator loss=0.707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #7, step 36, discriminator loss=0.700 , generator loss=0.725\n",
      "Training progress in epoch #7, step 37, discriminator loss=0.706 , generator loss=0.735\n",
      "Training progress in epoch #7, step 38, discriminator loss=0.708 , generator loss=0.749\n",
      "Training progress in epoch #7, step 39, discriminator loss=0.704 , generator loss=0.754\n",
      "Training progress in epoch #7, step 40, discriminator loss=0.703 , generator loss=0.728\n",
      "Training progress in epoch #7, step 41, discriminator loss=0.699 , generator loss=0.736\n",
      "Training progress in epoch #7, step 42, discriminator loss=0.690 , generator loss=0.727\n",
      "Training progress in epoch #7, step 43, discriminator loss=0.697 , generator loss=0.711\n",
      "Training progress in epoch #7, step 44, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #7, step 45, discriminator loss=0.697 , generator loss=0.700\n",
      "Training progress in epoch #7, step 46, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #7, step 47, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #7, step 48, discriminator loss=0.674 , generator loss=0.699\n",
      "Training progress in epoch #7, step 49, discriminator loss=0.683 , generator loss=0.695\n",
      "Training progress in epoch #7, step 50, discriminator loss=0.680 , generator loss=0.694\n",
      "Training progress in epoch #7, step 51, discriminator loss=0.669 , generator loss=0.706\n",
      "Training progress in epoch #7, step 52, discriminator loss=0.664 , generator loss=0.719\n",
      "Training progress in epoch #7, step 53, discriminator loss=0.657 , generator loss=0.734\n",
      "Training progress in epoch #7, step 54, discriminator loss=0.655 , generator loss=0.742\n",
      "Training progress in epoch #7, step 55, discriminator loss=0.670 , generator loss=0.767\n",
      "Training progress in epoch #7, step 56, discriminator loss=0.665 , generator loss=0.787\n",
      "Training progress in epoch #7, step 57, discriminator loss=0.664 , generator loss=0.793\n",
      "Training progress in epoch #7, step 58, discriminator loss=0.669 , generator loss=0.816\n",
      "Training progress in epoch #7, step 59, discriminator loss=0.653 , generator loss=0.851\n",
      "Training progress in epoch #7, step 60, discriminator loss=0.656 , generator loss=0.829\n",
      "Training progress in epoch #7, step 61, discriminator loss=0.647 , generator loss=0.823\n",
      "Training progress in epoch #7, step 62, discriminator loss=0.654 , generator loss=0.776\n",
      "Training progress in epoch #7, step 63, discriminator loss=0.673 , generator loss=0.757\n",
      "Training progress in epoch #7, step 64, discriminator loss=0.668 , generator loss=0.723\n",
      "Training progress in epoch #7, step 65, discriminator loss=0.676 , generator loss=0.690\n",
      "Training progress in epoch #7, step 66, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #7, step 67, discriminator loss=0.694 , generator loss=0.660\n",
      "Training progress in epoch #7, step 68, discriminator loss=0.700 , generator loss=0.638\n",
      "Training progress in epoch #7, step 69, discriminator loss=0.699 , generator loss=0.636\n",
      "Training progress in epoch #7, step 70, discriminator loss=0.702 , generator loss=0.641\n",
      "Training progress in epoch #7, step 71, discriminator loss=0.691 , generator loss=0.656\n",
      "Training progress in epoch #7, step 72, discriminator loss=0.693 , generator loss=0.668\n",
      "Training progress in epoch #7, step 73, discriminator loss=0.696 , generator loss=0.672\n",
      "Training progress in epoch #7, step 74, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #7, step 75, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #7, step 76, discriminator loss=0.704 , generator loss=0.712\n",
      "Training progress in epoch #7, step 77, discriminator loss=0.706 , generator loss=0.751\n",
      "Training progress in epoch #7, step 78, discriminator loss=0.703 , generator loss=0.785\n",
      "Training progress in epoch #7, step 79, discriminator loss=0.697 , generator loss=0.795\n",
      "Training progress in epoch #7, step 80, discriminator loss=0.685 , generator loss=0.800\n",
      "Training progress in epoch #7, step 81, discriminator loss=0.678 , generator loss=0.799\n",
      "Training progress in epoch #7, step 82, discriminator loss=0.668 , generator loss=0.793\n",
      "Training progress in epoch #7, step 83, discriminator loss=0.663 , generator loss=0.780\n",
      "Training progress in epoch #7, step 84, discriminator loss=0.657 , generator loss=0.773\n",
      "Training progress in epoch #7, step 85, discriminator loss=0.663 , generator loss=0.762\n",
      "Training progress in epoch #7, step 86, discriminator loss=0.657 , generator loss=0.732\n",
      "Training progress in epoch #7, step 87, discriminator loss=0.671 , generator loss=0.715\n",
      "Training progress in epoch #7, step 88, discriminator loss=0.669 , generator loss=0.698\n",
      "Training progress in epoch #7, step 89, discriminator loss=0.664 , generator loss=0.687\n",
      "Training progress in epoch #7, step 90, discriminator loss=0.662 , generator loss=0.690\n",
      "Training progress in epoch #7, step 91, discriminator loss=0.670 , generator loss=0.692\n",
      "Training progress in epoch #7, step 92, discriminator loss=0.670 , generator loss=0.685\n",
      "Training progress in epoch #7, step 93, discriminator loss=0.666 , generator loss=0.698\n",
      "Training progress in epoch #7, step 94, discriminator loss=0.665 , generator loss=0.707\n",
      "Training progress in epoch #7, step 95, discriminator loss=0.670 , generator loss=0.707\n",
      "Training progress in epoch #7, step 96, discriminator loss=0.682 , generator loss=0.729\n",
      "Training progress in epoch #7, step 97, discriminator loss=0.683 , generator loss=0.735\n",
      "Training progress in epoch #7, step 98, discriminator loss=0.690 , generator loss=0.732\n",
      "Training progress in epoch #7, step 99, discriminator loss=0.691 , generator loss=0.741\n",
      "Training progress in epoch #7, step 100, discriminator loss=0.700 , generator loss=0.761\n",
      "Training progress in epoch #7, step 101, discriminator loss=0.697 , generator loss=0.755\n",
      "Training progress in epoch #7, step 102, discriminator loss=0.697 , generator loss=0.766\n",
      "Training progress in epoch #7, step 103, discriminator loss=0.697 , generator loss=0.768\n",
      "Training progress in epoch #7, step 104, discriminator loss=0.691 , generator loss=0.753\n",
      "Training progress in epoch #7, step 105, discriminator loss=0.694 , generator loss=0.770\n",
      "Training progress in epoch #7, step 106, discriminator loss=0.696 , generator loss=0.750\n",
      "Training progress in epoch #7, step 107, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #7, step 108, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #7, step 109, discriminator loss=0.701 , generator loss=0.674\n",
      "Training progress in epoch #7, step 110, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #7, step 111, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #7, step 112, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #7, step 113, discriminator loss=0.675 , generator loss=0.697\n",
      "Training progress in epoch #7, step 114, discriminator loss=0.673 , generator loss=0.694\n",
      "Training progress in epoch #7, step 115, discriminator loss=0.673 , generator loss=0.679\n",
      "Training progress in epoch #7, step 116, discriminator loss=0.662 , generator loss=0.685\n",
      "Training progress in epoch #7, step 117, discriminator loss=0.661 , generator loss=0.709\n",
      "Training progress in epoch #7, step 118, discriminator loss=0.668 , generator loss=0.726\n",
      "Training progress in epoch #7, step 119, discriminator loss=0.666 , generator loss=0.751\n",
      "Training progress in epoch #7, step 120, discriminator loss=0.663 , generator loss=0.772\n",
      "Training progress in epoch #7, step 121, discriminator loss=0.668 , generator loss=0.770\n",
      "Training progress in epoch #7, step 122, discriminator loss=0.665 , generator loss=0.785\n",
      "Training progress in epoch #7, step 123, discriminator loss=0.672 , generator loss=0.786\n",
      "Training progress in epoch #7, step 124, discriminator loss=0.667 , generator loss=0.813\n",
      "Training progress in epoch #7, step 125, discriminator loss=0.666 , generator loss=0.813\n",
      "Training progress in epoch #7, step 126, discriminator loss=0.668 , generator loss=0.805\n",
      "Training progress in epoch #7, step 127, discriminator loss=0.663 , generator loss=0.783\n",
      "Training progress in epoch #7, step 128, discriminator loss=0.674 , generator loss=0.764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #7, step 129, discriminator loss=0.683 , generator loss=0.744\n",
      "Training progress in epoch #7, step 130, discriminator loss=0.686 , generator loss=0.730\n",
      "Training progress in epoch #7, step 131, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #7, step 132, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #7, step 133, discriminator loss=0.699 , generator loss=0.689\n",
      "Training progress in epoch #7, step 134, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #7, step 135, discriminator loss=0.700 , generator loss=0.676\n",
      "Training progress in epoch #7, step 136, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #7, step 137, discriminator loss=0.685 , generator loss=0.676\n",
      "Training progress in epoch #7, step 138, discriminator loss=0.684 , generator loss=0.680\n",
      "Training progress in epoch #7, step 139, discriminator loss=0.680 , generator loss=0.714\n",
      "Training progress in epoch #7, step 140, discriminator loss=0.679 , generator loss=0.726\n",
      "Training progress in epoch #7, step 141, discriminator loss=0.674 , generator loss=0.748\n",
      "Training progress in epoch #7, step 142, discriminator loss=0.676 , generator loss=0.748\n",
      "Training progress in epoch #7, step 143, discriminator loss=0.670 , generator loss=0.757\n",
      "Training progress in epoch #7, step 144, discriminator loss=0.667 , generator loss=0.770\n",
      "Training progress in epoch #7, step 145, discriminator loss=0.667 , generator loss=0.779\n",
      "Training progress in epoch #7, step 146, discriminator loss=0.665 , generator loss=0.785\n",
      "Training progress in epoch #7, step 147, discriminator loss=0.666 , generator loss=0.800\n",
      "Training progress in epoch #7, step 148, discriminator loss=0.667 , generator loss=0.825\n",
      "Training progress in epoch #7, step 149, discriminator loss=0.666 , generator loss=0.826\n",
      "Training progress in epoch #7, step 150, discriminator loss=0.657 , generator loss=0.806\n",
      "Training progress in epoch #7, step 151, discriminator loss=0.675 , generator loss=0.770\n",
      "Training progress in epoch #7, step 152, discriminator loss=0.665 , generator loss=0.740\n",
      "Training progress in epoch #7, step 153, discriminator loss=0.676 , generator loss=0.735\n",
      "Training progress in epoch #7, step 154, discriminator loss=0.681 , generator loss=0.709\n",
      "Training progress in epoch #7, step 155, discriminator loss=0.682 , generator loss=0.694\n",
      "Training progress in epoch #7, step 156, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #7, step 157, discriminator loss=0.684 , generator loss=0.692\n",
      "Training progress in epoch #7, step 158, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #7, step 159, discriminator loss=0.695 , generator loss=0.664\n",
      "Training progress in epoch #7, step 160, discriminator loss=0.697 , generator loss=0.666\n",
      "Training progress in epoch #7, step 161, discriminator loss=0.696 , generator loss=0.647\n",
      "Training progress in epoch #7, step 162, discriminator loss=0.681 , generator loss=0.668\n",
      "Training progress in epoch #7, step 163, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #7, step 164, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #7, step 165, discriminator loss=0.680 , generator loss=0.710\n",
      "Training progress in epoch #7, step 166, discriminator loss=0.680 , generator loss=0.736\n",
      "Training progress in epoch #7, step 167, discriminator loss=0.681 , generator loss=0.746\n",
      "Training progress in epoch #7, step 168, discriminator loss=0.676 , generator loss=0.745\n",
      "Training progress in epoch #7, step 169, discriminator loss=0.677 , generator loss=0.763\n",
      "Training progress in epoch #7, step 170, discriminator loss=0.677 , generator loss=0.767\n",
      "Training progress in epoch #7, step 171, discriminator loss=0.670 , generator loss=0.784\n",
      "Training progress in epoch #7, step 172, discriminator loss=0.671 , generator loss=0.806\n",
      "Training progress in epoch #7, step 173, discriminator loss=0.665 , generator loss=0.816\n",
      "Training progress in epoch #7, step 174, discriminator loss=0.672 , generator loss=0.787\n",
      "Training progress in epoch #7, step 175, discriminator loss=0.661 , generator loss=0.753\n",
      "Training progress in epoch #7, step 176, discriminator loss=0.676 , generator loss=0.756\n",
      "Training progress in epoch #7, step 177, discriminator loss=0.679 , generator loss=0.738\n",
      "Training progress in epoch #7, step 178, discriminator loss=0.679 , generator loss=0.723\n",
      "Training progress in epoch #7, step 179, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #7, step 180, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #7, step 181, discriminator loss=0.687 , generator loss=0.666\n",
      "Training progress in epoch #7, step 182, discriminator loss=0.684 , generator loss=0.677\n",
      "Training progress in epoch #7, step 183, discriminator loss=0.694 , generator loss=0.679\n",
      "Training progress in epoch #7, step 184, discriminator loss=0.686 , generator loss=0.682\n",
      "Training progress in epoch #7, step 185, discriminator loss=0.685 , generator loss=0.684\n",
      "Training progress in epoch #7, step 186, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #7, step 187, discriminator loss=0.674 , generator loss=0.702\n",
      "Training progress in epoch #7, step 188, discriminator loss=0.675 , generator loss=0.747\n",
      "Training progress in epoch #7, step 189, discriminator loss=0.676 , generator loss=0.759\n",
      "Training progress in epoch #7, step 190, discriminator loss=0.670 , generator loss=0.756\n",
      "Training progress in epoch #7, step 191, discriminator loss=0.678 , generator loss=0.765\n",
      "Training progress in epoch #7, step 192, discriminator loss=0.669 , generator loss=0.776\n",
      "Training progress in epoch #7, step 193, discriminator loss=0.681 , generator loss=0.784\n",
      "Training progress in epoch #7, step 194, discriminator loss=0.668 , generator loss=0.787\n",
      "Training progress in epoch #7, step 195, discriminator loss=0.661 , generator loss=0.785\n",
      "Training progress in epoch #7, step 196, discriminator loss=0.665 , generator loss=0.819\n",
      "Training progress in epoch #7, step 197, discriminator loss=0.668 , generator loss=0.815\n",
      "Training progress in epoch #7, step 198, discriminator loss=0.665 , generator loss=0.797\n",
      "Training progress in epoch #7, step 199, discriminator loss=0.658 , generator loss=0.772\n",
      "Training progress in epoch #7, step 200, discriminator loss=0.657 , generator loss=0.765\n",
      "Training progress in epoch #7, step 201, discriminator loss=0.653 , generator loss=0.759\n",
      "Training progress in epoch #7, step 202, discriminator loss=0.666 , generator loss=0.749\n",
      "Training progress in epoch #7, step 203, discriminator loss=0.669 , generator loss=0.742\n",
      "Training progress in epoch #7, step 204, discriminator loss=0.669 , generator loss=0.717\n",
      "Training progress in epoch #7, step 205, discriminator loss=0.679 , generator loss=0.702\n",
      "Training progress in epoch #7, step 206, discriminator loss=0.678 , generator loss=0.692\n",
      "Training progress in epoch #7, step 207, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #7, step 208, discriminator loss=0.696 , generator loss=0.668\n",
      "Training progress in epoch #7, step 209, discriminator loss=0.690 , generator loss=0.674\n",
      "Training progress in epoch #7, step 210, discriminator loss=0.697 , generator loss=0.648\n",
      "Training progress in epoch #7, step 211, discriminator loss=0.697 , generator loss=0.654\n",
      "Training progress in epoch #7, step 212, discriminator loss=0.713 , generator loss=0.685\n",
      "Training progress in epoch #7, step 213, discriminator loss=0.707 , generator loss=0.707\n",
      "Training progress in epoch #7, step 214, discriminator loss=0.719 , generator loss=0.712\n",
      "Training progress in epoch #7, step 215, discriminator loss=0.718 , generator loss=0.717\n",
      "Training progress in epoch #7, step 216, discriminator loss=0.710 , generator loss=0.696\n",
      "Training progress in epoch #7, step 217, discriminator loss=0.709 , generator loss=0.719\n",
      "Training progress in epoch #7, step 218, discriminator loss=0.706 , generator loss=0.755\n",
      "Training progress in epoch #7, step 219, discriminator loss=0.704 , generator loss=0.766\n",
      "Training progress in epoch #7, step 220, discriminator loss=0.690 , generator loss=0.764\n",
      "Training progress in epoch #7, step 221, discriminator loss=0.690 , generator loss=0.771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #7, step 222, discriminator loss=0.675 , generator loss=0.777\n",
      "Training progress in epoch #7, step 223, discriminator loss=0.677 , generator loss=0.764\n",
      "Training progress in epoch #7, step 224, discriminator loss=0.668 , generator loss=0.756\n",
      "Training progress in epoch #7, step 225, discriminator loss=0.662 , generator loss=0.758\n",
      "Training progress in epoch #7, step 226, discriminator loss=0.665 , generator loss=0.750\n",
      "Training progress in epoch #7, step 227, discriminator loss=0.656 , generator loss=0.749\n",
      "Training progress in epoch #7, step 228, discriminator loss=0.652 , generator loss=0.729\n",
      "Training progress in epoch #7, step 229, discriminator loss=0.654 , generator loss=0.736\n",
      "Training progress in epoch #7, step 230, discriminator loss=0.652 , generator loss=0.739\n",
      "Training progress in epoch #7, step 231, discriminator loss=0.656 , generator loss=0.730\n",
      "Training progress in epoch #7, step 232, discriminator loss=0.655 , generator loss=0.723\n",
      "Training progress in epoch #7, step 233, discriminator loss=0.659 , generator loss=0.698\n",
      "Disciminator Accuracy on real images: 91%, on fake images: 52%\n",
      "Training progress in epoch #8, step 0, discriminator loss=0.659 , generator loss=0.713\n",
      "Training progress in epoch #8, step 1, discriminator loss=0.672 , generator loss=0.720\n",
      "Training progress in epoch #8, step 2, discriminator loss=0.672 , generator loss=0.709\n",
      "Training progress in epoch #8, step 3, discriminator loss=0.666 , generator loss=0.705\n",
      "Training progress in epoch #8, step 4, discriminator loss=0.678 , generator loss=0.715\n",
      "Training progress in epoch #8, step 5, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #8, step 6, discriminator loss=0.699 , generator loss=0.718\n",
      "Training progress in epoch #8, step 7, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #8, step 8, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #8, step 9, discriminator loss=0.706 , generator loss=0.739\n",
      "Training progress in epoch #8, step 10, discriminator loss=0.689 , generator loss=0.728\n",
      "Training progress in epoch #8, step 11, discriminator loss=0.705 , generator loss=0.753\n",
      "Training progress in epoch #8, step 12, discriminator loss=0.706 , generator loss=0.737\n",
      "Training progress in epoch #8, step 13, discriminator loss=0.705 , generator loss=0.725\n",
      "Training progress in epoch #8, step 14, discriminator loss=0.703 , generator loss=0.736\n",
      "Training progress in epoch #8, step 15, discriminator loss=0.697 , generator loss=0.734\n",
      "Training progress in epoch #8, step 16, discriminator loss=0.693 , generator loss=0.738\n",
      "Training progress in epoch #8, step 17, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #8, step 18, discriminator loss=0.687 , generator loss=0.741\n",
      "Training progress in epoch #8, step 19, discriminator loss=0.684 , generator loss=0.742\n",
      "Training progress in epoch #8, step 20, discriminator loss=0.684 , generator loss=0.709\n",
      "Training progress in epoch #8, step 21, discriminator loss=0.680 , generator loss=0.694\n",
      "Training progress in epoch #8, step 22, discriminator loss=0.676 , generator loss=0.687\n",
      "Training progress in epoch #8, step 23, discriminator loss=0.664 , generator loss=0.691\n",
      "Training progress in epoch #8, step 24, discriminator loss=0.663 , generator loss=0.717\n",
      "Training progress in epoch #8, step 25, discriminator loss=0.666 , generator loss=0.735\n",
      "Training progress in epoch #8, step 26, discriminator loss=0.658 , generator loss=0.739\n",
      "Training progress in epoch #8, step 27, discriminator loss=0.661 , generator loss=0.746\n",
      "Training progress in epoch #8, step 28, discriminator loss=0.665 , generator loss=0.744\n",
      "Training progress in epoch #8, step 29, discriminator loss=0.664 , generator loss=0.752\n",
      "Training progress in epoch #8, step 30, discriminator loss=0.671 , generator loss=0.782\n",
      "Training progress in epoch #8, step 31, discriminator loss=0.658 , generator loss=0.772\n",
      "Training progress in epoch #8, step 32, discriminator loss=0.659 , generator loss=0.777\n",
      "Training progress in epoch #8, step 33, discriminator loss=0.658 , generator loss=0.781\n",
      "Training progress in epoch #8, step 34, discriminator loss=0.656 , generator loss=0.773\n",
      "Training progress in epoch #8, step 35, discriminator loss=0.665 , generator loss=0.776\n",
      "Training progress in epoch #8, step 36, discriminator loss=0.663 , generator loss=0.743\n",
      "Training progress in epoch #8, step 37, discriminator loss=0.671 , generator loss=0.740\n",
      "Training progress in epoch #8, step 38, discriminator loss=0.678 , generator loss=0.750\n",
      "Training progress in epoch #8, step 39, discriminator loss=0.681 , generator loss=0.733\n",
      "Training progress in epoch #8, step 40, discriminator loss=0.680 , generator loss=0.704\n",
      "Training progress in epoch #8, step 41, discriminator loss=0.681 , generator loss=0.689\n",
      "Training progress in epoch #8, step 42, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #8, step 43, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #8, step 44, discriminator loss=0.685 , generator loss=0.701\n",
      "Training progress in epoch #8, step 45, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #8, step 46, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #8, step 47, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #8, step 48, discriminator loss=0.686 , generator loss=0.687\n",
      "Training progress in epoch #8, step 49, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #8, step 50, discriminator loss=0.683 , generator loss=0.719\n",
      "Training progress in epoch #8, step 51, discriminator loss=0.683 , generator loss=0.718\n",
      "Training progress in epoch #8, step 52, discriminator loss=0.683 , generator loss=0.722\n",
      "Training progress in epoch #8, step 53, discriminator loss=0.683 , generator loss=0.736\n",
      "Training progress in epoch #8, step 54, discriminator loss=0.680 , generator loss=0.742\n",
      "Training progress in epoch #8, step 55, discriminator loss=0.683 , generator loss=0.735\n",
      "Training progress in epoch #8, step 56, discriminator loss=0.682 , generator loss=0.733\n",
      "Training progress in epoch #8, step 57, discriminator loss=0.676 , generator loss=0.724\n",
      "Training progress in epoch #8, step 58, discriminator loss=0.676 , generator loss=0.740\n",
      "Training progress in epoch #8, step 59, discriminator loss=0.673 , generator loss=0.766\n",
      "Training progress in epoch #8, step 60, discriminator loss=0.669 , generator loss=0.767\n",
      "Training progress in epoch #8, step 61, discriminator loss=0.676 , generator loss=0.760\n",
      "Training progress in epoch #8, step 62, discriminator loss=0.668 , generator loss=0.737\n",
      "Training progress in epoch #8, step 63, discriminator loss=0.668 , generator loss=0.728\n",
      "Training progress in epoch #8, step 64, discriminator loss=0.665 , generator loss=0.713\n",
      "Training progress in epoch #8, step 65, discriminator loss=0.678 , generator loss=0.718\n",
      "Training progress in epoch #8, step 66, discriminator loss=0.678 , generator loss=0.721\n",
      "Training progress in epoch #8, step 67, discriminator loss=0.669 , generator loss=0.732\n",
      "Training progress in epoch #8, step 68, discriminator loss=0.674 , generator loss=0.719\n",
      "Training progress in epoch #8, step 69, discriminator loss=0.670 , generator loss=0.707\n",
      "Training progress in epoch #8, step 70, discriminator loss=0.673 , generator loss=0.696\n",
      "Training progress in epoch #8, step 71, discriminator loss=0.682 , generator loss=0.718\n",
      "Training progress in epoch #8, step 72, discriminator loss=0.679 , generator loss=0.705\n",
      "Training progress in epoch #8, step 73, discriminator loss=0.673 , generator loss=0.715\n",
      "Training progress in epoch #8, step 74, discriminator loss=0.673 , generator loss=0.730\n",
      "Training progress in epoch #8, step 75, discriminator loss=0.673 , generator loss=0.730\n",
      "Training progress in epoch #8, step 76, discriminator loss=0.680 , generator loss=0.754\n",
      "Training progress in epoch #8, step 77, discriminator loss=0.684 , generator loss=0.756\n",
      "Training progress in epoch #8, step 78, discriminator loss=0.684 , generator loss=0.752\n",
      "Training progress in epoch #8, step 79, discriminator loss=0.681 , generator loss=0.737\n",
      "Training progress in epoch #8, step 80, discriminator loss=0.683 , generator loss=0.727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #8, step 81, discriminator loss=0.683 , generator loss=0.746\n",
      "Training progress in epoch #8, step 82, discriminator loss=0.682 , generator loss=0.749\n",
      "Training progress in epoch #8, step 83, discriminator loss=0.680 , generator loss=0.759\n",
      "Training progress in epoch #8, step 84, discriminator loss=0.688 , generator loss=0.761\n",
      "Training progress in epoch #8, step 85, discriminator loss=0.679 , generator loss=0.749\n",
      "Training progress in epoch #8, step 86, discriminator loss=0.679 , generator loss=0.749\n",
      "Training progress in epoch #8, step 87, discriminator loss=0.685 , generator loss=0.736\n",
      "Training progress in epoch #8, step 88, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #8, step 89, discriminator loss=0.675 , generator loss=0.743\n",
      "Training progress in epoch #8, step 90, discriminator loss=0.673 , generator loss=0.721\n",
      "Training progress in epoch #8, step 91, discriminator loss=0.679 , generator loss=0.710\n",
      "Training progress in epoch #8, step 92, discriminator loss=0.678 , generator loss=0.706\n",
      "Training progress in epoch #8, step 93, discriminator loss=0.683 , generator loss=0.692\n",
      "Training progress in epoch #8, step 94, discriminator loss=0.676 , generator loss=0.704\n",
      "Training progress in epoch #8, step 95, discriminator loss=0.676 , generator loss=0.725\n",
      "Training progress in epoch #8, step 96, discriminator loss=0.672 , generator loss=0.741\n",
      "Training progress in epoch #8, step 97, discriminator loss=0.667 , generator loss=0.733\n",
      "Training progress in epoch #8, step 98, discriminator loss=0.677 , generator loss=0.714\n",
      "Training progress in epoch #8, step 99, discriminator loss=0.669 , generator loss=0.712\n",
      "Training progress in epoch #8, step 100, discriminator loss=0.678 , generator loss=0.722\n",
      "Training progress in epoch #8, step 101, discriminator loss=0.672 , generator loss=0.727\n",
      "Training progress in epoch #8, step 102, discriminator loss=0.680 , generator loss=0.741\n",
      "Training progress in epoch #8, step 103, discriminator loss=0.687 , generator loss=0.741\n",
      "Training progress in epoch #8, step 104, discriminator loss=0.678 , generator loss=0.727\n",
      "Training progress in epoch #8, step 105, discriminator loss=0.680 , generator loss=0.725\n",
      "Training progress in epoch #8, step 106, discriminator loss=0.679 , generator loss=0.716\n",
      "Training progress in epoch #8, step 107, discriminator loss=0.684 , generator loss=0.696\n",
      "Training progress in epoch #8, step 108, discriminator loss=0.679 , generator loss=0.706\n",
      "Training progress in epoch #8, step 109, discriminator loss=0.683 , generator loss=0.711\n",
      "Training progress in epoch #8, step 110, discriminator loss=0.682 , generator loss=0.685\n",
      "Training progress in epoch #8, step 111, discriminator loss=0.683 , generator loss=0.693\n",
      "Training progress in epoch #8, step 112, discriminator loss=0.681 , generator loss=0.717\n",
      "Training progress in epoch #8, step 113, discriminator loss=0.678 , generator loss=0.712\n",
      "Training progress in epoch #8, step 114, discriminator loss=0.681 , generator loss=0.715\n",
      "Training progress in epoch #8, step 115, discriminator loss=0.681 , generator loss=0.701\n",
      "Training progress in epoch #8, step 116, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #8, step 117, discriminator loss=0.684 , generator loss=0.690\n",
      "Training progress in epoch #8, step 118, discriminator loss=0.682 , generator loss=0.704\n",
      "Training progress in epoch #8, step 119, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #8, step 120, discriminator loss=0.685 , generator loss=0.723\n",
      "Training progress in epoch #8, step 121, discriminator loss=0.684 , generator loss=0.741\n",
      "Training progress in epoch #8, step 122, discriminator loss=0.692 , generator loss=0.758\n",
      "Training progress in epoch #8, step 123, discriminator loss=0.671 , generator loss=0.754\n",
      "Training progress in epoch #8, step 124, discriminator loss=0.673 , generator loss=0.761\n",
      "Training progress in epoch #8, step 125, discriminator loss=0.678 , generator loss=0.759\n",
      "Training progress in epoch #8, step 126, discriminator loss=0.676 , generator loss=0.752\n",
      "Training progress in epoch #8, step 127, discriminator loss=0.673 , generator loss=0.749\n",
      "Training progress in epoch #8, step 128, discriminator loss=0.669 , generator loss=0.740\n",
      "Training progress in epoch #8, step 129, discriminator loss=0.671 , generator loss=0.735\n",
      "Training progress in epoch #8, step 130, discriminator loss=0.660 , generator loss=0.756\n",
      "Training progress in epoch #8, step 131, discriminator loss=0.661 , generator loss=0.737\n",
      "Training progress in epoch #8, step 132, discriminator loss=0.666 , generator loss=0.722\n",
      "Training progress in epoch #8, step 133, discriminator loss=0.659 , generator loss=0.723\n",
      "Training progress in epoch #8, step 134, discriminator loss=0.654 , generator loss=0.732\n",
      "Training progress in epoch #8, step 135, discriminator loss=0.664 , generator loss=0.744\n",
      "Training progress in epoch #8, step 136, discriminator loss=0.657 , generator loss=0.739\n",
      "Training progress in epoch #8, step 137, discriminator loss=0.654 , generator loss=0.739\n",
      "Training progress in epoch #8, step 138, discriminator loss=0.667 , generator loss=0.751\n",
      "Training progress in epoch #8, step 139, discriminator loss=0.669 , generator loss=0.762\n",
      "Training progress in epoch #8, step 140, discriminator loss=0.676 , generator loss=0.779\n",
      "Training progress in epoch #8, step 141, discriminator loss=0.676 , generator loss=0.763\n",
      "Training progress in epoch #8, step 142, discriminator loss=0.683 , generator loss=0.743\n",
      "Training progress in epoch #8, step 143, discriminator loss=0.691 , generator loss=0.748\n",
      "Training progress in epoch #8, step 144, discriminator loss=0.692 , generator loss=0.757\n",
      "Training progress in epoch #8, step 145, discriminator loss=0.690 , generator loss=0.734\n",
      "Training progress in epoch #8, step 146, discriminator loss=0.693 , generator loss=0.762\n",
      "Training progress in epoch #8, step 147, discriminator loss=0.693 , generator loss=0.753\n",
      "Training progress in epoch #8, step 148, discriminator loss=0.702 , generator loss=0.715\n",
      "Training progress in epoch #8, step 149, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #8, step 150, discriminator loss=0.699 , generator loss=0.691\n",
      "Training progress in epoch #8, step 151, discriminator loss=0.702 , generator loss=0.696\n",
      "Training progress in epoch #8, step 152, discriminator loss=0.701 , generator loss=0.678\n",
      "Training progress in epoch #8, step 153, discriminator loss=0.703 , generator loss=0.667\n",
      "Training progress in epoch #8, step 154, discriminator loss=0.693 , generator loss=0.672\n",
      "Training progress in epoch #8, step 155, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #8, step 156, discriminator loss=0.682 , generator loss=0.695\n",
      "Training progress in epoch #8, step 157, discriminator loss=0.675 , generator loss=0.721\n",
      "Training progress in epoch #8, step 158, discriminator loss=0.679 , generator loss=0.749\n",
      "Training progress in epoch #8, step 159, discriminator loss=0.667 , generator loss=0.758\n",
      "Training progress in epoch #8, step 160, discriminator loss=0.658 , generator loss=0.755\n",
      "Training progress in epoch #8, step 161, discriminator loss=0.664 , generator loss=0.761\n",
      "Training progress in epoch #8, step 162, discriminator loss=0.668 , generator loss=0.792\n",
      "Training progress in epoch #8, step 163, discriminator loss=0.653 , generator loss=0.821\n",
      "Training progress in epoch #8, step 164, discriminator loss=0.654 , generator loss=0.829\n",
      "Training progress in epoch #8, step 165, discriminator loss=0.659 , generator loss=0.811\n",
      "Training progress in epoch #8, step 166, discriminator loss=0.644 , generator loss=0.790\n",
      "Training progress in epoch #8, step 167, discriminator loss=0.656 , generator loss=0.796\n",
      "Training progress in epoch #8, step 168, discriminator loss=0.648 , generator loss=0.787\n",
      "Training progress in epoch #8, step 169, discriminator loss=0.653 , generator loss=0.772\n",
      "Training progress in epoch #8, step 170, discriminator loss=0.676 , generator loss=0.744\n",
      "Training progress in epoch #8, step 171, discriminator loss=0.674 , generator loss=0.727\n",
      "Training progress in epoch #8, step 172, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #8, step 173, discriminator loss=0.690 , generator loss=0.689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #8, step 174, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #8, step 175, discriminator loss=0.695 , generator loss=0.677\n",
      "Training progress in epoch #8, step 176, discriminator loss=0.708 , generator loss=0.676\n",
      "Training progress in epoch #8, step 177, discriminator loss=0.703 , generator loss=0.680\n",
      "Training progress in epoch #8, step 178, discriminator loss=0.709 , generator loss=0.690\n",
      "Training progress in epoch #8, step 179, discriminator loss=0.705 , generator loss=0.692\n",
      "Training progress in epoch #8, step 180, discriminator loss=0.700 , generator loss=0.700\n",
      "Training progress in epoch #8, step 181, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #8, step 182, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #8, step 183, discriminator loss=0.700 , generator loss=0.718\n",
      "Training progress in epoch #8, step 184, discriminator loss=0.697 , generator loss=0.726\n",
      "Training progress in epoch #8, step 185, discriminator loss=0.691 , generator loss=0.739\n",
      "Training progress in epoch #8, step 186, discriminator loss=0.686 , generator loss=0.758\n",
      "Training progress in epoch #8, step 187, discriminator loss=0.676 , generator loss=0.754\n",
      "Training progress in epoch #8, step 188, discriminator loss=0.674 , generator loss=0.780\n",
      "Training progress in epoch #8, step 189, discriminator loss=0.675 , generator loss=0.767\n",
      "Training progress in epoch #8, step 190, discriminator loss=0.675 , generator loss=0.752\n",
      "Training progress in epoch #8, step 191, discriminator loss=0.665 , generator loss=0.741\n",
      "Training progress in epoch #8, step 192, discriminator loss=0.663 , generator loss=0.740\n",
      "Training progress in epoch #8, step 193, discriminator loss=0.660 , generator loss=0.725\n",
      "Training progress in epoch #8, step 194, discriminator loss=0.660 , generator loss=0.734\n",
      "Training progress in epoch #8, step 195, discriminator loss=0.659 , generator loss=0.738\n",
      "Training progress in epoch #8, step 196, discriminator loss=0.657 , generator loss=0.744\n",
      "Training progress in epoch #8, step 197, discriminator loss=0.654 , generator loss=0.721\n",
      "Training progress in epoch #8, step 198, discriminator loss=0.657 , generator loss=0.723\n",
      "Training progress in epoch #8, step 199, discriminator loss=0.648 , generator loss=0.727\n",
      "Training progress in epoch #8, step 200, discriminator loss=0.657 , generator loss=0.746\n",
      "Training progress in epoch #8, step 201, discriminator loss=0.654 , generator loss=0.748\n",
      "Training progress in epoch #8, step 202, discriminator loss=0.658 , generator loss=0.736\n",
      "Training progress in epoch #8, step 203, discriminator loss=0.663 , generator loss=0.753\n",
      "Training progress in epoch #8, step 204, discriminator loss=0.651 , generator loss=0.768\n",
      "Training progress in epoch #8, step 205, discriminator loss=0.671 , generator loss=0.735\n",
      "Training progress in epoch #8, step 206, discriminator loss=0.664 , generator loss=0.754\n",
      "Training progress in epoch #8, step 207, discriminator loss=0.676 , generator loss=0.767\n",
      "Training progress in epoch #8, step 208, discriminator loss=0.670 , generator loss=0.752\n",
      "Training progress in epoch #8, step 209, discriminator loss=0.668 , generator loss=0.737\n",
      "Training progress in epoch #8, step 210, discriminator loss=0.672 , generator loss=0.746\n",
      "Training progress in epoch #8, step 211, discriminator loss=0.668 , generator loss=0.740\n",
      "Training progress in epoch #8, step 212, discriminator loss=0.676 , generator loss=0.751\n",
      "Training progress in epoch #8, step 213, discriminator loss=0.681 , generator loss=0.717\n",
      "Training progress in epoch #8, step 214, discriminator loss=0.681 , generator loss=0.709\n",
      "Training progress in epoch #8, step 215, discriminator loss=0.678 , generator loss=0.699\n",
      "Training progress in epoch #8, step 216, discriminator loss=0.679 , generator loss=0.684\n",
      "Training progress in epoch #8, step 217, discriminator loss=0.681 , generator loss=0.690\n",
      "Training progress in epoch #8, step 218, discriminator loss=0.678 , generator loss=0.709\n",
      "Training progress in epoch #8, step 219, discriminator loss=0.681 , generator loss=0.706\n",
      "Training progress in epoch #8, step 220, discriminator loss=0.678 , generator loss=0.702\n",
      "Training progress in epoch #8, step 221, discriminator loss=0.676 , generator loss=0.717\n",
      "Training progress in epoch #8, step 222, discriminator loss=0.688 , generator loss=0.731\n",
      "Training progress in epoch #8, step 223, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #8, step 224, discriminator loss=0.690 , generator loss=0.732\n",
      "Training progress in epoch #8, step 225, discriminator loss=0.695 , generator loss=0.760\n",
      "Training progress in epoch #8, step 226, discriminator loss=0.696 , generator loss=0.762\n",
      "Training progress in epoch #8, step 227, discriminator loss=0.687 , generator loss=0.763\n",
      "Training progress in epoch #8, step 228, discriminator loss=0.697 , generator loss=0.749\n",
      "Training progress in epoch #8, step 229, discriminator loss=0.689 , generator loss=0.763\n",
      "Training progress in epoch #8, step 230, discriminator loss=0.680 , generator loss=0.750\n",
      "Training progress in epoch #8, step 231, discriminator loss=0.691 , generator loss=0.733\n",
      "Training progress in epoch #8, step 232, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #8, step 233, discriminator loss=0.682 , generator loss=0.717\n",
      "Disciminator Accuracy on real images: 56%, on fake images: 60%\n",
      "Training progress in epoch #9, step 0, discriminator loss=0.698 , generator loss=0.699\n",
      "Training progress in epoch #9, step 1, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #9, step 2, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #9, step 3, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #9, step 4, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #9, step 5, discriminator loss=0.672 , generator loss=0.719\n",
      "Training progress in epoch #9, step 6, discriminator loss=0.676 , generator loss=0.717\n",
      "Training progress in epoch #9, step 7, discriminator loss=0.666 , generator loss=0.726\n",
      "Training progress in epoch #9, step 8, discriminator loss=0.663 , generator loss=0.756\n",
      "Training progress in epoch #9, step 9, discriminator loss=0.668 , generator loss=0.776\n",
      "Training progress in epoch #9, step 10, discriminator loss=0.666 , generator loss=0.786\n",
      "Training progress in epoch #9, step 11, discriminator loss=0.669 , generator loss=0.802\n",
      "Training progress in epoch #9, step 12, discriminator loss=0.656 , generator loss=0.788\n",
      "Training progress in epoch #9, step 13, discriminator loss=0.662 , generator loss=0.780\n",
      "Training progress in epoch #9, step 14, discriminator loss=0.657 , generator loss=0.814\n",
      "Training progress in epoch #9, step 15, discriminator loss=0.665 , generator loss=0.819\n",
      "Training progress in epoch #9, step 16, discriminator loss=0.646 , generator loss=0.824\n",
      "Training progress in epoch #9, step 17, discriminator loss=0.669 , generator loss=0.792\n",
      "Training progress in epoch #9, step 18, discriminator loss=0.669 , generator loss=0.758\n",
      "Training progress in epoch #9, step 19, discriminator loss=0.667 , generator loss=0.752\n",
      "Training progress in epoch #9, step 20, discriminator loss=0.675 , generator loss=0.774\n",
      "Training progress in epoch #9, step 21, discriminator loss=0.670 , generator loss=0.761\n",
      "Training progress in epoch #9, step 22, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #9, step 23, discriminator loss=0.700 , generator loss=0.679\n",
      "Training progress in epoch #9, step 24, discriminator loss=0.699 , generator loss=0.673\n",
      "Training progress in epoch #9, step 25, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #9, step 26, discriminator loss=0.706 , generator loss=0.699\n",
      "Training progress in epoch #9, step 27, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #9, step 28, discriminator loss=0.698 , generator loss=0.668\n",
      "Training progress in epoch #9, step 29, discriminator loss=0.698 , generator loss=0.675\n",
      "Training progress in epoch #9, step 30, discriminator loss=0.703 , generator loss=0.697\n",
      "Training progress in epoch #9, step 31, discriminator loss=0.696 , generator loss=0.699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #9, step 32, discriminator loss=0.700 , generator loss=0.713\n",
      "Training progress in epoch #9, step 33, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #9, step 34, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #9, step 35, discriminator loss=0.686 , generator loss=0.739\n",
      "Training progress in epoch #9, step 36, discriminator loss=0.666 , generator loss=0.778\n",
      "Training progress in epoch #9, step 37, discriminator loss=0.670 , generator loss=0.777\n",
      "Training progress in epoch #9, step 38, discriminator loss=0.666 , generator loss=0.766\n",
      "Training progress in epoch #9, step 39, discriminator loss=0.663 , generator loss=0.741\n",
      "Training progress in epoch #9, step 40, discriminator loss=0.664 , generator loss=0.771\n",
      "Training progress in epoch #9, step 41, discriminator loss=0.671 , generator loss=0.754\n",
      "Training progress in epoch #9, step 42, discriminator loss=0.677 , generator loss=0.756\n",
      "Training progress in epoch #9, step 43, discriminator loss=0.673 , generator loss=0.714\n",
      "Training progress in epoch #9, step 44, discriminator loss=0.676 , generator loss=0.681\n",
      "Training progress in epoch #9, step 45, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #9, step 46, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #9, step 47, discriminator loss=0.707 , generator loss=0.670\n",
      "Training progress in epoch #9, step 48, discriminator loss=0.702 , generator loss=0.675\n",
      "Training progress in epoch #9, step 49, discriminator loss=0.699 , generator loss=0.672\n",
      "Training progress in epoch #9, step 50, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #9, step 51, discriminator loss=0.698 , generator loss=0.695\n",
      "Training progress in epoch #9, step 52, discriminator loss=0.702 , generator loss=0.700\n",
      "Training progress in epoch #9, step 53, discriminator loss=0.682 , generator loss=0.696\n",
      "Training progress in epoch #9, step 54, discriminator loss=0.684 , generator loss=0.732\n",
      "Training progress in epoch #9, step 55, discriminator loss=0.688 , generator loss=0.752\n",
      "Training progress in epoch #9, step 56, discriminator loss=0.679 , generator loss=0.778\n",
      "Training progress in epoch #9, step 57, discriminator loss=0.675 , generator loss=0.787\n",
      "Training progress in epoch #9, step 58, discriminator loss=0.677 , generator loss=0.791\n",
      "Training progress in epoch #9, step 59, discriminator loss=0.674 , generator loss=0.785\n",
      "Training progress in epoch #9, step 60, discriminator loss=0.668 , generator loss=0.787\n",
      "Training progress in epoch #9, step 61, discriminator loss=0.661 , generator loss=0.800\n",
      "Training progress in epoch #9, step 62, discriminator loss=0.662 , generator loss=0.779\n",
      "Training progress in epoch #9, step 63, discriminator loss=0.661 , generator loss=0.803\n",
      "Training progress in epoch #9, step 64, discriminator loss=0.663 , generator loss=0.810\n",
      "Training progress in epoch #9, step 65, discriminator loss=0.665 , generator loss=0.774\n",
      "Training progress in epoch #9, step 66, discriminator loss=0.676 , generator loss=0.728\n",
      "Training progress in epoch #9, step 67, discriminator loss=0.669 , generator loss=0.712\n",
      "Training progress in epoch #9, step 68, discriminator loss=0.682 , generator loss=0.700\n",
      "Training progress in epoch #9, step 69, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #9, step 70, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #9, step 71, discriminator loss=0.701 , generator loss=0.674\n",
      "Training progress in epoch #9, step 72, discriminator loss=0.699 , generator loss=0.671\n",
      "Training progress in epoch #9, step 73, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #9, step 74, discriminator loss=0.682 , generator loss=0.677\n",
      "Training progress in epoch #9, step 75, discriminator loss=0.690 , generator loss=0.660\n",
      "Training progress in epoch #9, step 76, discriminator loss=0.683 , generator loss=0.698\n",
      "Training progress in epoch #9, step 77, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #9, step 78, discriminator loss=0.694 , generator loss=0.733\n",
      "Training progress in epoch #9, step 79, discriminator loss=0.698 , generator loss=0.740\n",
      "Training progress in epoch #9, step 80, discriminator loss=0.695 , generator loss=0.743\n",
      "Training progress in epoch #9, step 81, discriminator loss=0.690 , generator loss=0.758\n",
      "Training progress in epoch #9, step 82, discriminator loss=0.685 , generator loss=0.803\n",
      "Training progress in epoch #9, step 83, discriminator loss=0.684 , generator loss=0.824\n",
      "Training progress in epoch #9, step 84, discriminator loss=0.675 , generator loss=0.825\n",
      "Training progress in epoch #9, step 85, discriminator loss=0.654 , generator loss=0.814\n",
      "Training progress in epoch #9, step 86, discriminator loss=0.650 , generator loss=0.816\n",
      "Training progress in epoch #9, step 87, discriminator loss=0.659 , generator loss=0.800\n",
      "Training progress in epoch #9, step 88, discriminator loss=0.656 , generator loss=0.781\n",
      "Training progress in epoch #9, step 89, discriminator loss=0.663 , generator loss=0.759\n",
      "Training progress in epoch #9, step 90, discriminator loss=0.663 , generator loss=0.723\n",
      "Training progress in epoch #9, step 91, discriminator loss=0.670 , generator loss=0.719\n",
      "Training progress in epoch #9, step 92, discriminator loss=0.670 , generator loss=0.711\n",
      "Training progress in epoch #9, step 93, discriminator loss=0.682 , generator loss=0.684\n",
      "Training progress in epoch #9, step 94, discriminator loss=0.672 , generator loss=0.663\n",
      "Training progress in epoch #9, step 95, discriminator loss=0.678 , generator loss=0.662\n",
      "Training progress in epoch #9, step 96, discriminator loss=0.677 , generator loss=0.666\n",
      "Training progress in epoch #9, step 97, discriminator loss=0.678 , generator loss=0.694\n",
      "Training progress in epoch #9, step 98, discriminator loss=0.668 , generator loss=0.704\n",
      "Training progress in epoch #9, step 99, discriminator loss=0.673 , generator loss=0.695\n",
      "Training progress in epoch #9, step 100, discriminator loss=0.670 , generator loss=0.708\n",
      "Training progress in epoch #9, step 101, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #9, step 102, discriminator loss=0.686 , generator loss=0.749\n",
      "Training progress in epoch #9, step 103, discriminator loss=0.701 , generator loss=0.773\n",
      "Training progress in epoch #9, step 104, discriminator loss=0.690 , generator loss=0.760\n",
      "Training progress in epoch #9, step 105, discriminator loss=0.703 , generator loss=0.756\n",
      "Training progress in epoch #9, step 106, discriminator loss=0.696 , generator loss=0.801\n",
      "Training progress in epoch #9, step 107, discriminator loss=0.686 , generator loss=0.806\n",
      "Training progress in epoch #9, step 108, discriminator loss=0.685 , generator loss=0.801\n",
      "Training progress in epoch #9, step 109, discriminator loss=0.673 , generator loss=0.809\n",
      "Training progress in epoch #9, step 110, discriminator loss=0.670 , generator loss=0.775\n",
      "Training progress in epoch #9, step 111, discriminator loss=0.675 , generator loss=0.795\n",
      "Training progress in epoch #9, step 112, discriminator loss=0.663 , generator loss=0.798\n",
      "Training progress in epoch #9, step 113, discriminator loss=0.673 , generator loss=0.748\n",
      "Training progress in epoch #9, step 114, discriminator loss=0.668 , generator loss=0.758\n",
      "Training progress in epoch #9, step 115, discriminator loss=0.675 , generator loss=0.709\n",
      "Training progress in epoch #9, step 116, discriminator loss=0.679 , generator loss=0.673\n",
      "Training progress in epoch #9, step 117, discriminator loss=0.678 , generator loss=0.663\n",
      "Training progress in epoch #9, step 118, discriminator loss=0.673 , generator loss=0.682\n",
      "Training progress in epoch #9, step 119, discriminator loss=0.675 , generator loss=0.689\n",
      "Training progress in epoch #9, step 120, discriminator loss=0.679 , generator loss=0.669\n",
      "Training progress in epoch #9, step 121, discriminator loss=0.679 , generator loss=0.673\n",
      "Training progress in epoch #9, step 122, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #9, step 123, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #9, step 124, discriminator loss=0.699 , generator loss=0.718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #9, step 125, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #9, step 126, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #9, step 127, discriminator loss=0.695 , generator loss=0.734\n",
      "Training progress in epoch #9, step 128, discriminator loss=0.691 , generator loss=0.755\n",
      "Training progress in epoch #9, step 129, discriminator loss=0.694 , generator loss=0.755\n",
      "Training progress in epoch #9, step 130, discriminator loss=0.680 , generator loss=0.759\n",
      "Training progress in epoch #9, step 131, discriminator loss=0.690 , generator loss=0.742\n",
      "Training progress in epoch #9, step 132, discriminator loss=0.667 , generator loss=0.745\n",
      "Training progress in epoch #9, step 133, discriminator loss=0.675 , generator loss=0.750\n",
      "Training progress in epoch #9, step 134, discriminator loss=0.660 , generator loss=0.739\n",
      "Training progress in epoch #9, step 135, discriminator loss=0.674 , generator loss=0.714\n",
      "Training progress in epoch #9, step 136, discriminator loss=0.662 , generator loss=0.721\n",
      "Training progress in epoch #9, step 137, discriminator loss=0.661 , generator loss=0.714\n",
      "Training progress in epoch #9, step 138, discriminator loss=0.652 , generator loss=0.708\n",
      "Training progress in epoch #9, step 139, discriminator loss=0.654 , generator loss=0.710\n",
      "Training progress in epoch #9, step 140, discriminator loss=0.656 , generator loss=0.703\n",
      "Training progress in epoch #9, step 141, discriminator loss=0.658 , generator loss=0.708\n",
      "Training progress in epoch #9, step 142, discriminator loss=0.654 , generator loss=0.724\n",
      "Training progress in epoch #9, step 143, discriminator loss=0.648 , generator loss=0.725\n",
      "Training progress in epoch #9, step 144, discriminator loss=0.648 , generator loss=0.749\n",
      "Training progress in epoch #9, step 145, discriminator loss=0.666 , generator loss=0.751\n",
      "Training progress in epoch #9, step 146, discriminator loss=0.653 , generator loss=0.750\n",
      "Training progress in epoch #9, step 147, discriminator loss=0.664 , generator loss=0.747\n",
      "Training progress in epoch #9, step 148, discriminator loss=0.668 , generator loss=0.748\n",
      "Training progress in epoch #9, step 149, discriminator loss=0.674 , generator loss=0.758\n",
      "Training progress in epoch #9, step 150, discriminator loss=0.685 , generator loss=0.767\n",
      "Training progress in epoch #9, step 151, discriminator loss=0.692 , generator loss=0.759\n",
      "Training progress in epoch #9, step 152, discriminator loss=0.688 , generator loss=0.749\n",
      "Training progress in epoch #9, step 153, discriminator loss=0.698 , generator loss=0.777\n",
      "Training progress in epoch #9, step 154, discriminator loss=0.698 , generator loss=0.740\n",
      "Training progress in epoch #9, step 155, discriminator loss=0.700 , generator loss=0.749\n",
      "Training progress in epoch #9, step 156, discriminator loss=0.701 , generator loss=0.721\n",
      "Training progress in epoch #9, step 157, discriminator loss=0.704 , generator loss=0.701\n",
      "Training progress in epoch #9, step 158, discriminator loss=0.699 , generator loss=0.702\n",
      "Training progress in epoch #9, step 159, discriminator loss=0.704 , generator loss=0.709\n",
      "Training progress in epoch #9, step 160, discriminator loss=0.697 , generator loss=0.710\n",
      "Training progress in epoch #9, step 161, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #9, step 162, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #9, step 163, discriminator loss=0.684 , generator loss=0.746\n",
      "Training progress in epoch #9, step 164, discriminator loss=0.685 , generator loss=0.758\n",
      "Training progress in epoch #9, step 165, discriminator loss=0.672 , generator loss=0.726\n",
      "Training progress in epoch #9, step 166, discriminator loss=0.673 , generator loss=0.735\n",
      "Training progress in epoch #9, step 167, discriminator loss=0.671 , generator loss=0.750\n",
      "Training progress in epoch #9, step 168, discriminator loss=0.668 , generator loss=0.751\n",
      "Training progress in epoch #9, step 169, discriminator loss=0.665 , generator loss=0.778\n",
      "Training progress in epoch #9, step 170, discriminator loss=0.656 , generator loss=0.767\n",
      "Training progress in epoch #9, step 171, discriminator loss=0.655 , generator loss=0.767\n",
      "Training progress in epoch #9, step 172, discriminator loss=0.659 , generator loss=0.734\n",
      "Training progress in epoch #9, step 173, discriminator loss=0.674 , generator loss=0.740\n",
      "Training progress in epoch #9, step 174, discriminator loss=0.666 , generator loss=0.736\n",
      "Training progress in epoch #9, step 175, discriminator loss=0.671 , generator loss=0.755\n",
      "Training progress in epoch #9, step 176, discriminator loss=0.675 , generator loss=0.752\n",
      "Training progress in epoch #9, step 177, discriminator loss=0.677 , generator loss=0.745\n",
      "Training progress in epoch #9, step 178, discriminator loss=0.683 , generator loss=0.720\n",
      "Training progress in epoch #9, step 179, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #9, step 180, discriminator loss=0.680 , generator loss=0.726\n",
      "Training progress in epoch #9, step 181, discriminator loss=0.677 , generator loss=0.719\n",
      "Training progress in epoch #9, step 182, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #9, step 183, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #9, step 184, discriminator loss=0.684 , generator loss=0.696\n",
      "Training progress in epoch #9, step 185, discriminator loss=0.681 , generator loss=0.708\n",
      "Training progress in epoch #9, step 186, discriminator loss=0.676 , generator loss=0.728\n",
      "Training progress in epoch #9, step 187, discriminator loss=0.678 , generator loss=0.716\n",
      "Training progress in epoch #9, step 188, discriminator loss=0.672 , generator loss=0.709\n",
      "Training progress in epoch #9, step 189, discriminator loss=0.675 , generator loss=0.713\n",
      "Training progress in epoch #9, step 190, discriminator loss=0.668 , generator loss=0.728\n",
      "Training progress in epoch #9, step 191, discriminator loss=0.664 , generator loss=0.738\n",
      "Training progress in epoch #9, step 192, discriminator loss=0.659 , generator loss=0.735\n",
      "Training progress in epoch #9, step 193, discriminator loss=0.657 , generator loss=0.742\n",
      "Training progress in epoch #9, step 194, discriminator loss=0.668 , generator loss=0.751\n",
      "Training progress in epoch #9, step 195, discriminator loss=0.651 , generator loss=0.769\n",
      "Training progress in epoch #9, step 196, discriminator loss=0.660 , generator loss=0.758\n",
      "Training progress in epoch #9, step 197, discriminator loss=0.652 , generator loss=0.778\n",
      "Training progress in epoch #9, step 198, discriminator loss=0.658 , generator loss=0.735\n",
      "Training progress in epoch #9, step 199, discriminator loss=0.661 , generator loss=0.739\n",
      "Training progress in epoch #9, step 200, discriminator loss=0.680 , generator loss=0.747\n",
      "Training progress in epoch #9, step 201, discriminator loss=0.666 , generator loss=0.739\n",
      "Training progress in epoch #9, step 202, discriminator loss=0.675 , generator loss=0.714\n",
      "Training progress in epoch #9, step 203, discriminator loss=0.676 , generator loss=0.692\n",
      "Training progress in epoch #9, step 204, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #9, step 205, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #9, step 206, discriminator loss=0.697 , generator loss=0.692\n",
      "Training progress in epoch #9, step 207, discriminator loss=0.703 , generator loss=0.700\n",
      "Training progress in epoch #9, step 208, discriminator loss=0.709 , generator loss=0.681\n",
      "Training progress in epoch #9, step 209, discriminator loss=0.711 , generator loss=0.710\n",
      "Training progress in epoch #9, step 210, discriminator loss=0.712 , generator loss=0.705\n",
      "Training progress in epoch #9, step 211, discriminator loss=0.713 , generator loss=0.723\n",
      "Training progress in epoch #9, step 212, discriminator loss=0.708 , generator loss=0.732\n",
      "Training progress in epoch #9, step 213, discriminator loss=0.713 , generator loss=0.715\n",
      "Training progress in epoch #9, step 214, discriminator loss=0.705 , generator loss=0.720\n",
      "Training progress in epoch #9, step 215, discriminator loss=0.698 , generator loss=0.712\n",
      "Training progress in epoch #9, step 216, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #9, step 217, discriminator loss=0.683 , generator loss=0.750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #9, step 218, discriminator loss=0.673 , generator loss=0.754\n",
      "Training progress in epoch #9, step 219, discriminator loss=0.667 , generator loss=0.738\n",
      "Training progress in epoch #9, step 220, discriminator loss=0.663 , generator loss=0.756\n",
      "Training progress in epoch #9, step 221, discriminator loss=0.662 , generator loss=0.768\n",
      "Training progress in epoch #9, step 222, discriminator loss=0.660 , generator loss=0.754\n",
      "Training progress in epoch #9, step 223, discriminator loss=0.668 , generator loss=0.755\n",
      "Training progress in epoch #9, step 224, discriminator loss=0.663 , generator loss=0.711\n",
      "Training progress in epoch #9, step 225, discriminator loss=0.657 , generator loss=0.718\n",
      "Training progress in epoch #9, step 226, discriminator loss=0.661 , generator loss=0.722\n",
      "Training progress in epoch #9, step 227, discriminator loss=0.660 , generator loss=0.721\n",
      "Training progress in epoch #9, step 228, discriminator loss=0.666 , generator loss=0.732\n",
      "Training progress in epoch #9, step 229, discriminator loss=0.673 , generator loss=0.722\n",
      "Training progress in epoch #9, step 230, discriminator loss=0.668 , generator loss=0.733\n",
      "Training progress in epoch #9, step 231, discriminator loss=0.669 , generator loss=0.726\n",
      "Training progress in epoch #9, step 232, discriminator loss=0.665 , generator loss=0.720\n",
      "Training progress in epoch #9, step 233, discriminator loss=0.679 , generator loss=0.734\n",
      "Disciminator Accuracy on real images: 63%, on fake images: 68%\n",
      "Training progress in epoch #10, step 0, discriminator loss=0.678 , generator loss=0.738\n",
      "Training progress in epoch #10, step 1, discriminator loss=0.678 , generator loss=0.736\n",
      "Training progress in epoch #10, step 2, discriminator loss=0.686 , generator loss=0.741\n",
      "Training progress in epoch #10, step 3, discriminator loss=0.689 , generator loss=0.739\n",
      "Training progress in epoch #10, step 4, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #10, step 5, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #10, step 6, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #10, step 7, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #10, step 8, discriminator loss=0.694 , generator loss=0.726\n",
      "Training progress in epoch #10, step 9, discriminator loss=0.692 , generator loss=0.736\n",
      "Training progress in epoch #10, step 10, discriminator loss=0.675 , generator loss=0.735\n",
      "Training progress in epoch #10, step 11, discriminator loss=0.685 , generator loss=0.728\n",
      "Training progress in epoch #10, step 12, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #10, step 13, discriminator loss=0.676 , generator loss=0.727\n",
      "Training progress in epoch #10, step 14, discriminator loss=0.672 , generator loss=0.724\n",
      "Training progress in epoch #10, step 15, discriminator loss=0.679 , generator loss=0.703\n",
      "Training progress in epoch #10, step 16, discriminator loss=0.669 , generator loss=0.720\n",
      "Training progress in epoch #10, step 17, discriminator loss=0.672 , generator loss=0.733\n",
      "Training progress in epoch #10, step 18, discriminator loss=0.659 , generator loss=0.748\n",
      "Training progress in epoch #10, step 19, discriminator loss=0.660 , generator loss=0.723\n",
      "Training progress in epoch #10, step 20, discriminator loss=0.669 , generator loss=0.724\n",
      "Training progress in epoch #10, step 21, discriminator loss=0.666 , generator loss=0.736\n",
      "Training progress in epoch #10, step 22, discriminator loss=0.675 , generator loss=0.757\n",
      "Training progress in epoch #10, step 23, discriminator loss=0.681 , generator loss=0.734\n",
      "Training progress in epoch #10, step 24, discriminator loss=0.673 , generator loss=0.722\n",
      "Training progress in epoch #10, step 25, discriminator loss=0.679 , generator loss=0.752\n",
      "Training progress in epoch #10, step 26, discriminator loss=0.686 , generator loss=0.742\n",
      "Training progress in epoch #10, step 27, discriminator loss=0.681 , generator loss=0.705\n",
      "Training progress in epoch #10, step 28, discriminator loss=0.686 , generator loss=0.731\n",
      "Training progress in epoch #10, step 29, discriminator loss=0.695 , generator loss=0.737\n",
      "Training progress in epoch #10, step 30, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #10, step 31, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #10, step 32, discriminator loss=0.698 , generator loss=0.695\n",
      "Training progress in epoch #10, step 33, discriminator loss=0.696 , generator loss=0.680\n",
      "Training progress in epoch #10, step 34, discriminator loss=0.707 , generator loss=0.720\n",
      "Training progress in epoch #10, step 35, discriminator loss=0.683 , generator loss=0.714\n",
      "Training progress in epoch #10, step 36, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #10, step 37, discriminator loss=0.681 , generator loss=0.721\n",
      "Training progress in epoch #10, step 38, discriminator loss=0.680 , generator loss=0.705\n",
      "Training progress in epoch #10, step 39, discriminator loss=0.673 , generator loss=0.738\n",
      "Training progress in epoch #10, step 40, discriminator loss=0.671 , generator loss=0.754\n",
      "Training progress in epoch #10, step 41, discriminator loss=0.677 , generator loss=0.763\n",
      "Training progress in epoch #10, step 42, discriminator loss=0.674 , generator loss=0.741\n",
      "Training progress in epoch #10, step 43, discriminator loss=0.673 , generator loss=0.752\n",
      "Training progress in epoch #10, step 44, discriminator loss=0.671 , generator loss=0.731\n",
      "Training progress in epoch #10, step 45, discriminator loss=0.668 , generator loss=0.760\n",
      "Training progress in epoch #10, step 46, discriminator loss=0.660 , generator loss=0.796\n",
      "Training progress in epoch #10, step 47, discriminator loss=0.668 , generator loss=0.771\n",
      "Training progress in epoch #10, step 48, discriminator loss=0.661 , generator loss=0.759\n",
      "Training progress in epoch #10, step 49, discriminator loss=0.662 , generator loss=0.733\n",
      "Training progress in epoch #10, step 50, discriminator loss=0.658 , generator loss=0.726\n",
      "Training progress in epoch #10, step 51, discriminator loss=0.667 , generator loss=0.749\n",
      "Training progress in epoch #10, step 52, discriminator loss=0.671 , generator loss=0.739\n",
      "Training progress in epoch #10, step 53, discriminator loss=0.665 , generator loss=0.752\n",
      "Training progress in epoch #10, step 54, discriminator loss=0.680 , generator loss=0.740\n",
      "Training progress in epoch #10, step 55, discriminator loss=0.681 , generator loss=0.714\n",
      "Training progress in epoch #10, step 56, discriminator loss=0.682 , generator loss=0.725\n",
      "Training progress in epoch #10, step 57, discriminator loss=0.689 , generator loss=0.732\n",
      "Training progress in epoch #10, step 58, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #10, step 59, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #10, step 60, discriminator loss=0.699 , generator loss=0.720\n",
      "Training progress in epoch #10, step 61, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #10, step 62, discriminator loss=0.702 , generator loss=0.689\n",
      "Training progress in epoch #10, step 63, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #10, step 64, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #10, step 65, discriminator loss=0.677 , generator loss=0.704\n",
      "Training progress in epoch #10, step 66, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #10, step 67, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #10, step 68, discriminator loss=0.683 , generator loss=0.719\n",
      "Training progress in epoch #10, step 69, discriminator loss=0.670 , generator loss=0.713\n",
      "Training progress in epoch #10, step 70, discriminator loss=0.676 , generator loss=0.731\n",
      "Training progress in epoch #10, step 71, discriminator loss=0.673 , generator loss=0.740\n",
      "Training progress in epoch #10, step 72, discriminator loss=0.671 , generator loss=0.737\n",
      "Training progress in epoch #10, step 73, discriminator loss=0.672 , generator loss=0.733\n",
      "Training progress in epoch #10, step 74, discriminator loss=0.671 , generator loss=0.725\n",
      "Training progress in epoch #10, step 75, discriminator loss=0.665 , generator loss=0.728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #10, step 76, discriminator loss=0.663 , generator loss=0.741\n",
      "Training progress in epoch #10, step 77, discriminator loss=0.673 , generator loss=0.743\n",
      "Training progress in epoch #10, step 78, discriminator loss=0.662 , generator loss=0.722\n",
      "Training progress in epoch #10, step 79, discriminator loss=0.676 , generator loss=0.697\n",
      "Training progress in epoch #10, step 80, discriminator loss=0.669 , generator loss=0.712\n",
      "Training progress in epoch #10, step 81, discriminator loss=0.672 , generator loss=0.721\n",
      "Training progress in epoch #10, step 82, discriminator loss=0.665 , generator loss=0.726\n",
      "Training progress in epoch #10, step 83, discriminator loss=0.676 , generator loss=0.725\n",
      "Training progress in epoch #10, step 84, discriminator loss=0.673 , generator loss=0.718\n",
      "Training progress in epoch #10, step 85, discriminator loss=0.680 , generator loss=0.731\n",
      "Training progress in epoch #10, step 86, discriminator loss=0.681 , generator loss=0.714\n",
      "Training progress in epoch #10, step 87, discriminator loss=0.681 , generator loss=0.720\n",
      "Training progress in epoch #10, step 88, discriminator loss=0.681 , generator loss=0.722\n",
      "Training progress in epoch #10, step 89, discriminator loss=0.683 , generator loss=0.727\n",
      "Training progress in epoch #10, step 90, discriminator loss=0.697 , generator loss=0.735\n",
      "Training progress in epoch #10, step 91, discriminator loss=0.681 , generator loss=0.744\n",
      "Training progress in epoch #10, step 92, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #10, step 93, discriminator loss=0.691 , generator loss=0.731\n",
      "Training progress in epoch #10, step 94, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #10, step 95, discriminator loss=0.677 , generator loss=0.725\n",
      "Training progress in epoch #10, step 96, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #10, step 97, discriminator loss=0.681 , generator loss=0.704\n",
      "Training progress in epoch #10, step 98, discriminator loss=0.681 , generator loss=0.728\n",
      "Training progress in epoch #10, step 99, discriminator loss=0.682 , generator loss=0.756\n",
      "Training progress in epoch #10, step 100, discriminator loss=0.672 , generator loss=0.735\n",
      "Training progress in epoch #10, step 101, discriminator loss=0.677 , generator loss=0.724\n",
      "Training progress in epoch #10, step 102, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #10, step 103, discriminator loss=0.674 , generator loss=0.748\n",
      "Training progress in epoch #10, step 104, discriminator loss=0.673 , generator loss=0.755\n",
      "Training progress in epoch #10, step 105, discriminator loss=0.674 , generator loss=0.770\n",
      "Training progress in epoch #10, step 106, discriminator loss=0.672 , generator loss=0.752\n",
      "Training progress in epoch #10, step 107, discriminator loss=0.675 , generator loss=0.733\n",
      "Training progress in epoch #10, step 108, discriminator loss=0.666 , generator loss=0.735\n",
      "Training progress in epoch #10, step 109, discriminator loss=0.674 , generator loss=0.740\n",
      "Training progress in epoch #10, step 110, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #10, step 111, discriminator loss=0.678 , generator loss=0.685\n",
      "Training progress in epoch #10, step 112, discriminator loss=0.675 , generator loss=0.716\n",
      "Training progress in epoch #10, step 113, discriminator loss=0.684 , generator loss=0.742\n",
      "Training progress in epoch #10, step 114, discriminator loss=0.678 , generator loss=0.708\n",
      "Training progress in epoch #10, step 115, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #10, step 116, discriminator loss=0.686 , generator loss=0.683\n",
      "Training progress in epoch #10, step 117, discriminator loss=0.679 , generator loss=0.712\n",
      "Training progress in epoch #10, step 118, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #10, step 119, discriminator loss=0.681 , generator loss=0.704\n",
      "Training progress in epoch #10, step 120, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #10, step 121, discriminator loss=0.694 , generator loss=0.726\n",
      "Training progress in epoch #10, step 122, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #10, step 123, discriminator loss=0.678 , generator loss=0.718\n",
      "Training progress in epoch #10, step 124, discriminator loss=0.678 , generator loss=0.729\n",
      "Training progress in epoch #10, step 125, discriminator loss=0.684 , generator loss=0.721\n",
      "Training progress in epoch #10, step 126, discriminator loss=0.682 , generator loss=0.739\n",
      "Training progress in epoch #10, step 127, discriminator loss=0.680 , generator loss=0.738\n",
      "Training progress in epoch #10, step 128, discriminator loss=0.679 , generator loss=0.727\n",
      "Training progress in epoch #10, step 129, discriminator loss=0.675 , generator loss=0.694\n",
      "Training progress in epoch #10, step 130, discriminator loss=0.676 , generator loss=0.707\n",
      "Training progress in epoch #10, step 131, discriminator loss=0.666 , generator loss=0.722\n",
      "Training progress in epoch #10, step 132, discriminator loss=0.671 , generator loss=0.732\n",
      "Training progress in epoch #10, step 133, discriminator loss=0.670 , generator loss=0.719\n",
      "Training progress in epoch #10, step 134, discriminator loss=0.673 , generator loss=0.702\n",
      "Training progress in epoch #10, step 135, discriminator loss=0.671 , generator loss=0.714\n",
      "Training progress in epoch #10, step 136, discriminator loss=0.681 , generator loss=0.714\n",
      "Training progress in epoch #10, step 137, discriminator loss=0.666 , generator loss=0.721\n",
      "Training progress in epoch #10, step 138, discriminator loss=0.664 , generator loss=0.744\n",
      "Training progress in epoch #10, step 139, discriminator loss=0.666 , generator loss=0.745\n",
      "Training progress in epoch #10, step 140, discriminator loss=0.682 , generator loss=0.731\n",
      "Training progress in epoch #10, step 141, discriminator loss=0.680 , generator loss=0.708\n",
      "Training progress in epoch #10, step 142, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #10, step 143, discriminator loss=0.693 , generator loss=0.728\n",
      "Training progress in epoch #10, step 144, discriminator loss=0.704 , generator loss=0.718\n",
      "Training progress in epoch #10, step 145, discriminator loss=0.702 , generator loss=0.719\n",
      "Training progress in epoch #10, step 146, discriminator loss=0.696 , generator loss=0.721\n",
      "Training progress in epoch #10, step 147, discriminator loss=0.709 , generator loss=0.695\n",
      "Training progress in epoch #10, step 148, discriminator loss=0.706 , generator loss=0.719\n",
      "Training progress in epoch #10, step 149, discriminator loss=0.708 , generator loss=0.705\n",
      "Training progress in epoch #10, step 150, discriminator loss=0.711 , generator loss=0.704\n",
      "Training progress in epoch #10, step 151, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #10, step 152, discriminator loss=0.698 , generator loss=0.718\n",
      "Training progress in epoch #10, step 153, discriminator loss=0.681 , generator loss=0.744\n",
      "Training progress in epoch #10, step 154, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #10, step 155, discriminator loss=0.674 , generator loss=0.724\n",
      "Training progress in epoch #10, step 156, discriminator loss=0.680 , generator loss=0.733\n",
      "Training progress in epoch #10, step 157, discriminator loss=0.678 , generator loss=0.735\n",
      "Training progress in epoch #10, step 158, discriminator loss=0.683 , generator loss=0.744\n",
      "Training progress in epoch #10, step 159, discriminator loss=0.670 , generator loss=0.769\n",
      "Training progress in epoch #10, step 160, discriminator loss=0.669 , generator loss=0.791\n",
      "Training progress in epoch #10, step 161, discriminator loss=0.669 , generator loss=0.754\n",
      "Training progress in epoch #10, step 162, discriminator loss=0.672 , generator loss=0.734\n",
      "Training progress in epoch #10, step 163, discriminator loss=0.667 , generator loss=0.737\n",
      "Training progress in epoch #10, step 164, discriminator loss=0.667 , generator loss=0.741\n",
      "Training progress in epoch #10, step 165, discriminator loss=0.665 , generator loss=0.746\n",
      "Training progress in epoch #10, step 166, discriminator loss=0.680 , generator loss=0.729\n",
      "Training progress in epoch #10, step 167, discriminator loss=0.668 , generator loss=0.715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #10, step 168, discriminator loss=0.674 , generator loss=0.718\n",
      "Training progress in epoch #10, step 169, discriminator loss=0.678 , generator loss=0.724\n",
      "Training progress in epoch #10, step 170, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #10, step 171, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #10, step 172, discriminator loss=0.686 , generator loss=0.726\n",
      "Training progress in epoch #10, step 173, discriminator loss=0.698 , generator loss=0.714\n",
      "Training progress in epoch #10, step 174, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #10, step 175, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #10, step 176, discriminator loss=0.697 , generator loss=0.727\n",
      "Training progress in epoch #10, step 177, discriminator loss=0.698 , generator loss=0.736\n",
      "Training progress in epoch #10, step 178, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #10, step 179, discriminator loss=0.684 , generator loss=0.693\n",
      "Training progress in epoch #10, step 180, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #10, step 181, discriminator loss=0.689 , generator loss=0.742\n",
      "Training progress in epoch #10, step 182, discriminator loss=0.679 , generator loss=0.720\n",
      "Training progress in epoch #10, step 183, discriminator loss=0.671 , generator loss=0.720\n",
      "Training progress in epoch #10, step 184, discriminator loss=0.680 , generator loss=0.702\n",
      "Training progress in epoch #10, step 185, discriminator loss=0.673 , generator loss=0.707\n",
      "Training progress in epoch #10, step 186, discriminator loss=0.675 , generator loss=0.727\n",
      "Training progress in epoch #10, step 187, discriminator loss=0.668 , generator loss=0.715\n",
      "Training progress in epoch #10, step 188, discriminator loss=0.670 , generator loss=0.726\n",
      "Training progress in epoch #10, step 189, discriminator loss=0.672 , generator loss=0.719\n",
      "Training progress in epoch #10, step 190, discriminator loss=0.656 , generator loss=0.718\n",
      "Training progress in epoch #10, step 191, discriminator loss=0.663 , generator loss=0.717\n",
      "Training progress in epoch #10, step 192, discriminator loss=0.672 , generator loss=0.749\n",
      "Training progress in epoch #10, step 193, discriminator loss=0.679 , generator loss=0.751\n",
      "Training progress in epoch #10, step 194, discriminator loss=0.677 , generator loss=0.757\n",
      "Training progress in epoch #10, step 195, discriminator loss=0.686 , generator loss=0.740\n",
      "Training progress in epoch #10, step 196, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #10, step 197, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #10, step 198, discriminator loss=0.695 , generator loss=0.739\n",
      "Training progress in epoch #10, step 199, discriminator loss=0.697 , generator loss=0.766\n",
      "Training progress in epoch #10, step 200, discriminator loss=0.700 , generator loss=0.737\n",
      "Training progress in epoch #10, step 201, discriminator loss=0.710 , generator loss=0.701\n",
      "Training progress in epoch #10, step 202, discriminator loss=0.707 , generator loss=0.700\n",
      "Training progress in epoch #10, step 203, discriminator loss=0.703 , generator loss=0.699\n",
      "Training progress in epoch #10, step 204, discriminator loss=0.699 , generator loss=0.707\n",
      "Training progress in epoch #10, step 205, discriminator loss=0.698 , generator loss=0.690\n",
      "Training progress in epoch #10, step 206, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #10, step 207, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #10, step 208, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #10, step 209, discriminator loss=0.677 , generator loss=0.730\n",
      "Training progress in epoch #10, step 210, discriminator loss=0.675 , generator loss=0.729\n",
      "Training progress in epoch #10, step 211, discriminator loss=0.667 , generator loss=0.729\n",
      "Training progress in epoch #10, step 212, discriminator loss=0.673 , generator loss=0.751\n",
      "Training progress in epoch #10, step 213, discriminator loss=0.675 , generator loss=0.743\n",
      "Training progress in epoch #10, step 214, discriminator loss=0.673 , generator loss=0.760\n",
      "Training progress in epoch #10, step 215, discriminator loss=0.671 , generator loss=0.787\n",
      "Training progress in epoch #10, step 216, discriminator loss=0.664 , generator loss=0.771\n",
      "Training progress in epoch #10, step 217, discriminator loss=0.662 , generator loss=0.772\n",
      "Training progress in epoch #10, step 218, discriminator loss=0.651 , generator loss=0.778\n",
      "Training progress in epoch #10, step 219, discriminator loss=0.666 , generator loss=0.794\n",
      "Training progress in epoch #10, step 220, discriminator loss=0.668 , generator loss=0.762\n",
      "Training progress in epoch #10, step 221, discriminator loss=0.676 , generator loss=0.754\n",
      "Training progress in epoch #10, step 222, discriminator loss=0.673 , generator loss=0.741\n",
      "Training progress in epoch #10, step 223, discriminator loss=0.670 , generator loss=0.719\n",
      "Training progress in epoch #10, step 224, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #10, step 225, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #10, step 226, discriminator loss=0.689 , generator loss=0.655\n",
      "Training progress in epoch #10, step 227, discriminator loss=0.688 , generator loss=0.651\n",
      "Training progress in epoch #10, step 228, discriminator loss=0.684 , generator loss=0.666\n",
      "Training progress in epoch #10, step 229, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #10, step 230, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #10, step 231, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #10, step 232, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #10, step 233, discriminator loss=0.690 , generator loss=0.721\n",
      "Disciminator Accuracy on real images: 54%, on fake images: 55%\n",
      "Training progress in epoch #11, step 0, discriminator loss=0.697 , generator loss=0.719\n",
      "Training progress in epoch #11, step 1, discriminator loss=0.697 , generator loss=0.740\n",
      "Training progress in epoch #11, step 2, discriminator loss=0.698 , generator loss=0.767\n",
      "Training progress in epoch #11, step 3, discriminator loss=0.700 , generator loss=0.780\n",
      "Training progress in epoch #11, step 4, discriminator loss=0.684 , generator loss=0.772\n",
      "Training progress in epoch #11, step 5, discriminator loss=0.687 , generator loss=0.751\n",
      "Training progress in epoch #11, step 6, discriminator loss=0.670 , generator loss=0.753\n",
      "Training progress in epoch #11, step 7, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #11, step 8, discriminator loss=0.675 , generator loss=0.733\n",
      "Training progress in epoch #11, step 9, discriminator loss=0.680 , generator loss=0.694\n",
      "Training progress in epoch #11, step 10, discriminator loss=0.672 , generator loss=0.680\n",
      "Training progress in epoch #11, step 11, discriminator loss=0.680 , generator loss=0.670\n",
      "Training progress in epoch #11, step 12, discriminator loss=0.669 , generator loss=0.701\n",
      "Training progress in epoch #11, step 13, discriminator loss=0.662 , generator loss=0.711\n",
      "Training progress in epoch #11, step 14, discriminator loss=0.666 , generator loss=0.712\n",
      "Training progress in epoch #11, step 15, discriminator loss=0.666 , generator loss=0.707\n",
      "Training progress in epoch #11, step 16, discriminator loss=0.658 , generator loss=0.710\n",
      "Training progress in epoch #11, step 17, discriminator loss=0.667 , generator loss=0.725\n",
      "Training progress in epoch #11, step 18, discriminator loss=0.669 , generator loss=0.743\n",
      "Training progress in epoch #11, step 19, discriminator loss=0.693 , generator loss=0.740\n",
      "Training progress in epoch #11, step 20, discriminator loss=0.694 , generator loss=0.766\n",
      "Training progress in epoch #11, step 21, discriminator loss=0.696 , generator loss=0.761\n",
      "Training progress in epoch #11, step 22, discriminator loss=0.703 , generator loss=0.779\n",
      "Training progress in epoch #11, step 23, discriminator loss=0.704 , generator loss=0.778\n",
      "Training progress in epoch #11, step 24, discriminator loss=0.702 , generator loss=0.761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #11, step 25, discriminator loss=0.691 , generator loss=0.775\n",
      "Training progress in epoch #11, step 26, discriminator loss=0.693 , generator loss=0.763\n",
      "Training progress in epoch #11, step 27, discriminator loss=0.689 , generator loss=0.732\n",
      "Training progress in epoch #11, step 28, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #11, step 29, discriminator loss=0.699 , generator loss=0.686\n",
      "Training progress in epoch #11, step 30, discriminator loss=0.702 , generator loss=0.662\n",
      "Training progress in epoch #11, step 31, discriminator loss=0.698 , generator loss=0.678\n",
      "Training progress in epoch #11, step 32, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #11, step 33, discriminator loss=0.698 , generator loss=0.673\n",
      "Training progress in epoch #11, step 34, discriminator loss=0.678 , generator loss=0.685\n",
      "Training progress in epoch #11, step 35, discriminator loss=0.674 , generator loss=0.712\n",
      "Training progress in epoch #11, step 36, discriminator loss=0.675 , generator loss=0.734\n",
      "Training progress in epoch #11, step 37, discriminator loss=0.661 , generator loss=0.726\n",
      "Training progress in epoch #11, step 38, discriminator loss=0.671 , generator loss=0.760\n",
      "Training progress in epoch #11, step 39, discriminator loss=0.675 , generator loss=0.790\n",
      "Training progress in epoch #11, step 40, discriminator loss=0.672 , generator loss=0.784\n",
      "Training progress in epoch #11, step 41, discriminator loss=0.679 , generator loss=0.817\n",
      "Training progress in epoch #11, step 42, discriminator loss=0.679 , generator loss=0.808\n",
      "Training progress in epoch #11, step 43, discriminator loss=0.671 , generator loss=0.825\n",
      "Training progress in epoch #11, step 44, discriminator loss=0.672 , generator loss=0.793\n",
      "Training progress in epoch #11, step 45, discriminator loss=0.666 , generator loss=0.780\n",
      "Training progress in epoch #11, step 46, discriminator loss=0.671 , generator loss=0.777\n",
      "Training progress in epoch #11, step 47, discriminator loss=0.682 , generator loss=0.739\n",
      "Training progress in epoch #11, step 48, discriminator loss=0.678 , generator loss=0.716\n",
      "Training progress in epoch #11, step 49, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #11, step 50, discriminator loss=0.702 , generator loss=0.663\n",
      "Training progress in epoch #11, step 51, discriminator loss=0.709 , generator loss=0.644\n",
      "Training progress in epoch #11, step 52, discriminator loss=0.702 , generator loss=0.634\n",
      "Training progress in epoch #11, step 53, discriminator loss=0.697 , generator loss=0.643\n",
      "Training progress in epoch #11, step 54, discriminator loss=0.688 , generator loss=0.650\n",
      "Training progress in epoch #11, step 55, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #11, step 56, discriminator loss=0.680 , generator loss=0.700\n",
      "Training progress in epoch #11, step 57, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #11, step 58, discriminator loss=0.704 , generator loss=0.720\n",
      "Training progress in epoch #11, step 59, discriminator loss=0.702 , generator loss=0.750\n",
      "Training progress in epoch #11, step 60, discriminator loss=0.697 , generator loss=0.761\n",
      "Training progress in epoch #11, step 61, discriminator loss=0.688 , generator loss=0.764\n",
      "Training progress in epoch #11, step 62, discriminator loss=0.682 , generator loss=0.765\n",
      "Training progress in epoch #11, step 63, discriminator loss=0.681 , generator loss=0.784\n",
      "Training progress in epoch #11, step 64, discriminator loss=0.664 , generator loss=0.791\n",
      "Training progress in epoch #11, step 65, discriminator loss=0.665 , generator loss=0.774\n",
      "Training progress in epoch #11, step 66, discriminator loss=0.661 , generator loss=0.755\n",
      "Training progress in epoch #11, step 67, discriminator loss=0.664 , generator loss=0.748\n",
      "Training progress in epoch #11, step 68, discriminator loss=0.660 , generator loss=0.734\n",
      "Training progress in epoch #11, step 69, discriminator loss=0.679 , generator loss=0.705\n",
      "Training progress in epoch #11, step 70, discriminator loss=0.678 , generator loss=0.684\n",
      "Training progress in epoch #11, step 71, discriminator loss=0.679 , generator loss=0.680\n",
      "Training progress in epoch #11, step 72, discriminator loss=0.675 , generator loss=0.671\n",
      "Training progress in epoch #11, step 73, discriminator loss=0.674 , generator loss=0.678\n",
      "Training progress in epoch #11, step 74, discriminator loss=0.675 , generator loss=0.686\n",
      "Training progress in epoch #11, step 75, discriminator loss=0.672 , generator loss=0.693\n",
      "Training progress in epoch #11, step 76, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #11, step 77, discriminator loss=0.683 , generator loss=0.708\n",
      "Training progress in epoch #11, step 78, discriminator loss=0.680 , generator loss=0.721\n",
      "Training progress in epoch #11, step 79, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #11, step 80, discriminator loss=0.701 , generator loss=0.736\n",
      "Training progress in epoch #11, step 81, discriminator loss=0.693 , generator loss=0.755\n",
      "Training progress in epoch #11, step 82, discriminator loss=0.708 , generator loss=0.760\n",
      "Training progress in epoch #11, step 83, discriminator loss=0.703 , generator loss=0.751\n",
      "Training progress in epoch #11, step 84, discriminator loss=0.696 , generator loss=0.755\n",
      "Training progress in epoch #11, step 85, discriminator loss=0.694 , generator loss=0.783\n",
      "Training progress in epoch #11, step 86, discriminator loss=0.691 , generator loss=0.775\n",
      "Training progress in epoch #11, step 87, discriminator loss=0.698 , generator loss=0.758\n",
      "Training progress in epoch #11, step 88, discriminator loss=0.693 , generator loss=0.727\n",
      "Training progress in epoch #11, step 89, discriminator loss=0.684 , generator loss=0.710\n",
      "Training progress in epoch #11, step 90, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #11, step 91, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #11, step 92, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #11, step 93, discriminator loss=0.681 , generator loss=0.698\n",
      "Training progress in epoch #11, step 94, discriminator loss=0.673 , generator loss=0.720\n",
      "Training progress in epoch #11, step 95, discriminator loss=0.666 , generator loss=0.708\n",
      "Training progress in epoch #11, step 96, discriminator loss=0.668 , generator loss=0.705\n",
      "Training progress in epoch #11, step 97, discriminator loss=0.663 , generator loss=0.729\n",
      "Training progress in epoch #11, step 98, discriminator loss=0.665 , generator loss=0.753\n",
      "Training progress in epoch #11, step 99, discriminator loss=0.673 , generator loss=0.758\n",
      "Training progress in epoch #11, step 100, discriminator loss=0.669 , generator loss=0.739\n",
      "Training progress in epoch #11, step 101, discriminator loss=0.677 , generator loss=0.750\n",
      "Training progress in epoch #11, step 102, discriminator loss=0.685 , generator loss=0.750\n",
      "Training progress in epoch #11, step 103, discriminator loss=0.683 , generator loss=0.754\n",
      "Training progress in epoch #11, step 104, discriminator loss=0.689 , generator loss=0.762\n",
      "Training progress in epoch #11, step 105, discriminator loss=0.679 , generator loss=0.753\n",
      "Training progress in epoch #11, step 106, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #11, step 107, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #11, step 108, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #11, step 109, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #11, step 110, discriminator loss=0.699 , generator loss=0.695\n",
      "Training progress in epoch #11, step 111, discriminator loss=0.702 , generator loss=0.660\n",
      "Training progress in epoch #11, step 112, discriminator loss=0.704 , generator loss=0.653\n",
      "Training progress in epoch #11, step 113, discriminator loss=0.705 , generator loss=0.671\n",
      "Training progress in epoch #11, step 114, discriminator loss=0.712 , generator loss=0.661\n",
      "Training progress in epoch #11, step 115, discriminator loss=0.700 , generator loss=0.678\n",
      "Training progress in epoch #11, step 116, discriminator loss=0.701 , generator loss=0.692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #11, step 117, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #11, step 118, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #11, step 119, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #11, step 120, discriminator loss=0.683 , generator loss=0.743\n",
      "Training progress in epoch #11, step 121, discriminator loss=0.671 , generator loss=0.744\n",
      "Training progress in epoch #11, step 122, discriminator loss=0.673 , generator loss=0.733\n",
      "Training progress in epoch #11, step 123, discriminator loss=0.664 , generator loss=0.765\n",
      "Training progress in epoch #11, step 124, discriminator loss=0.670 , generator loss=0.742\n",
      "Training progress in epoch #11, step 125, discriminator loss=0.665 , generator loss=0.763\n",
      "Training progress in epoch #11, step 126, discriminator loss=0.668 , generator loss=0.737\n",
      "Training progress in epoch #11, step 127, discriminator loss=0.666 , generator loss=0.740\n",
      "Training progress in epoch #11, step 128, discriminator loss=0.666 , generator loss=0.736\n",
      "Training progress in epoch #11, step 129, discriminator loss=0.670 , generator loss=0.735\n",
      "Training progress in epoch #11, step 130, discriminator loss=0.678 , generator loss=0.686\n",
      "Training progress in epoch #11, step 131, discriminator loss=0.678 , generator loss=0.694\n",
      "Training progress in epoch #11, step 132, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #11, step 133, discriminator loss=0.684 , generator loss=0.714\n",
      "Training progress in epoch #11, step 134, discriminator loss=0.681 , generator loss=0.710\n",
      "Training progress in epoch #11, step 135, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #11, step 136, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #11, step 137, discriminator loss=0.685 , generator loss=0.711\n",
      "Training progress in epoch #11, step 138, discriminator loss=0.698 , generator loss=0.720\n",
      "Training progress in epoch #11, step 139, discriminator loss=0.697 , generator loss=0.718\n",
      "Training progress in epoch #11, step 140, discriminator loss=0.693 , generator loss=0.729\n",
      "Training progress in epoch #11, step 141, discriminator loss=0.695 , generator loss=0.727\n",
      "Training progress in epoch #11, step 142, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #11, step 143, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #11, step 144, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #11, step 145, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #11, step 146, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #11, step 147, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #11, step 148, discriminator loss=0.685 , generator loss=0.718\n",
      "Training progress in epoch #11, step 149, discriminator loss=0.673 , generator loss=0.712\n",
      "Training progress in epoch #11, step 150, discriminator loss=0.679 , generator loss=0.732\n",
      "Training progress in epoch #11, step 151, discriminator loss=0.667 , generator loss=0.735\n",
      "Training progress in epoch #11, step 152, discriminator loss=0.671 , generator loss=0.743\n",
      "Training progress in epoch #11, step 153, discriminator loss=0.673 , generator loss=0.736\n",
      "Training progress in epoch #11, step 154, discriminator loss=0.666 , generator loss=0.722\n",
      "Training progress in epoch #11, step 155, discriminator loss=0.667 , generator loss=0.729\n",
      "Training progress in epoch #11, step 156, discriminator loss=0.669 , generator loss=0.748\n",
      "Training progress in epoch #11, step 157, discriminator loss=0.670 , generator loss=0.746\n",
      "Training progress in epoch #11, step 158, discriminator loss=0.676 , generator loss=0.737\n",
      "Training progress in epoch #11, step 159, discriminator loss=0.669 , generator loss=0.734\n",
      "Training progress in epoch #11, step 160, discriminator loss=0.674 , generator loss=0.714\n",
      "Training progress in epoch #11, step 161, discriminator loss=0.679 , generator loss=0.719\n",
      "Training progress in epoch #11, step 162, discriminator loss=0.683 , generator loss=0.729\n",
      "Training progress in epoch #11, step 163, discriminator loss=0.683 , generator loss=0.732\n",
      "Training progress in epoch #11, step 164, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #11, step 165, discriminator loss=0.688 , generator loss=0.676\n",
      "Training progress in epoch #11, step 166, discriminator loss=0.675 , generator loss=0.702\n",
      "Training progress in epoch #11, step 167, discriminator loss=0.675 , generator loss=0.697\n",
      "Training progress in epoch #11, step 168, discriminator loss=0.682 , generator loss=0.701\n",
      "Training progress in epoch #11, step 169, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #11, step 170, discriminator loss=0.697 , generator loss=0.699\n",
      "Training progress in epoch #11, step 171, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #11, step 172, discriminator loss=0.697 , generator loss=0.677\n",
      "Training progress in epoch #11, step 173, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #11, step 174, discriminator loss=0.684 , generator loss=0.707\n",
      "Training progress in epoch #11, step 175, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #11, step 176, discriminator loss=0.683 , generator loss=0.728\n",
      "Training progress in epoch #11, step 177, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #11, step 178, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #11, step 179, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #11, step 180, discriminator loss=0.676 , generator loss=0.720\n",
      "Training progress in epoch #11, step 181, discriminator loss=0.675 , generator loss=0.699\n",
      "Training progress in epoch #11, step 182, discriminator loss=0.681 , generator loss=0.702\n",
      "Training progress in epoch #11, step 183, discriminator loss=0.680 , generator loss=0.706\n",
      "Training progress in epoch #11, step 184, discriminator loss=0.678 , generator loss=0.718\n",
      "Training progress in epoch #11, step 185, discriminator loss=0.674 , generator loss=0.717\n",
      "Training progress in epoch #11, step 186, discriminator loss=0.675 , generator loss=0.720\n",
      "Training progress in epoch #11, step 187, discriminator loss=0.670 , generator loss=0.738\n",
      "Training progress in epoch #11, step 188, discriminator loss=0.680 , generator loss=0.740\n",
      "Training progress in epoch #11, step 189, discriminator loss=0.673 , generator loss=0.743\n",
      "Training progress in epoch #11, step 190, discriminator loss=0.674 , generator loss=0.731\n",
      "Training progress in epoch #11, step 191, discriminator loss=0.685 , generator loss=0.709\n",
      "Training progress in epoch #11, step 192, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #11, step 193, discriminator loss=0.680 , generator loss=0.710\n",
      "Training progress in epoch #11, step 194, discriminator loss=0.676 , generator loss=0.703\n",
      "Training progress in epoch #11, step 195, discriminator loss=0.682 , generator loss=0.719\n",
      "Training progress in epoch #11, step 196, discriminator loss=0.685 , generator loss=0.709\n",
      "Training progress in epoch #11, step 197, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #11, step 198, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #11, step 199, discriminator loss=0.698 , generator loss=0.710\n",
      "Training progress in epoch #11, step 200, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #11, step 201, discriminator loss=0.700 , generator loss=0.717\n",
      "Training progress in epoch #11, step 202, discriminator loss=0.697 , generator loss=0.703\n",
      "Training progress in epoch #11, step 203, discriminator loss=0.681 , generator loss=0.720\n",
      "Training progress in epoch #11, step 204, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #11, step 205, discriminator loss=0.691 , generator loss=0.732\n",
      "Training progress in epoch #11, step 206, discriminator loss=0.685 , generator loss=0.726\n",
      "Training progress in epoch #11, step 207, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #11, step 208, discriminator loss=0.683 , generator loss=0.712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #11, step 209, discriminator loss=0.680 , generator loss=0.742\n",
      "Training progress in epoch #11, step 210, discriminator loss=0.681 , generator loss=0.744\n",
      "Training progress in epoch #11, step 211, discriminator loss=0.681 , generator loss=0.727\n",
      "Training progress in epoch #11, step 212, discriminator loss=0.672 , generator loss=0.736\n",
      "Training progress in epoch #11, step 213, discriminator loss=0.683 , generator loss=0.721\n",
      "Training progress in epoch #11, step 214, discriminator loss=0.677 , generator loss=0.729\n",
      "Training progress in epoch #11, step 215, discriminator loss=0.683 , generator loss=0.720\n",
      "Training progress in epoch #11, step 216, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #11, step 217, discriminator loss=0.683 , generator loss=0.695\n",
      "Training progress in epoch #11, step 218, discriminator loss=0.681 , generator loss=0.701\n",
      "Training progress in epoch #11, step 219, discriminator loss=0.681 , generator loss=0.723\n",
      "Training progress in epoch #11, step 220, discriminator loss=0.676 , generator loss=0.734\n",
      "Training progress in epoch #11, step 221, discriminator loss=0.683 , generator loss=0.710\n",
      "Training progress in epoch #11, step 222, discriminator loss=0.677 , generator loss=0.683\n",
      "Training progress in epoch #11, step 223, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #11, step 224, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #11, step 225, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #11, step 226, discriminator loss=0.682 , generator loss=0.721\n",
      "Training progress in epoch #11, step 227, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #11, step 228, discriminator loss=0.678 , generator loss=0.731\n",
      "Training progress in epoch #11, step 229, discriminator loss=0.692 , generator loss=0.740\n",
      "Training progress in epoch #11, step 230, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #11, step 231, discriminator loss=0.685 , generator loss=0.724\n",
      "Training progress in epoch #11, step 232, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #11, step 233, discriminator loss=0.687 , generator loss=0.703\n",
      "Disciminator Accuracy on real images: 72%, on fake images: 48%\n",
      "Training progress in epoch #12, step 0, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #12, step 1, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #12, step 2, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #12, step 3, discriminator loss=0.683 , generator loss=0.704\n",
      "Training progress in epoch #12, step 4, discriminator loss=0.681 , generator loss=0.699\n",
      "Training progress in epoch #12, step 5, discriminator loss=0.669 , generator loss=0.722\n",
      "Training progress in epoch #12, step 6, discriminator loss=0.682 , generator loss=0.710\n",
      "Training progress in epoch #12, step 7, discriminator loss=0.680 , generator loss=0.717\n",
      "Training progress in epoch #12, step 8, discriminator loss=0.675 , generator loss=0.722\n",
      "Training progress in epoch #12, step 9, discriminator loss=0.673 , generator loss=0.743\n",
      "Training progress in epoch #12, step 10, discriminator loss=0.676 , generator loss=0.742\n",
      "Training progress in epoch #12, step 11, discriminator loss=0.682 , generator loss=0.739\n",
      "Training progress in epoch #12, step 12, discriminator loss=0.694 , generator loss=0.754\n",
      "Training progress in epoch #12, step 13, discriminator loss=0.676 , generator loss=0.744\n",
      "Training progress in epoch #12, step 14, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #12, step 15, discriminator loss=0.681 , generator loss=0.743\n",
      "Training progress in epoch #12, step 16, discriminator loss=0.683 , generator loss=0.722\n",
      "Training progress in epoch #12, step 17, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #12, step 18, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #12, step 19, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #12, step 20, discriminator loss=0.684 , generator loss=0.721\n",
      "Training progress in epoch #12, step 21, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #12, step 22, discriminator loss=0.685 , generator loss=0.679\n",
      "Training progress in epoch #12, step 23, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #12, step 24, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #12, step 25, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #12, step 26, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #12, step 27, discriminator loss=0.695 , generator loss=0.750\n",
      "Training progress in epoch #12, step 28, discriminator loss=0.681 , generator loss=0.747\n",
      "Training progress in epoch #12, step 29, discriminator loss=0.689 , generator loss=0.755\n",
      "Training progress in epoch #12, step 30, discriminator loss=0.687 , generator loss=0.752\n",
      "Training progress in epoch #12, step 31, discriminator loss=0.688 , generator loss=0.751\n",
      "Training progress in epoch #12, step 32, discriminator loss=0.682 , generator loss=0.758\n",
      "Training progress in epoch #12, step 33, discriminator loss=0.679 , generator loss=0.746\n",
      "Training progress in epoch #12, step 34, discriminator loss=0.679 , generator loss=0.725\n",
      "Training progress in epoch #12, step 35, discriminator loss=0.680 , generator loss=0.731\n",
      "Training progress in epoch #12, step 36, discriminator loss=0.683 , generator loss=0.710\n",
      "Training progress in epoch #12, step 37, discriminator loss=0.685 , generator loss=0.690\n",
      "Training progress in epoch #12, step 38, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #12, step 39, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #12, step 40, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #12, step 41, discriminator loss=0.678 , generator loss=0.690\n",
      "Training progress in epoch #12, step 42, discriminator loss=0.683 , generator loss=0.716\n",
      "Training progress in epoch #12, step 43, discriminator loss=0.683 , generator loss=0.722\n",
      "Training progress in epoch #12, step 44, discriminator loss=0.680 , generator loss=0.712\n",
      "Training progress in epoch #12, step 45, discriminator loss=0.682 , generator loss=0.725\n",
      "Training progress in epoch #12, step 46, discriminator loss=0.677 , generator loss=0.750\n",
      "Training progress in epoch #12, step 47, discriminator loss=0.696 , generator loss=0.768\n",
      "Training progress in epoch #12, step 48, discriminator loss=0.689 , generator loss=0.783\n",
      "Training progress in epoch #12, step 49, discriminator loss=0.679 , generator loss=0.742\n",
      "Training progress in epoch #12, step 50, discriminator loss=0.676 , generator loss=0.746\n",
      "Training progress in epoch #12, step 51, discriminator loss=0.684 , generator loss=0.720\n",
      "Training progress in epoch #12, step 52, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #12, step 53, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #12, step 54, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #12, step 55, discriminator loss=0.697 , generator loss=0.665\n",
      "Training progress in epoch #12, step 56, discriminator loss=0.681 , generator loss=0.675\n",
      "Training progress in epoch #12, step 57, discriminator loss=0.684 , generator loss=0.679\n",
      "Training progress in epoch #12, step 58, discriminator loss=0.685 , generator loss=0.678\n",
      "Training progress in epoch #12, step 59, discriminator loss=0.680 , generator loss=0.699\n",
      "Training progress in epoch #12, step 60, discriminator loss=0.682 , generator loss=0.717\n",
      "Training progress in epoch #12, step 61, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #12, step 62, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #12, step 63, discriminator loss=0.681 , generator loss=0.740\n",
      "Training progress in epoch #12, step 64, discriminator loss=0.685 , generator loss=0.748\n",
      "Training progress in epoch #12, step 65, discriminator loss=0.682 , generator loss=0.758\n",
      "Training progress in epoch #12, step 66, discriminator loss=0.688 , generator loss=0.733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #12, step 67, discriminator loss=0.695 , generator loss=0.749\n",
      "Training progress in epoch #12, step 68, discriminator loss=0.675 , generator loss=0.759\n",
      "Training progress in epoch #12, step 69, discriminator loss=0.688 , generator loss=0.731\n",
      "Training progress in epoch #12, step 70, discriminator loss=0.680 , generator loss=0.716\n",
      "Training progress in epoch #12, step 71, discriminator loss=0.681 , generator loss=0.708\n",
      "Training progress in epoch #12, step 72, discriminator loss=0.673 , generator loss=0.679\n",
      "Training progress in epoch #12, step 73, discriminator loss=0.684 , generator loss=0.695\n",
      "Training progress in epoch #12, step 74, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #12, step 75, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #12, step 76, discriminator loss=0.681 , generator loss=0.700\n",
      "Training progress in epoch #12, step 77, discriminator loss=0.682 , generator loss=0.699\n",
      "Training progress in epoch #12, step 78, discriminator loss=0.681 , generator loss=0.709\n",
      "Training progress in epoch #12, step 79, discriminator loss=0.693 , generator loss=0.728\n",
      "Training progress in epoch #12, step 80, discriminator loss=0.684 , generator loss=0.748\n",
      "Training progress in epoch #12, step 81, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #12, step 82, discriminator loss=0.702 , generator loss=0.729\n",
      "Training progress in epoch #12, step 83, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #12, step 84, discriminator loss=0.686 , generator loss=0.739\n",
      "Training progress in epoch #12, step 85, discriminator loss=0.684 , generator loss=0.774\n",
      "Training progress in epoch #12, step 86, discriminator loss=0.683 , generator loss=0.759\n",
      "Training progress in epoch #12, step 87, discriminator loss=0.678 , generator loss=0.725\n",
      "Training progress in epoch #12, step 88, discriminator loss=0.683 , generator loss=0.709\n",
      "Training progress in epoch #12, step 89, discriminator loss=0.674 , generator loss=0.711\n",
      "Training progress in epoch #12, step 90, discriminator loss=0.678 , generator loss=0.674\n",
      "Training progress in epoch #12, step 91, discriminator loss=0.666 , generator loss=0.708\n",
      "Training progress in epoch #12, step 92, discriminator loss=0.672 , generator loss=0.731\n",
      "Training progress in epoch #12, step 93, discriminator loss=0.677 , generator loss=0.702\n",
      "Training progress in epoch #12, step 94, discriminator loss=0.678 , generator loss=0.717\n",
      "Training progress in epoch #12, step 95, discriminator loss=0.674 , generator loss=0.725\n",
      "Training progress in epoch #12, step 96, discriminator loss=0.681 , generator loss=0.717\n",
      "Training progress in epoch #12, step 97, discriminator loss=0.675 , generator loss=0.736\n",
      "Training progress in epoch #12, step 98, discriminator loss=0.686 , generator loss=0.741\n",
      "Training progress in epoch #12, step 99, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #12, step 100, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #12, step 101, discriminator loss=0.685 , generator loss=0.739\n",
      "Training progress in epoch #12, step 102, discriminator loss=0.695 , generator loss=0.741\n",
      "Training progress in epoch #12, step 103, discriminator loss=0.683 , generator loss=0.732\n",
      "Training progress in epoch #12, step 104, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #12, step 105, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #12, step 106, discriminator loss=0.680 , generator loss=0.687\n",
      "Training progress in epoch #12, step 107, discriminator loss=0.676 , generator loss=0.690\n",
      "Training progress in epoch #12, step 108, discriminator loss=0.690 , generator loss=0.679\n",
      "Training progress in epoch #12, step 109, discriminator loss=0.677 , generator loss=0.698\n",
      "Training progress in epoch #12, step 110, discriminator loss=0.679 , generator loss=0.691\n",
      "Training progress in epoch #12, step 111, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #12, step 112, discriminator loss=0.680 , generator loss=0.712\n",
      "Training progress in epoch #12, step 113, discriminator loss=0.683 , generator loss=0.719\n",
      "Training progress in epoch #12, step 114, discriminator loss=0.683 , generator loss=0.703\n",
      "Training progress in epoch #12, step 115, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #12, step 116, discriminator loss=0.684 , generator loss=0.736\n",
      "Training progress in epoch #12, step 117, discriminator loss=0.693 , generator loss=0.733\n",
      "Training progress in epoch #12, step 118, discriminator loss=0.685 , generator loss=0.721\n",
      "Training progress in epoch #12, step 119, discriminator loss=0.694 , generator loss=0.724\n",
      "Training progress in epoch #12, step 120, discriminator loss=0.695 , generator loss=0.716\n",
      "Training progress in epoch #12, step 121, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #12, step 122, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #12, step 123, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #12, step 124, discriminator loss=0.682 , generator loss=0.730\n",
      "Training progress in epoch #12, step 125, discriminator loss=0.684 , generator loss=0.700\n",
      "Training progress in epoch #12, step 126, discriminator loss=0.684 , generator loss=0.697\n",
      "Training progress in epoch #12, step 127, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #12, step 128, discriminator loss=0.684 , generator loss=0.710\n",
      "Training progress in epoch #12, step 129, discriminator loss=0.683 , generator loss=0.720\n",
      "Training progress in epoch #12, step 130, discriminator loss=0.681 , generator loss=0.707\n",
      "Training progress in epoch #12, step 131, discriminator loss=0.682 , generator loss=0.716\n",
      "Training progress in epoch #12, step 132, discriminator loss=0.683 , generator loss=0.720\n",
      "Training progress in epoch #12, step 133, discriminator loss=0.687 , generator loss=0.730\n",
      "Training progress in epoch #12, step 134, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #12, step 135, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #12, step 136, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #12, step 137, discriminator loss=0.679 , generator loss=0.722\n",
      "Training progress in epoch #12, step 138, discriminator loss=0.690 , generator loss=0.737\n",
      "Training progress in epoch #12, step 139, discriminator loss=0.689 , generator loss=0.743\n",
      "Training progress in epoch #12, step 140, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #12, step 141, discriminator loss=0.681 , generator loss=0.722\n",
      "Training progress in epoch #12, step 142, discriminator loss=0.677 , generator loss=0.729\n",
      "Training progress in epoch #12, step 143, discriminator loss=0.679 , generator loss=0.719\n",
      "Training progress in epoch #12, step 144, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #12, step 145, discriminator loss=0.675 , generator loss=0.679\n",
      "Training progress in epoch #12, step 146, discriminator loss=0.676 , generator loss=0.681\n",
      "Training progress in epoch #12, step 147, discriminator loss=0.682 , generator loss=0.719\n",
      "Training progress in epoch #12, step 148, discriminator loss=0.676 , generator loss=0.726\n",
      "Training progress in epoch #12, step 149, discriminator loss=0.680 , generator loss=0.730\n",
      "Training progress in epoch #12, step 150, discriminator loss=0.682 , generator loss=0.722\n",
      "Training progress in epoch #12, step 151, discriminator loss=0.683 , generator loss=0.698\n",
      "Training progress in epoch #12, step 152, discriminator loss=0.697 , generator loss=0.727\n",
      "Training progress in epoch #12, step 153, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #12, step 154, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #12, step 155, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #12, step 156, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #12, step 157, discriminator loss=0.690 , generator loss=0.678\n",
      "Training progress in epoch #12, step 158, discriminator loss=0.699 , generator loss=0.694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #12, step 159, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #12, step 160, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #12, step 161, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #12, step 162, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #12, step 163, discriminator loss=0.680 , generator loss=0.724\n",
      "Training progress in epoch #12, step 164, discriminator loss=0.679 , generator loss=0.747\n",
      "Training progress in epoch #12, step 165, discriminator loss=0.675 , generator loss=0.745\n",
      "Training progress in epoch #12, step 166, discriminator loss=0.682 , generator loss=0.736\n",
      "Training progress in epoch #12, step 167, discriminator loss=0.671 , generator loss=0.714\n",
      "Training progress in epoch #12, step 168, discriminator loss=0.683 , generator loss=0.719\n",
      "Training progress in epoch #12, step 169, discriminator loss=0.681 , generator loss=0.726\n",
      "Training progress in epoch #12, step 170, discriminator loss=0.679 , generator loss=0.715\n",
      "Training progress in epoch #12, step 171, discriminator loss=0.685 , generator loss=0.714\n",
      "Training progress in epoch #12, step 172, discriminator loss=0.685 , generator loss=0.686\n",
      "Training progress in epoch #12, step 173, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #12, step 174, discriminator loss=0.694 , generator loss=0.678\n",
      "Training progress in epoch #12, step 175, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #12, step 176, discriminator loss=0.697 , generator loss=0.690\n",
      "Training progress in epoch #12, step 177, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #12, step 178, discriminator loss=0.692 , generator loss=0.728\n",
      "Training progress in epoch #12, step 179, discriminator loss=0.698 , generator loss=0.729\n",
      "Training progress in epoch #12, step 180, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #12, step 181, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #12, step 182, discriminator loss=0.680 , generator loss=0.736\n",
      "Training progress in epoch #12, step 183, discriminator loss=0.678 , generator loss=0.751\n",
      "Training progress in epoch #12, step 184, discriminator loss=0.676 , generator loss=0.737\n",
      "Training progress in epoch #12, step 185, discriminator loss=0.684 , generator loss=0.721\n",
      "Training progress in epoch #12, step 186, discriminator loss=0.684 , generator loss=0.732\n",
      "Training progress in epoch #12, step 187, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #12, step 188, discriminator loss=0.680 , generator loss=0.679\n",
      "Training progress in epoch #12, step 189, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #12, step 190, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #12, step 191, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #12, step 192, discriminator loss=0.679 , generator loss=0.705\n",
      "Training progress in epoch #12, step 193, discriminator loss=0.679 , generator loss=0.717\n",
      "Training progress in epoch #12, step 194, discriminator loss=0.681 , generator loss=0.722\n",
      "Training progress in epoch #12, step 195, discriminator loss=0.680 , generator loss=0.714\n",
      "Training progress in epoch #12, step 196, discriminator loss=0.699 , generator loss=0.705\n",
      "Training progress in epoch #12, step 197, discriminator loss=0.694 , generator loss=0.742\n",
      "Training progress in epoch #12, step 198, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #12, step 199, discriminator loss=0.690 , generator loss=0.743\n",
      "Training progress in epoch #12, step 200, discriminator loss=0.679 , generator loss=0.749\n",
      "Training progress in epoch #12, step 201, discriminator loss=0.686 , generator loss=0.733\n",
      "Training progress in epoch #12, step 202, discriminator loss=0.685 , generator loss=0.712\n",
      "Training progress in epoch #12, step 203, discriminator loss=0.689 , generator loss=0.669\n",
      "Training progress in epoch #12, step 204, discriminator loss=0.679 , generator loss=0.681\n",
      "Training progress in epoch #12, step 205, discriminator loss=0.684 , generator loss=0.703\n",
      "Training progress in epoch #12, step 206, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #12, step 207, discriminator loss=0.684 , generator loss=0.698\n",
      "Training progress in epoch #12, step 208, discriminator loss=0.688 , generator loss=0.677\n",
      "Training progress in epoch #12, step 209, discriminator loss=0.680 , generator loss=0.694\n",
      "Training progress in epoch #12, step 210, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #12, step 211, discriminator loss=0.687 , generator loss=0.736\n",
      "Training progress in epoch #12, step 212, discriminator loss=0.698 , generator loss=0.721\n",
      "Training progress in epoch #12, step 213, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #12, step 214, discriminator loss=0.694 , generator loss=0.751\n",
      "Training progress in epoch #12, step 215, discriminator loss=0.690 , generator loss=0.739\n",
      "Training progress in epoch #12, step 216, discriminator loss=0.695 , generator loss=0.739\n",
      "Training progress in epoch #12, step 217, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #12, step 218, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #12, step 219, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #12, step 220, discriminator loss=0.683 , generator loss=0.697\n",
      "Training progress in epoch #12, step 221, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #12, step 222, discriminator loss=0.682 , generator loss=0.694\n",
      "Training progress in epoch #12, step 223, discriminator loss=0.683 , generator loss=0.704\n",
      "Training progress in epoch #12, step 224, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #12, step 225, discriminator loss=0.685 , generator loss=0.733\n",
      "Training progress in epoch #12, step 226, discriminator loss=0.674 , generator loss=0.712\n",
      "Training progress in epoch #12, step 227, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #12, step 228, discriminator loss=0.677 , generator loss=0.719\n",
      "Training progress in epoch #12, step 229, discriminator loss=0.678 , generator loss=0.720\n",
      "Training progress in epoch #12, step 230, discriminator loss=0.693 , generator loss=0.735\n",
      "Training progress in epoch #12, step 231, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #12, step 232, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #12, step 233, discriminator loss=0.688 , generator loss=0.687\n",
      "Disciminator Accuracy on real images: 65%, on fake images: 43%\n",
      "Training progress in epoch #13, step 0, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #13, step 1, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #13, step 2, discriminator loss=0.699 , generator loss=0.716\n",
      "Training progress in epoch #13, step 3, discriminator loss=0.703 , generator loss=0.710\n",
      "Training progress in epoch #13, step 4, discriminator loss=0.698 , generator loss=0.701\n",
      "Training progress in epoch #13, step 5, discriminator loss=0.693 , generator loss=0.756\n",
      "Training progress in epoch #13, step 6, discriminator loss=0.697 , generator loss=0.721\n",
      "Training progress in epoch #13, step 7, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #13, step 8, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #13, step 9, discriminator loss=0.685 , generator loss=0.688\n",
      "Training progress in epoch #13, step 10, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #13, step 11, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #13, step 12, discriminator loss=0.680 , generator loss=0.705\n",
      "Training progress in epoch #13, step 13, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #13, step 14, discriminator loss=0.682 , generator loss=0.724\n",
      "Training progress in epoch #13, step 15, discriminator loss=0.687 , generator loss=0.717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #13, step 16, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #13, step 17, discriminator loss=0.681 , generator loss=0.727\n",
      "Training progress in epoch #13, step 18, discriminator loss=0.699 , generator loss=0.724\n",
      "Training progress in epoch #13, step 19, discriminator loss=0.695 , generator loss=0.748\n",
      "Training progress in epoch #13, step 20, discriminator loss=0.685 , generator loss=0.711\n",
      "Training progress in epoch #13, step 21, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #13, step 22, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #13, step 23, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #13, step 24, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #13, step 25, discriminator loss=0.700 , generator loss=0.697\n",
      "Training progress in epoch #13, step 26, discriminator loss=0.698 , generator loss=0.714\n",
      "Training progress in epoch #13, step 27, discriminator loss=0.693 , generator loss=0.741\n",
      "Training progress in epoch #13, step 28, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #13, step 29, discriminator loss=0.682 , generator loss=0.716\n",
      "Training progress in epoch #13, step 30, discriminator loss=0.679 , generator loss=0.736\n",
      "Training progress in epoch #13, step 31, discriminator loss=0.675 , generator loss=0.737\n",
      "Training progress in epoch #13, step 32, discriminator loss=0.680 , generator loss=0.740\n",
      "Training progress in epoch #13, step 33, discriminator loss=0.678 , generator loss=0.737\n",
      "Training progress in epoch #13, step 34, discriminator loss=0.679 , generator loss=0.727\n",
      "Training progress in epoch #13, step 35, discriminator loss=0.679 , generator loss=0.697\n",
      "Training progress in epoch #13, step 36, discriminator loss=0.683 , generator loss=0.685\n",
      "Training progress in epoch #13, step 37, discriminator loss=0.673 , generator loss=0.715\n",
      "Training progress in epoch #13, step 38, discriminator loss=0.682 , generator loss=0.703\n",
      "Training progress in epoch #13, step 39, discriminator loss=0.682 , generator loss=0.694\n",
      "Training progress in epoch #13, step 40, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #13, step 41, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #13, step 42, discriminator loss=0.697 , generator loss=0.724\n",
      "Training progress in epoch #13, step 43, discriminator loss=0.688 , generator loss=0.737\n",
      "Training progress in epoch #13, step 44, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #13, step 45, discriminator loss=0.699 , generator loss=0.702\n",
      "Training progress in epoch #13, step 46, discriminator loss=0.701 , generator loss=0.765\n",
      "Training progress in epoch #13, step 47, discriminator loss=0.680 , generator loss=0.763\n",
      "Training progress in epoch #13, step 48, discriminator loss=0.685 , generator loss=0.726\n",
      "Training progress in epoch #13, step 49, discriminator loss=0.679 , generator loss=0.702\n",
      "Training progress in epoch #13, step 50, discriminator loss=0.694 , generator loss=0.739\n",
      "Training progress in epoch #13, step 51, discriminator loss=0.688 , generator loss=0.732\n",
      "Training progress in epoch #13, step 52, discriminator loss=0.681 , generator loss=0.720\n",
      "Training progress in epoch #13, step 53, discriminator loss=0.683 , generator loss=0.688\n",
      "Training progress in epoch #13, step 54, discriminator loss=0.684 , generator loss=0.688\n",
      "Training progress in epoch #13, step 55, discriminator loss=0.676 , generator loss=0.686\n",
      "Training progress in epoch #13, step 56, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #13, step 57, discriminator loss=0.684 , generator loss=0.695\n",
      "Training progress in epoch #13, step 58, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #13, step 59, discriminator loss=0.683 , generator loss=0.720\n",
      "Training progress in epoch #13, step 60, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #13, step 61, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #13, step 62, discriminator loss=0.683 , generator loss=0.735\n",
      "Training progress in epoch #13, step 63, discriminator loss=0.681 , generator loss=0.734\n",
      "Training progress in epoch #13, step 64, discriminator loss=0.692 , generator loss=0.761\n",
      "Training progress in epoch #13, step 65, discriminator loss=0.685 , generator loss=0.753\n",
      "Training progress in epoch #13, step 66, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #13, step 67, discriminator loss=0.680 , generator loss=0.714\n",
      "Training progress in epoch #13, step 68, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #13, step 69, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #13, step 70, discriminator loss=0.685 , generator loss=0.698\n",
      "Training progress in epoch #13, step 71, discriminator loss=0.691 , generator loss=0.664\n",
      "Training progress in epoch #13, step 72, discriminator loss=0.686 , generator loss=0.681\n",
      "Training progress in epoch #13, step 73, discriminator loss=0.683 , generator loss=0.685\n",
      "Training progress in epoch #13, step 74, discriminator loss=0.678 , generator loss=0.715\n",
      "Training progress in epoch #13, step 75, discriminator loss=0.684 , generator loss=0.723\n",
      "Training progress in epoch #13, step 76, discriminator loss=0.683 , generator loss=0.700\n",
      "Training progress in epoch #13, step 77, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #13, step 78, discriminator loss=0.693 , generator loss=0.746\n",
      "Training progress in epoch #13, step 79, discriminator loss=0.683 , generator loss=0.742\n",
      "Training progress in epoch #13, step 80, discriminator loss=0.686 , generator loss=0.768\n",
      "Training progress in epoch #13, step 81, discriminator loss=0.687 , generator loss=0.766\n",
      "Training progress in epoch #13, step 82, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #13, step 83, discriminator loss=0.684 , generator loss=0.714\n",
      "Training progress in epoch #13, step 84, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #13, step 85, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #13, step 86, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #13, step 87, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #13, step 88, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #13, step 89, discriminator loss=0.677 , generator loss=0.698\n",
      "Training progress in epoch #13, step 90, discriminator loss=0.684 , generator loss=0.698\n",
      "Training progress in epoch #13, step 91, discriminator loss=0.681 , generator loss=0.696\n",
      "Training progress in epoch #13, step 92, discriminator loss=0.685 , generator loss=0.725\n",
      "Training progress in epoch #13, step 93, discriminator loss=0.679 , generator loss=0.737\n",
      "Training progress in epoch #13, step 94, discriminator loss=0.676 , generator loss=0.740\n",
      "Training progress in epoch #13, step 95, discriminator loss=0.671 , generator loss=0.734\n",
      "Training progress in epoch #13, step 96, discriminator loss=0.686 , generator loss=0.737\n",
      "Training progress in epoch #13, step 97, discriminator loss=0.686 , generator loss=0.742\n",
      "Training progress in epoch #13, step 98, discriminator loss=0.691 , generator loss=0.735\n",
      "Training progress in epoch #13, step 99, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #13, step 100, discriminator loss=0.680 , generator loss=0.701\n",
      "Training progress in epoch #13, step 101, discriminator loss=0.700 , generator loss=0.715\n",
      "Training progress in epoch #13, step 102, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #13, step 103, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #13, step 104, discriminator loss=0.701 , generator loss=0.671\n",
      "Training progress in epoch #13, step 105, discriminator loss=0.686 , generator loss=0.666\n",
      "Training progress in epoch #13, step 106, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #13, step 107, discriminator loss=0.694 , generator loss=0.707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #13, step 108, discriminator loss=0.681 , generator loss=0.718\n",
      "Training progress in epoch #13, step 109, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #13, step 110, discriminator loss=0.689 , generator loss=0.743\n",
      "Training progress in epoch #13, step 111, discriminator loss=0.690 , generator loss=0.754\n",
      "Training progress in epoch #13, step 112, discriminator loss=0.677 , generator loss=0.744\n",
      "Training progress in epoch #13, step 113, discriminator loss=0.684 , generator loss=0.763\n",
      "Training progress in epoch #13, step 114, discriminator loss=0.686 , generator loss=0.728\n",
      "Training progress in epoch #13, step 115, discriminator loss=0.680 , generator loss=0.740\n",
      "Training progress in epoch #13, step 116, discriminator loss=0.676 , generator loss=0.754\n",
      "Training progress in epoch #13, step 117, discriminator loss=0.680 , generator loss=0.727\n",
      "Training progress in epoch #13, step 118, discriminator loss=0.682 , generator loss=0.691\n",
      "Training progress in epoch #13, step 119, discriminator loss=0.689 , generator loss=0.665\n",
      "Training progress in epoch #13, step 120, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #13, step 121, discriminator loss=0.699 , generator loss=0.684\n",
      "Training progress in epoch #13, step 122, discriminator loss=0.697 , generator loss=0.679\n",
      "Training progress in epoch #13, step 123, discriminator loss=0.689 , generator loss=0.666\n",
      "Training progress in epoch #13, step 124, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #13, step 125, discriminator loss=0.697 , generator loss=0.713\n",
      "Training progress in epoch #13, step 126, discriminator loss=0.694 , generator loss=0.735\n",
      "Training progress in epoch #13, step 127, discriminator loss=0.701 , generator loss=0.751\n",
      "Training progress in epoch #13, step 128, discriminator loss=0.700 , generator loss=0.749\n",
      "Training progress in epoch #13, step 129, discriminator loss=0.694 , generator loss=0.754\n",
      "Training progress in epoch #13, step 130, discriminator loss=0.683 , generator loss=0.736\n",
      "Training progress in epoch #13, step 131, discriminator loss=0.680 , generator loss=0.726\n",
      "Training progress in epoch #13, step 132, discriminator loss=0.678 , generator loss=0.741\n",
      "Training progress in epoch #13, step 133, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #13, step 134, discriminator loss=0.689 , generator loss=0.658\n",
      "Training progress in epoch #13, step 135, discriminator loss=0.683 , generator loss=0.665\n",
      "Training progress in epoch #13, step 136, discriminator loss=0.674 , generator loss=0.686\n",
      "Training progress in epoch #13, step 137, discriminator loss=0.678 , generator loss=0.703\n",
      "Training progress in epoch #13, step 138, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #13, step 139, discriminator loss=0.678 , generator loss=0.689\n",
      "Training progress in epoch #13, step 140, discriminator loss=0.679 , generator loss=0.713\n",
      "Training progress in epoch #13, step 141, discriminator loss=0.692 , generator loss=0.739\n",
      "Training progress in epoch #13, step 142, discriminator loss=0.692 , generator loss=0.742\n",
      "Training progress in epoch #13, step 143, discriminator loss=0.697 , generator loss=0.757\n",
      "Training progress in epoch #13, step 144, discriminator loss=0.686 , generator loss=0.746\n",
      "Training progress in epoch #13, step 145, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #13, step 146, discriminator loss=0.695 , generator loss=0.740\n",
      "Training progress in epoch #13, step 147, discriminator loss=0.687 , generator loss=0.730\n",
      "Training progress in epoch #13, step 148, discriminator loss=0.687 , generator loss=0.731\n",
      "Training progress in epoch #13, step 149, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #13, step 150, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #13, step 151, discriminator loss=0.684 , generator loss=0.658\n",
      "Training progress in epoch #13, step 152, discriminator loss=0.687 , generator loss=0.672\n",
      "Training progress in epoch #13, step 153, discriminator loss=0.678 , generator loss=0.686\n",
      "Training progress in epoch #13, step 154, discriminator loss=0.681 , generator loss=0.699\n",
      "Training progress in epoch #13, step 155, discriminator loss=0.678 , generator loss=0.703\n",
      "Training progress in epoch #13, step 156, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #13, step 157, discriminator loss=0.679 , generator loss=0.729\n",
      "Training progress in epoch #13, step 158, discriminator loss=0.680 , generator loss=0.745\n",
      "Training progress in epoch #13, step 159, discriminator loss=0.692 , generator loss=0.747\n",
      "Training progress in epoch #13, step 160, discriminator loss=0.691 , generator loss=0.762\n",
      "Training progress in epoch #13, step 161, discriminator loss=0.695 , generator loss=0.754\n",
      "Training progress in epoch #13, step 162, discriminator loss=0.693 , generator loss=0.748\n",
      "Training progress in epoch #13, step 163, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #13, step 164, discriminator loss=0.694 , generator loss=0.737\n",
      "Training progress in epoch #13, step 165, discriminator loss=0.689 , generator loss=0.728\n",
      "Training progress in epoch #13, step 166, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #13, step 167, discriminator loss=0.685 , generator loss=0.649\n",
      "Training progress in epoch #13, step 168, discriminator loss=0.693 , generator loss=0.648\n",
      "Training progress in epoch #13, step 169, discriminator loss=0.692 , generator loss=0.662\n",
      "Training progress in epoch #13, step 170, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #13, step 171, discriminator loss=0.681 , generator loss=0.690\n",
      "Training progress in epoch #13, step 172, discriminator loss=0.682 , generator loss=0.703\n",
      "Training progress in epoch #13, step 173, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #13, step 174, discriminator loss=0.678 , generator loss=0.711\n",
      "Training progress in epoch #13, step 175, discriminator loss=0.688 , generator loss=0.740\n",
      "Training progress in epoch #13, step 176, discriminator loss=0.681 , generator loss=0.745\n",
      "Training progress in epoch #13, step 177, discriminator loss=0.687 , generator loss=0.752\n",
      "Training progress in epoch #13, step 178, discriminator loss=0.691 , generator loss=0.799\n",
      "Training progress in epoch #13, step 179, discriminator loss=0.687 , generator loss=0.771\n",
      "Training progress in epoch #13, step 180, discriminator loss=0.678 , generator loss=0.780\n",
      "Training progress in epoch #13, step 181, discriminator loss=0.686 , generator loss=0.741\n",
      "Training progress in epoch #13, step 182, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #13, step 183, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #13, step 184, discriminator loss=0.690 , generator loss=0.647\n",
      "Training progress in epoch #13, step 185, discriminator loss=0.697 , generator loss=0.665\n",
      "Training progress in epoch #13, step 186, discriminator loss=0.697 , generator loss=0.636\n",
      "Training progress in epoch #13, step 187, discriminator loss=0.685 , generator loss=0.647\n",
      "Training progress in epoch #13, step 188, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #13, step 189, discriminator loss=0.683 , generator loss=0.713\n",
      "Training progress in epoch #13, step 190, discriminator loss=0.673 , generator loss=0.730\n",
      "Training progress in epoch #13, step 191, discriminator loss=0.680 , generator loss=0.743\n",
      "Training progress in epoch #13, step 192, discriminator loss=0.684 , generator loss=0.759\n",
      "Training progress in epoch #13, step 193, discriminator loss=0.689 , generator loss=0.765\n",
      "Training progress in epoch #13, step 194, discriminator loss=0.682 , generator loss=0.771\n",
      "Training progress in epoch #13, step 195, discriminator loss=0.691 , generator loss=0.773\n",
      "Training progress in epoch #13, step 196, discriminator loss=0.684 , generator loss=0.776\n",
      "Training progress in epoch #13, step 197, discriminator loss=0.673 , generator loss=0.783\n",
      "Training progress in epoch #13, step 198, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #13, step 199, discriminator loss=0.690 , generator loss=0.682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #13, step 200, discriminator loss=0.690 , generator loss=0.668\n",
      "Training progress in epoch #13, step 201, discriminator loss=0.697 , generator loss=0.653\n",
      "Training progress in epoch #13, step 202, discriminator loss=0.688 , generator loss=0.663\n",
      "Training progress in epoch #13, step 203, discriminator loss=0.699 , generator loss=0.684\n",
      "Training progress in epoch #13, step 204, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #13, step 205, discriminator loss=0.684 , generator loss=0.690\n",
      "Training progress in epoch #13, step 206, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #13, step 207, discriminator loss=0.684 , generator loss=0.734\n",
      "Training progress in epoch #13, step 208, discriminator loss=0.691 , generator loss=0.733\n",
      "Training progress in epoch #13, step 209, discriminator loss=0.694 , generator loss=0.744\n",
      "Training progress in epoch #13, step 210, discriminator loss=0.692 , generator loss=0.771\n",
      "Training progress in epoch #13, step 211, discriminator loss=0.693 , generator loss=0.767\n",
      "Training progress in epoch #13, step 212, discriminator loss=0.685 , generator loss=0.746\n",
      "Training progress in epoch #13, step 213, discriminator loss=0.683 , generator loss=0.756\n",
      "Training progress in epoch #13, step 214, discriminator loss=0.682 , generator loss=0.740\n",
      "Training progress in epoch #13, step 215, discriminator loss=0.690 , generator loss=0.732\n",
      "Training progress in epoch #13, step 216, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #13, step 217, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #13, step 218, discriminator loss=0.685 , generator loss=0.681\n",
      "Training progress in epoch #13, step 219, discriminator loss=0.691 , generator loss=0.668\n",
      "Training progress in epoch #13, step 220, discriminator loss=0.684 , generator loss=0.663\n",
      "Training progress in epoch #13, step 221, discriminator loss=0.685 , generator loss=0.667\n",
      "Training progress in epoch #13, step 222, discriminator loss=0.684 , generator loss=0.689\n",
      "Training progress in epoch #13, step 223, discriminator loss=0.682 , generator loss=0.713\n",
      "Training progress in epoch #13, step 224, discriminator loss=0.683 , generator loss=0.736\n",
      "Training progress in epoch #13, step 225, discriminator loss=0.686 , generator loss=0.745\n",
      "Training progress in epoch #13, step 226, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #13, step 227, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #13, step 228, discriminator loss=0.696 , generator loss=0.751\n",
      "Training progress in epoch #13, step 229, discriminator loss=0.691 , generator loss=0.743\n",
      "Training progress in epoch #13, step 230, discriminator loss=0.686 , generator loss=0.735\n",
      "Training progress in epoch #13, step 231, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #13, step 232, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #13, step 233, discriminator loss=0.686 , generator loss=0.700\n",
      "Disciminator Accuracy on real images: 82%, on fake images: 48%\n",
      "Training progress in epoch #14, step 0, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #14, step 1, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #14, step 2, discriminator loss=0.677 , generator loss=0.693\n",
      "Training progress in epoch #14, step 3, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #14, step 4, discriminator loss=0.682 , generator loss=0.712\n",
      "Training progress in epoch #14, step 5, discriminator loss=0.682 , generator loss=0.716\n",
      "Training progress in epoch #14, step 6, discriminator loss=0.678 , generator loss=0.722\n",
      "Training progress in epoch #14, step 7, discriminator loss=0.680 , generator loss=0.728\n",
      "Training progress in epoch #14, step 8, discriminator loss=0.685 , generator loss=0.712\n",
      "Training progress in epoch #14, step 9, discriminator loss=0.689 , generator loss=0.728\n",
      "Training progress in epoch #14, step 10, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #14, step 11, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #14, step 12, discriminator loss=0.688 , generator loss=0.733\n",
      "Training progress in epoch #14, step 13, discriminator loss=0.680 , generator loss=0.739\n",
      "Training progress in epoch #14, step 14, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #14, step 15, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #14, step 16, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #14, step 17, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #14, step 18, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #14, step 19, discriminator loss=0.684 , generator loss=0.685\n",
      "Training progress in epoch #14, step 20, discriminator loss=0.680 , generator loss=0.695\n",
      "Training progress in epoch #14, step 21, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #14, step 22, discriminator loss=0.681 , generator loss=0.725\n",
      "Training progress in epoch #14, step 23, discriminator loss=0.679 , generator loss=0.715\n",
      "Training progress in epoch #14, step 24, discriminator loss=0.686 , generator loss=0.731\n",
      "Training progress in epoch #14, step 25, discriminator loss=0.686 , generator loss=0.740\n",
      "Training progress in epoch #14, step 26, discriminator loss=0.684 , generator loss=0.752\n",
      "Training progress in epoch #14, step 27, discriminator loss=0.692 , generator loss=0.752\n",
      "Training progress in epoch #14, step 28, discriminator loss=0.684 , generator loss=0.738\n",
      "Training progress in epoch #14, step 29, discriminator loss=0.684 , generator loss=0.703\n",
      "Training progress in epoch #14, step 30, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #14, step 31, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #14, step 32, discriminator loss=0.680 , generator loss=0.684\n",
      "Training progress in epoch #14, step 33, discriminator loss=0.689 , generator loss=0.661\n",
      "Training progress in epoch #14, step 34, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #14, step 35, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #14, step 36, discriminator loss=0.681 , generator loss=0.700\n",
      "Training progress in epoch #14, step 37, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #14, step 38, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #14, step 39, discriminator loss=0.698 , generator loss=0.743\n",
      "Training progress in epoch #14, step 40, discriminator loss=0.689 , generator loss=0.739\n",
      "Training progress in epoch #14, step 41, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #14, step 42, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #14, step 43, discriminator loss=0.685 , generator loss=0.756\n",
      "Training progress in epoch #14, step 44, discriminator loss=0.685 , generator loss=0.757\n",
      "Training progress in epoch #14, step 45, discriminator loss=0.688 , generator loss=0.724\n",
      "Training progress in epoch #14, step 46, discriminator loss=0.679 , generator loss=0.710\n",
      "Training progress in epoch #14, step 47, discriminator loss=0.683 , generator loss=0.703\n",
      "Training progress in epoch #14, step 48, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #14, step 49, discriminator loss=0.691 , generator loss=0.665\n",
      "Training progress in epoch #14, step 50, discriminator loss=0.691 , generator loss=0.649\n",
      "Training progress in epoch #14, step 51, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #14, step 52, discriminator loss=0.680 , generator loss=0.701\n",
      "Training progress in epoch #14, step 53, discriminator loss=0.685 , generator loss=0.701\n",
      "Training progress in epoch #14, step 54, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #14, step 55, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #14, step 56, discriminator loss=0.691 , generator loss=0.735\n",
      "Training progress in epoch #14, step 57, discriminator loss=0.691 , generator loss=0.727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #14, step 58, discriminator loss=0.690 , generator loss=0.750\n",
      "Training progress in epoch #14, step 59, discriminator loss=0.693 , generator loss=0.765\n",
      "Training progress in epoch #14, step 60, discriminator loss=0.688 , generator loss=0.756\n",
      "Training progress in epoch #14, step 61, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #14, step 62, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #14, step 63, discriminator loss=0.699 , generator loss=0.709\n",
      "Training progress in epoch #14, step 64, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #14, step 65, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #14, step 66, discriminator loss=0.697 , generator loss=0.666\n",
      "Training progress in epoch #14, step 67, discriminator loss=0.685 , generator loss=0.690\n",
      "Training progress in epoch #14, step 68, discriminator loss=0.683 , generator loss=0.700\n",
      "Training progress in epoch #14, step 69, discriminator loss=0.676 , generator loss=0.711\n",
      "Training progress in epoch #14, step 70, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #14, step 71, discriminator loss=0.682 , generator loss=0.717\n",
      "Training progress in epoch #14, step 72, discriminator loss=0.692 , generator loss=0.747\n",
      "Training progress in epoch #14, step 73, discriminator loss=0.686 , generator loss=0.733\n",
      "Training progress in epoch #14, step 74, discriminator loss=0.692 , generator loss=0.748\n",
      "Training progress in epoch #14, step 75, discriminator loss=0.687 , generator loss=0.737\n",
      "Training progress in epoch #14, step 76, discriminator loss=0.699 , generator loss=0.754\n",
      "Training progress in epoch #14, step 77, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #14, step 78, discriminator loss=0.681 , generator loss=0.730\n",
      "Training progress in epoch #14, step 79, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #14, step 80, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #14, step 81, discriminator loss=0.694 , generator loss=0.673\n",
      "Training progress in epoch #14, step 82, discriminator loss=0.689 , generator loss=0.667\n",
      "Training progress in epoch #14, step 83, discriminator loss=0.689 , generator loss=0.678\n",
      "Training progress in epoch #14, step 84, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #14, step 85, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #14, step 86, discriminator loss=0.684 , generator loss=0.709\n",
      "Training progress in epoch #14, step 87, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #14, step 88, discriminator loss=0.678 , generator loss=0.727\n",
      "Training progress in epoch #14, step 89, discriminator loss=0.683 , generator loss=0.700\n",
      "Training progress in epoch #14, step 90, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #14, step 91, discriminator loss=0.687 , generator loss=0.748\n",
      "Training progress in epoch #14, step 92, discriminator loss=0.685 , generator loss=0.764\n",
      "Training progress in epoch #14, step 93, discriminator loss=0.688 , generator loss=0.757\n",
      "Training progress in epoch #14, step 94, discriminator loss=0.684 , generator loss=0.752\n",
      "Training progress in epoch #14, step 95, discriminator loss=0.682 , generator loss=0.706\n",
      "Training progress in epoch #14, step 96, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #14, step 97, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #14, step 98, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #14, step 99, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #14, step 100, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #14, step 101, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #14, step 102, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #14, step 103, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #14, step 104, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #14, step 105, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #14, step 106, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #14, step 107, discriminator loss=0.685 , generator loss=0.741\n",
      "Training progress in epoch #14, step 108, discriminator loss=0.685 , generator loss=0.772\n",
      "Training progress in epoch #14, step 109, discriminator loss=0.683 , generator loss=0.744\n",
      "Training progress in epoch #14, step 110, discriminator loss=0.680 , generator loss=0.721\n",
      "Training progress in epoch #14, step 111, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #14, step 112, discriminator loss=0.684 , generator loss=0.714\n",
      "Training progress in epoch #14, step 113, discriminator loss=0.680 , generator loss=0.703\n",
      "Training progress in epoch #14, step 114, discriminator loss=0.678 , generator loss=0.717\n",
      "Training progress in epoch #14, step 115, discriminator loss=0.685 , generator loss=0.718\n",
      "Training progress in epoch #14, step 116, discriminator loss=0.684 , generator loss=0.686\n",
      "Training progress in epoch #14, step 117, discriminator loss=0.685 , generator loss=0.691\n",
      "Training progress in epoch #14, step 118, discriminator loss=0.684 , generator loss=0.696\n",
      "Training progress in epoch #14, step 119, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #14, step 120, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #14, step 121, discriminator loss=0.692 , generator loss=0.728\n",
      "Training progress in epoch #14, step 122, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #14, step 123, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #14, step 124, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #14, step 125, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #14, step 126, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #14, step 127, discriminator loss=0.684 , generator loss=0.724\n",
      "Training progress in epoch #14, step 128, discriminator loss=0.689 , generator loss=0.746\n",
      "Training progress in epoch #14, step 129, discriminator loss=0.683 , generator loss=0.725\n",
      "Training progress in epoch #14, step 130, discriminator loss=0.683 , generator loss=0.716\n",
      "Training progress in epoch #14, step 131, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #14, step 132, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #14, step 133, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #14, step 134, discriminator loss=0.684 , generator loss=0.689\n",
      "Training progress in epoch #14, step 135, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #14, step 136, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #14, step 137, discriminator loss=0.683 , generator loss=0.720\n",
      "Training progress in epoch #14, step 138, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #14, step 139, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #14, step 140, discriminator loss=0.699 , generator loss=0.739\n",
      "Training progress in epoch #14, step 141, discriminator loss=0.691 , generator loss=0.748\n",
      "Training progress in epoch #14, step 142, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #14, step 143, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #14, step 144, discriminator loss=0.681 , generator loss=0.740\n",
      "Training progress in epoch #14, step 145, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #14, step 146, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #14, step 147, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #14, step 148, discriminator loss=0.678 , generator loss=0.733\n",
      "Training progress in epoch #14, step 149, discriminator loss=0.688 , generator loss=0.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #14, step 150, discriminator loss=0.685 , generator loss=0.690\n",
      "Training progress in epoch #14, step 151, discriminator loss=0.686 , generator loss=0.685\n",
      "Training progress in epoch #14, step 152, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #14, step 153, discriminator loss=0.681 , generator loss=0.694\n",
      "Training progress in epoch #14, step 154, discriminator loss=0.684 , generator loss=0.701\n",
      "Training progress in epoch #14, step 155, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #14, step 156, discriminator loss=0.693 , generator loss=0.739\n",
      "Training progress in epoch #14, step 157, discriminator loss=0.693 , generator loss=0.752\n",
      "Training progress in epoch #14, step 158, discriminator loss=0.686 , generator loss=0.735\n",
      "Training progress in epoch #14, step 159, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #14, step 160, discriminator loss=0.681 , generator loss=0.695\n",
      "Training progress in epoch #14, step 161, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #14, step 162, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #14, step 163, discriminator loss=0.687 , generator loss=0.726\n",
      "Training progress in epoch #14, step 164, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #14, step 165, discriminator loss=0.686 , generator loss=0.678\n",
      "Training progress in epoch #14, step 166, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #14, step 167, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #14, step 168, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #14, step 169, discriminator loss=0.680 , generator loss=0.701\n",
      "Training progress in epoch #14, step 170, discriminator loss=0.681 , generator loss=0.698\n",
      "Training progress in epoch #14, step 171, discriminator loss=0.680 , generator loss=0.685\n",
      "Training progress in epoch #14, step 172, discriminator loss=0.676 , generator loss=0.696\n",
      "Training progress in epoch #14, step 173, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #14, step 174, discriminator loss=0.690 , generator loss=0.747\n",
      "Training progress in epoch #14, step 175, discriminator loss=0.693 , generator loss=0.750\n",
      "Training progress in epoch #14, step 176, discriminator loss=0.682 , generator loss=0.741\n",
      "Training progress in epoch #14, step 177, discriminator loss=0.687 , generator loss=0.726\n",
      "Training progress in epoch #14, step 178, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #14, step 179, discriminator loss=0.699 , generator loss=0.736\n",
      "Training progress in epoch #14, step 180, discriminator loss=0.696 , generator loss=0.713\n",
      "Training progress in epoch #14, step 181, discriminator loss=0.690 , generator loss=0.671\n",
      "Training progress in epoch #14, step 182, discriminator loss=0.683 , generator loss=0.659\n",
      "Training progress in epoch #14, step 183, discriminator loss=0.697 , generator loss=0.667\n",
      "Training progress in epoch #14, step 184, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #14, step 185, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #14, step 186, discriminator loss=0.683 , generator loss=0.674\n",
      "Training progress in epoch #14, step 187, discriminator loss=0.681 , generator loss=0.703\n",
      "Training progress in epoch #14, step 188, discriminator loss=0.686 , generator loss=0.746\n",
      "Training progress in epoch #14, step 189, discriminator loss=0.683 , generator loss=0.748\n",
      "Training progress in epoch #14, step 190, discriminator loss=0.695 , generator loss=0.742\n",
      "Training progress in epoch #14, step 191, discriminator loss=0.691 , generator loss=0.734\n",
      "Training progress in epoch #14, step 192, discriminator loss=0.683 , generator loss=0.740\n",
      "Training progress in epoch #14, step 193, discriminator loss=0.684 , generator loss=0.758\n",
      "Training progress in epoch #14, step 194, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #14, step 195, discriminator loss=0.691 , generator loss=0.677\n",
      "Training progress in epoch #14, step 196, discriminator loss=0.687 , generator loss=0.673\n",
      "Training progress in epoch #14, step 197, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #14, step 198, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #14, step 199, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #14, step 200, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #14, step 201, discriminator loss=0.685 , generator loss=0.682\n",
      "Training progress in epoch #14, step 202, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #14, step 203, discriminator loss=0.683 , generator loss=0.717\n",
      "Training progress in epoch #14, step 204, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #14, step 205, discriminator loss=0.685 , generator loss=0.752\n",
      "Training progress in epoch #14, step 206, discriminator loss=0.681 , generator loss=0.754\n",
      "Training progress in epoch #14, step 207, discriminator loss=0.686 , generator loss=0.749\n",
      "Training progress in epoch #14, step 208, discriminator loss=0.684 , generator loss=0.742\n",
      "Training progress in epoch #14, step 209, discriminator loss=0.685 , generator loss=0.764\n",
      "Training progress in epoch #14, step 210, discriminator loss=0.682 , generator loss=0.735\n",
      "Training progress in epoch #14, step 211, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #14, step 212, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #14, step 213, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #14, step 214, discriminator loss=0.691 , generator loss=0.675\n",
      "Training progress in epoch #14, step 215, discriminator loss=0.693 , generator loss=0.654\n",
      "Training progress in epoch #14, step 216, discriminator loss=0.693 , generator loss=0.670\n",
      "Training progress in epoch #14, step 217, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #14, step 218, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #14, step 219, discriminator loss=0.679 , generator loss=0.700\n",
      "Training progress in epoch #14, step 220, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #14, step 221, discriminator loss=0.697 , generator loss=0.739\n",
      "Training progress in epoch #14, step 222, discriminator loss=0.701 , generator loss=0.776\n",
      "Training progress in epoch #14, step 223, discriminator loss=0.697 , generator loss=0.797\n",
      "Training progress in epoch #14, step 224, discriminator loss=0.692 , generator loss=0.772\n",
      "Training progress in epoch #14, step 225, discriminator loss=0.688 , generator loss=0.768\n",
      "Training progress in epoch #14, step 226, discriminator loss=0.679 , generator loss=0.768\n",
      "Training progress in epoch #14, step 227, discriminator loss=0.684 , generator loss=0.738\n",
      "Training progress in epoch #14, step 228, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #14, step 229, discriminator loss=0.695 , generator loss=0.657\n",
      "Training progress in epoch #14, step 230, discriminator loss=0.697 , generator loss=0.636\n",
      "Training progress in epoch #14, step 231, discriminator loss=0.691 , generator loss=0.640\n",
      "Training progress in epoch #14, step 232, discriminator loss=0.691 , generator loss=0.664\n",
      "Training progress in epoch #14, step 233, discriminator loss=0.683 , generator loss=0.666\n",
      "Disciminator Accuracy on real images: 96%, on fake images: 21%\n",
      "Training progress in epoch #15, step 0, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #15, step 1, discriminator loss=0.680 , generator loss=0.707\n",
      "Training progress in epoch #15, step 2, discriminator loss=0.698 , generator loss=0.736\n",
      "Training progress in epoch #15, step 3, discriminator loss=0.688 , generator loss=0.740\n",
      "Training progress in epoch #15, step 4, discriminator loss=0.697 , generator loss=0.749\n",
      "Training progress in epoch #15, step 5, discriminator loss=0.692 , generator loss=0.766\n",
      "Training progress in epoch #15, step 6, discriminator loss=0.698 , generator loss=0.769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #15, step 7, discriminator loss=0.692 , generator loss=0.761\n",
      "Training progress in epoch #15, step 8, discriminator loss=0.684 , generator loss=0.723\n",
      "Training progress in epoch #15, step 9, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #15, step 10, discriminator loss=0.683 , generator loss=0.703\n",
      "Training progress in epoch #15, step 11, discriminator loss=0.686 , generator loss=0.672\n",
      "Training progress in epoch #15, step 12, discriminator loss=0.699 , generator loss=0.641\n",
      "Training progress in epoch #15, step 13, discriminator loss=0.696 , generator loss=0.649\n",
      "Training progress in epoch #15, step 14, discriminator loss=0.690 , generator loss=0.673\n",
      "Training progress in epoch #15, step 15, discriminator loss=0.685 , generator loss=0.686\n",
      "Training progress in epoch #15, step 16, discriminator loss=0.679 , generator loss=0.680\n",
      "Training progress in epoch #15, step 17, discriminator loss=0.679 , generator loss=0.701\n",
      "Training progress in epoch #15, step 18, discriminator loss=0.682 , generator loss=0.724\n",
      "Training progress in epoch #15, step 19, discriminator loss=0.696 , generator loss=0.752\n",
      "Training progress in epoch #15, step 20, discriminator loss=0.692 , generator loss=0.756\n",
      "Training progress in epoch #15, step 21, discriminator loss=0.690 , generator loss=0.745\n",
      "Training progress in epoch #15, step 22, discriminator loss=0.695 , generator loss=0.762\n",
      "Training progress in epoch #15, step 23, discriminator loss=0.693 , generator loss=0.766\n",
      "Training progress in epoch #15, step 24, discriminator loss=0.695 , generator loss=0.748\n",
      "Training progress in epoch #15, step 25, discriminator loss=0.684 , generator loss=0.732\n",
      "Training progress in epoch #15, step 26, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #15, step 27, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #15, step 28, discriminator loss=0.693 , generator loss=0.658\n",
      "Training progress in epoch #15, step 29, discriminator loss=0.691 , generator loss=0.674\n",
      "Training progress in epoch #15, step 30, discriminator loss=0.688 , generator loss=0.671\n",
      "Training progress in epoch #15, step 31, discriminator loss=0.691 , generator loss=0.657\n",
      "Training progress in epoch #15, step 32, discriminator loss=0.692 , generator loss=0.675\n",
      "Training progress in epoch #15, step 33, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #15, step 34, discriminator loss=0.684 , generator loss=0.726\n",
      "Training progress in epoch #15, step 35, discriminator loss=0.683 , generator loss=0.722\n",
      "Training progress in epoch #15, step 36, discriminator loss=0.684 , generator loss=0.728\n",
      "Training progress in epoch #15, step 37, discriminator loss=0.686 , generator loss=0.749\n",
      "Training progress in epoch #15, step 38, discriminator loss=0.681 , generator loss=0.753\n",
      "Training progress in epoch #15, step 39, discriminator loss=0.689 , generator loss=0.741\n",
      "Training progress in epoch #15, step 40, discriminator loss=0.684 , generator loss=0.731\n",
      "Training progress in epoch #15, step 41, discriminator loss=0.677 , generator loss=0.738\n",
      "Training progress in epoch #15, step 42, discriminator loss=0.686 , generator loss=0.737\n",
      "Training progress in epoch #15, step 43, discriminator loss=0.684 , generator loss=0.731\n",
      "Training progress in epoch #15, step 44, discriminator loss=0.694 , generator loss=0.679\n",
      "Training progress in epoch #15, step 45, discriminator loss=0.683 , generator loss=0.647\n",
      "Training progress in epoch #15, step 46, discriminator loss=0.689 , generator loss=0.645\n",
      "Training progress in epoch #15, step 47, discriminator loss=0.687 , generator loss=0.669\n",
      "Training progress in epoch #15, step 48, discriminator loss=0.691 , generator loss=0.667\n",
      "Training progress in epoch #15, step 49, discriminator loss=0.686 , generator loss=0.681\n",
      "Training progress in epoch #15, step 50, discriminator loss=0.687 , generator loss=0.674\n",
      "Training progress in epoch #15, step 51, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #15, step 52, discriminator loss=0.695 , generator loss=0.745\n",
      "Training progress in epoch #15, step 53, discriminator loss=0.690 , generator loss=0.742\n",
      "Training progress in epoch #15, step 54, discriminator loss=0.686 , generator loss=0.734\n",
      "Training progress in epoch #15, step 55, discriminator loss=0.689 , generator loss=0.762\n",
      "Training progress in epoch #15, step 56, discriminator loss=0.685 , generator loss=0.772\n",
      "Training progress in epoch #15, step 57, discriminator loss=0.686 , generator loss=0.743\n",
      "Training progress in epoch #15, step 58, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #15, step 59, discriminator loss=0.674 , generator loss=0.701\n",
      "Training progress in epoch #15, step 60, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #15, step 61, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #15, step 62, discriminator loss=0.694 , generator loss=0.676\n",
      "Training progress in epoch #15, step 63, discriminator loss=0.688 , generator loss=0.656\n",
      "Training progress in epoch #15, step 64, discriminator loss=0.682 , generator loss=0.680\n",
      "Training progress in epoch #15, step 65, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #15, step 66, discriminator loss=0.688 , generator loss=0.733\n",
      "Training progress in epoch #15, step 67, discriminator loss=0.686 , generator loss=0.733\n",
      "Training progress in epoch #15, step 68, discriminator loss=0.684 , generator loss=0.729\n",
      "Training progress in epoch #15, step 69, discriminator loss=0.688 , generator loss=0.731\n",
      "Training progress in epoch #15, step 70, discriminator loss=0.686 , generator loss=0.745\n",
      "Training progress in epoch #15, step 71, discriminator loss=0.690 , generator loss=0.746\n",
      "Training progress in epoch #15, step 72, discriminator loss=0.689 , generator loss=0.735\n",
      "Training progress in epoch #15, step 73, discriminator loss=0.685 , generator loss=0.723\n",
      "Training progress in epoch #15, step 74, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #15, step 75, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #15, step 76, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #15, step 77, discriminator loss=0.683 , generator loss=0.686\n",
      "Training progress in epoch #15, step 78, discriminator loss=0.694 , generator loss=0.674\n",
      "Training progress in epoch #15, step 79, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #15, step 80, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #15, step 81, discriminator loss=0.684 , generator loss=0.701\n",
      "Training progress in epoch #15, step 82, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #15, step 83, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #15, step 84, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #15, step 85, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #15, step 86, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #15, step 87, discriminator loss=0.693 , generator loss=0.726\n",
      "Training progress in epoch #15, step 88, discriminator loss=0.684 , generator loss=0.752\n",
      "Training progress in epoch #15, step 89, discriminator loss=0.681 , generator loss=0.744\n",
      "Training progress in epoch #15, step 90, discriminator loss=0.678 , generator loss=0.716\n",
      "Training progress in epoch #15, step 91, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #15, step 92, discriminator loss=0.687 , generator loss=0.660\n",
      "Training progress in epoch #15, step 93, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #15, step 94, discriminator loss=0.683 , generator loss=0.682\n",
      "Training progress in epoch #15, step 95, discriminator loss=0.681 , generator loss=0.684\n",
      "Training progress in epoch #15, step 96, discriminator loss=0.682 , generator loss=0.677\n",
      "Training progress in epoch #15, step 97, discriminator loss=0.681 , generator loss=0.677\n",
      "Training progress in epoch #15, step 98, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #15, step 99, discriminator loss=0.686 , generator loss=0.706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #15, step 100, discriminator loss=0.685 , generator loss=0.724\n",
      "Training progress in epoch #15, step 101, discriminator loss=0.684 , generator loss=0.736\n",
      "Training progress in epoch #15, step 102, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #15, step 103, discriminator loss=0.696 , generator loss=0.731\n",
      "Training progress in epoch #15, step 104, discriminator loss=0.688 , generator loss=0.737\n",
      "Training progress in epoch #15, step 105, discriminator loss=0.686 , generator loss=0.766\n",
      "Training progress in epoch #15, step 106, discriminator loss=0.687 , generator loss=0.750\n",
      "Training progress in epoch #15, step 107, discriminator loss=0.683 , generator loss=0.752\n",
      "Training progress in epoch #15, step 108, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #15, step 109, discriminator loss=0.686 , generator loss=0.678\n",
      "Training progress in epoch #15, step 110, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #15, step 111, discriminator loss=0.683 , generator loss=0.661\n",
      "Training progress in epoch #15, step 112, discriminator loss=0.697 , generator loss=0.656\n",
      "Training progress in epoch #15, step 113, discriminator loss=0.680 , generator loss=0.662\n",
      "Training progress in epoch #15, step 114, discriminator loss=0.684 , generator loss=0.695\n",
      "Training progress in epoch #15, step 115, discriminator loss=0.681 , generator loss=0.709\n",
      "Training progress in epoch #15, step 116, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #15, step 117, discriminator loss=0.680 , generator loss=0.714\n",
      "Training progress in epoch #15, step 118, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #15, step 119, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #15, step 120, discriminator loss=0.687 , generator loss=0.749\n",
      "Training progress in epoch #15, step 121, discriminator loss=0.690 , generator loss=0.760\n",
      "Training progress in epoch #15, step 122, discriminator loss=0.686 , generator loss=0.759\n",
      "Training progress in epoch #15, step 123, discriminator loss=0.686 , generator loss=0.732\n",
      "Training progress in epoch #15, step 124, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #15, step 125, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #15, step 126, discriminator loss=0.692 , generator loss=0.675\n",
      "Training progress in epoch #15, step 127, discriminator loss=0.698 , generator loss=0.651\n",
      "Training progress in epoch #15, step 128, discriminator loss=0.691 , generator loss=0.667\n",
      "Training progress in epoch #15, step 129, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #15, step 130, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #15, step 131, discriminator loss=0.684 , generator loss=0.678\n",
      "Training progress in epoch #15, step 132, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #15, step 133, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #15, step 134, discriminator loss=0.689 , generator loss=0.747\n",
      "Training progress in epoch #15, step 135, discriminator loss=0.691 , generator loss=0.750\n",
      "Training progress in epoch #15, step 136, discriminator loss=0.684 , generator loss=0.716\n",
      "Training progress in epoch #15, step 137, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #15, step 138, discriminator loss=0.694 , generator loss=0.768\n",
      "Training progress in epoch #15, step 139, discriminator loss=0.695 , generator loss=0.782\n",
      "Training progress in epoch #15, step 140, discriminator loss=0.684 , generator loss=0.724\n",
      "Training progress in epoch #15, step 141, discriminator loss=0.686 , generator loss=0.666\n",
      "Training progress in epoch #15, step 142, discriminator loss=0.687 , generator loss=0.652\n",
      "Training progress in epoch #15, step 143, discriminator loss=0.699 , generator loss=0.682\n",
      "Training progress in epoch #15, step 144, discriminator loss=0.689 , generator loss=0.664\n",
      "Training progress in epoch #15, step 145, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #15, step 146, discriminator loss=0.694 , generator loss=0.675\n",
      "Training progress in epoch #15, step 147, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #15, step 148, discriminator loss=0.682 , generator loss=0.707\n",
      "Training progress in epoch #15, step 149, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #15, step 150, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #15, step 151, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #15, step 152, discriminator loss=0.688 , generator loss=0.751\n",
      "Training progress in epoch #15, step 153, discriminator loss=0.687 , generator loss=0.777\n",
      "Training progress in epoch #15, step 154, discriminator loss=0.681 , generator loss=0.746\n",
      "Training progress in epoch #15, step 155, discriminator loss=0.688 , generator loss=0.738\n",
      "Training progress in epoch #15, step 156, discriminator loss=0.685 , generator loss=0.730\n",
      "Training progress in epoch #15, step 157, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #15, step 158, discriminator loss=0.688 , generator loss=0.668\n",
      "Training progress in epoch #15, step 159, discriminator loss=0.703 , generator loss=0.681\n",
      "Training progress in epoch #15, step 160, discriminator loss=0.684 , generator loss=0.673\n",
      "Training progress in epoch #15, step 161, discriminator loss=0.688 , generator loss=0.673\n",
      "Training progress in epoch #15, step 162, discriminator loss=0.685 , generator loss=0.667\n",
      "Training progress in epoch #15, step 163, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #15, step 164, discriminator loss=0.683 , generator loss=0.746\n",
      "Training progress in epoch #15, step 165, discriminator loss=0.692 , generator loss=0.733\n",
      "Training progress in epoch #15, step 166, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #15, step 167, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #15, step 168, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #15, step 169, discriminator loss=0.685 , generator loss=0.743\n",
      "Training progress in epoch #15, step 170, discriminator loss=0.690 , generator loss=0.761\n",
      "Training progress in epoch #15, step 171, discriminator loss=0.683 , generator loss=0.751\n",
      "Training progress in epoch #15, step 172, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #15, step 173, discriminator loss=0.699 , generator loss=0.701\n",
      "Training progress in epoch #15, step 174, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #15, step 175, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #15, step 176, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #15, step 177, discriminator loss=0.685 , generator loss=0.682\n",
      "Training progress in epoch #15, step 178, discriminator loss=0.689 , generator loss=0.677\n",
      "Training progress in epoch #15, step 179, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #15, step 180, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #15, step 181, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #15, step 182, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #15, step 183, discriminator loss=0.689 , generator loss=0.763\n",
      "Training progress in epoch #15, step 184, discriminator loss=0.692 , generator loss=0.750\n",
      "Training progress in epoch #15, step 185, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #15, step 186, discriminator loss=0.691 , generator loss=0.748\n",
      "Training progress in epoch #15, step 187, discriminator loss=0.690 , generator loss=0.764\n",
      "Training progress in epoch #15, step 188, discriminator loss=0.684 , generator loss=0.748\n",
      "Training progress in epoch #15, step 189, discriminator loss=0.677 , generator loss=0.718\n",
      "Training progress in epoch #15, step 190, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #15, step 191, discriminator loss=0.686 , generator loss=0.677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #15, step 192, discriminator loss=0.689 , generator loss=0.675\n",
      "Training progress in epoch #15, step 193, discriminator loss=0.687 , generator loss=0.677\n",
      "Training progress in epoch #15, step 194, discriminator loss=0.681 , generator loss=0.688\n",
      "Training progress in epoch #15, step 195, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #15, step 196, discriminator loss=0.685 , generator loss=0.721\n",
      "Training progress in epoch #15, step 197, discriminator loss=0.688 , generator loss=0.732\n",
      "Training progress in epoch #15, step 198, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #15, step 199, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #15, step 200, discriminator loss=0.690 , generator loss=0.729\n",
      "Training progress in epoch #15, step 201, discriminator loss=0.699 , generator loss=0.753\n",
      "Training progress in epoch #15, step 202, discriminator loss=0.681 , generator loss=0.753\n",
      "Training progress in epoch #15, step 203, discriminator loss=0.694 , generator loss=0.736\n",
      "Training progress in epoch #15, step 204, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #15, step 205, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #15, step 206, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #15, step 207, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #15, step 208, discriminator loss=0.687 , generator loss=0.664\n",
      "Training progress in epoch #15, step 209, discriminator loss=0.685 , generator loss=0.664\n",
      "Training progress in epoch #15, step 210, discriminator loss=0.674 , generator loss=0.692\n",
      "Training progress in epoch #15, step 211, discriminator loss=0.682 , generator loss=0.695\n",
      "Training progress in epoch #15, step 212, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #15, step 213, discriminator loss=0.681 , generator loss=0.739\n",
      "Training progress in epoch #15, step 214, discriminator loss=0.688 , generator loss=0.759\n",
      "Training progress in epoch #15, step 215, discriminator loss=0.699 , generator loss=0.755\n",
      "Training progress in epoch #15, step 216, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #15, step 217, discriminator loss=0.683 , generator loss=0.727\n",
      "Training progress in epoch #15, step 218, discriminator loss=0.684 , generator loss=0.733\n",
      "Training progress in epoch #15, step 219, discriminator loss=0.682 , generator loss=0.726\n",
      "Training progress in epoch #15, step 220, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #15, step 221, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #15, step 222, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #15, step 223, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #15, step 224, discriminator loss=0.699 , generator loss=0.688\n",
      "Training progress in epoch #15, step 225, discriminator loss=0.684 , generator loss=0.668\n",
      "Training progress in epoch #15, step 226, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #15, step 227, discriminator loss=0.679 , generator loss=0.701\n",
      "Training progress in epoch #15, step 228, discriminator loss=0.694 , generator loss=0.735\n",
      "Training progress in epoch #15, step 229, discriminator loss=0.686 , generator loss=0.744\n",
      "Training progress in epoch #15, step 230, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #15, step 231, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #15, step 232, discriminator loss=0.681 , generator loss=0.729\n",
      "Training progress in epoch #15, step 233, discriminator loss=0.689 , generator loss=0.726\n",
      "Disciminator Accuracy on real images: 45%, on fake images: 76%\n",
      "Training progress in epoch #16, step 0, discriminator loss=0.687 , generator loss=0.730\n",
      "Training progress in epoch #16, step 1, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #16, step 2, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #16, step 3, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #16, step 4, discriminator loss=0.684 , generator loss=0.672\n",
      "Training progress in epoch #16, step 5, discriminator loss=0.683 , generator loss=0.687\n",
      "Training progress in epoch #16, step 6, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #16, step 7, discriminator loss=0.684 , generator loss=0.694\n",
      "Training progress in epoch #16, step 8, discriminator loss=0.684 , generator loss=0.698\n",
      "Training progress in epoch #16, step 9, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #16, step 10, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #16, step 11, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #16, step 12, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #16, step 13, discriminator loss=0.692 , generator loss=0.728\n",
      "Training progress in epoch #16, step 14, discriminator loss=0.688 , generator loss=0.725\n",
      "Training progress in epoch #16, step 15, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #16, step 16, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #16, step 17, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #16, step 18, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #16, step 19, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #16, step 20, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #16, step 21, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #16, step 22, discriminator loss=0.684 , generator loss=0.687\n",
      "Training progress in epoch #16, step 23, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #16, step 24, discriminator loss=0.685 , generator loss=0.676\n",
      "Training progress in epoch #16, step 25, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #16, step 26, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #16, step 27, discriminator loss=0.696 , generator loss=0.725\n",
      "Training progress in epoch #16, step 28, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #16, step 29, discriminator loss=0.681 , generator loss=0.720\n",
      "Training progress in epoch #16, step 30, discriminator loss=0.685 , generator loss=0.732\n",
      "Training progress in epoch #16, step 31, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #16, step 32, discriminator loss=0.687 , generator loss=0.738\n",
      "Training progress in epoch #16, step 33, discriminator loss=0.681 , generator loss=0.742\n",
      "Training progress in epoch #16, step 34, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #16, step 35, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #16, step 36, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #16, step 37, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #16, step 38, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #16, step 39, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #16, step 40, discriminator loss=0.686 , generator loss=0.665\n",
      "Training progress in epoch #16, step 41, discriminator loss=0.698 , generator loss=0.680\n",
      "Training progress in epoch #16, step 42, discriminator loss=0.681 , generator loss=0.696\n",
      "Training progress in epoch #16, step 43, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #16, step 44, discriminator loss=0.685 , generator loss=0.701\n",
      "Training progress in epoch #16, step 45, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #16, step 46, discriminator loss=0.688 , generator loss=0.756\n",
      "Training progress in epoch #16, step 47, discriminator loss=0.696 , generator loss=0.774\n",
      "Training progress in epoch #16, step 48, discriminator loss=0.692 , generator loss=0.762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #16, step 49, discriminator loss=0.686 , generator loss=0.727\n",
      "Training progress in epoch #16, step 50, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #16, step 51, discriminator loss=0.682 , generator loss=0.726\n",
      "Training progress in epoch #16, step 52, discriminator loss=0.683 , generator loss=0.701\n",
      "Training progress in epoch #16, step 53, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #16, step 54, discriminator loss=0.689 , generator loss=0.675\n",
      "Training progress in epoch #16, step 55, discriminator loss=0.691 , generator loss=0.675\n",
      "Training progress in epoch #16, step 56, discriminator loss=0.694 , generator loss=0.664\n",
      "Training progress in epoch #16, step 57, discriminator loss=0.693 , generator loss=0.652\n",
      "Training progress in epoch #16, step 58, discriminator loss=0.693 , generator loss=0.676\n",
      "Training progress in epoch #16, step 59, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #16, step 60, discriminator loss=0.686 , generator loss=0.728\n",
      "Training progress in epoch #16, step 61, discriminator loss=0.685 , generator loss=0.737\n",
      "Training progress in epoch #16, step 62, discriminator loss=0.693 , generator loss=0.739\n",
      "Training progress in epoch #16, step 63, discriminator loss=0.689 , generator loss=0.777\n",
      "Training progress in epoch #16, step 64, discriminator loss=0.689 , generator loss=0.789\n",
      "Training progress in epoch #16, step 65, discriminator loss=0.694 , generator loss=0.779\n",
      "Training progress in epoch #16, step 66, discriminator loss=0.686 , generator loss=0.744\n",
      "Training progress in epoch #16, step 67, discriminator loss=0.680 , generator loss=0.711\n",
      "Training progress in epoch #16, step 68, discriminator loss=0.688 , generator loss=0.664\n",
      "Training progress in epoch #16, step 69, discriminator loss=0.696 , generator loss=0.651\n",
      "Training progress in epoch #16, step 70, discriminator loss=0.688 , generator loss=0.652\n",
      "Training progress in epoch #16, step 71, discriminator loss=0.684 , generator loss=0.658\n",
      "Training progress in epoch #16, step 72, discriminator loss=0.688 , generator loss=0.675\n",
      "Training progress in epoch #16, step 73, discriminator loss=0.679 , generator loss=0.678\n",
      "Training progress in epoch #16, step 74, discriminator loss=0.685 , generator loss=0.686\n",
      "Training progress in epoch #16, step 75, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #16, step 76, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #16, step 77, discriminator loss=0.700 , generator loss=0.743\n",
      "Training progress in epoch #16, step 78, discriminator loss=0.693 , generator loss=0.733\n",
      "Training progress in epoch #16, step 79, discriminator loss=0.690 , generator loss=0.764\n",
      "Training progress in epoch #16, step 80, discriminator loss=0.694 , generator loss=0.784\n",
      "Training progress in epoch #16, step 81, discriminator loss=0.685 , generator loss=0.783\n",
      "Training progress in epoch #16, step 82, discriminator loss=0.691 , generator loss=0.756\n",
      "Training progress in epoch #16, step 83, discriminator loss=0.677 , generator loss=0.735\n",
      "Training progress in epoch #16, step 84, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #16, step 85, discriminator loss=0.690 , generator loss=0.658\n",
      "Training progress in epoch #16, step 86, discriminator loss=0.690 , generator loss=0.643\n",
      "Training progress in epoch #16, step 87, discriminator loss=0.695 , generator loss=0.634\n",
      "Training progress in epoch #16, step 88, discriminator loss=0.685 , generator loss=0.650\n",
      "Training progress in epoch #16, step 89, discriminator loss=0.685 , generator loss=0.677\n",
      "Training progress in epoch #16, step 90, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #16, step 91, discriminator loss=0.686 , generator loss=0.724\n",
      "Training progress in epoch #16, step 92, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #16, step 93, discriminator loss=0.690 , generator loss=0.736\n",
      "Training progress in epoch #16, step 94, discriminator loss=0.687 , generator loss=0.754\n",
      "Training progress in epoch #16, step 95, discriminator loss=0.693 , generator loss=0.747\n",
      "Training progress in epoch #16, step 96, discriminator loss=0.684 , generator loss=0.773\n",
      "Training progress in epoch #16, step 97, discriminator loss=0.685 , generator loss=0.760\n",
      "Training progress in epoch #16, step 98, discriminator loss=0.692 , generator loss=0.762\n",
      "Training progress in epoch #16, step 99, discriminator loss=0.686 , generator loss=0.744\n",
      "Training progress in epoch #16, step 100, discriminator loss=0.680 , generator loss=0.739\n",
      "Training progress in epoch #16, step 101, discriminator loss=0.674 , generator loss=0.706\n",
      "Training progress in epoch #16, step 102, discriminator loss=0.683 , generator loss=0.698\n",
      "Training progress in epoch #16, step 103, discriminator loss=0.700 , generator loss=0.668\n",
      "Training progress in epoch #16, step 104, discriminator loss=0.690 , generator loss=0.654\n",
      "Training progress in epoch #16, step 105, discriminator loss=0.682 , generator loss=0.663\n",
      "Training progress in epoch #16, step 106, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #16, step 107, discriminator loss=0.682 , generator loss=0.692\n",
      "Training progress in epoch #16, step 108, discriminator loss=0.674 , generator loss=0.709\n",
      "Training progress in epoch #16, step 109, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #16, step 110, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #16, step 111, discriminator loss=0.696 , generator loss=0.744\n",
      "Training progress in epoch #16, step 112, discriminator loss=0.695 , generator loss=0.748\n",
      "Training progress in epoch #16, step 113, discriminator loss=0.691 , generator loss=0.777\n",
      "Training progress in epoch #16, step 114, discriminator loss=0.692 , generator loss=0.789\n",
      "Training progress in epoch #16, step 115, discriminator loss=0.684 , generator loss=0.746\n",
      "Training progress in epoch #16, step 116, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #16, step 117, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #16, step 118, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #16, step 119, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #16, step 120, discriminator loss=0.690 , generator loss=0.665\n",
      "Training progress in epoch #16, step 121, discriminator loss=0.689 , generator loss=0.669\n",
      "Training progress in epoch #16, step 122, discriminator loss=0.686 , generator loss=0.672\n",
      "Training progress in epoch #16, step 123, discriminator loss=0.689 , generator loss=0.669\n",
      "Training progress in epoch #16, step 124, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #16, step 125, discriminator loss=0.683 , generator loss=0.726\n",
      "Training progress in epoch #16, step 126, discriminator loss=0.687 , generator loss=0.742\n",
      "Training progress in epoch #16, step 127, discriminator loss=0.680 , generator loss=0.735\n",
      "Training progress in epoch #16, step 128, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #16, step 129, discriminator loss=0.688 , generator loss=0.746\n",
      "Training progress in epoch #16, step 130, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #16, step 131, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #16, step 132, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #16, step 133, discriminator loss=0.694 , generator loss=0.735\n",
      "Training progress in epoch #16, step 134, discriminator loss=0.695 , generator loss=0.723\n",
      "Training progress in epoch #16, step 135, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #16, step 136, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #16, step 137, discriminator loss=0.686 , generator loss=0.683\n",
      "Training progress in epoch #16, step 138, discriminator loss=0.686 , generator loss=0.684\n",
      "Training progress in epoch #16, step 139, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #16, step 140, discriminator loss=0.687 , generator loss=0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #16, step 141, discriminator loss=0.680 , generator loss=0.706\n",
      "Training progress in epoch #16, step 142, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #16, step 143, discriminator loss=0.686 , generator loss=0.732\n",
      "Training progress in epoch #16, step 144, discriminator loss=0.693 , generator loss=0.764\n",
      "Training progress in epoch #16, step 145, discriminator loss=0.685 , generator loss=0.746\n",
      "Training progress in epoch #16, step 146, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #16, step 147, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #16, step 148, discriminator loss=0.680 , generator loss=0.710\n",
      "Training progress in epoch #16, step 149, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #16, step 150, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #16, step 151, discriminator loss=0.696 , generator loss=0.681\n",
      "Training progress in epoch #16, step 152, discriminator loss=0.688 , generator loss=0.670\n",
      "Training progress in epoch #16, step 153, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #16, step 154, discriminator loss=0.686 , generator loss=0.680\n",
      "Training progress in epoch #16, step 155, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #16, step 156, discriminator loss=0.684 , generator loss=0.689\n",
      "Training progress in epoch #16, step 157, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #16, step 158, discriminator loss=0.681 , generator loss=0.736\n",
      "Training progress in epoch #16, step 159, discriminator loss=0.688 , generator loss=0.753\n",
      "Training progress in epoch #16, step 160, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #16, step 161, discriminator loss=0.687 , generator loss=0.727\n",
      "Training progress in epoch #16, step 162, discriminator loss=0.684 , generator loss=0.728\n",
      "Training progress in epoch #16, step 163, discriminator loss=0.693 , generator loss=0.727\n",
      "Training progress in epoch #16, step 164, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #16, step 165, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #16, step 166, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #16, step 167, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #16, step 168, discriminator loss=0.699 , generator loss=0.703\n",
      "Training progress in epoch #16, step 169, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #16, step 170, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #16, step 171, discriminator loss=0.685 , generator loss=0.692\n",
      "Training progress in epoch #16, step 172, discriminator loss=0.682 , generator loss=0.693\n",
      "Training progress in epoch #16, step 173, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #16, step 174, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #16, step 175, discriminator loss=0.697 , generator loss=0.708\n",
      "Training progress in epoch #16, step 176, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #16, step 177, discriminator loss=0.685 , generator loss=0.723\n",
      "Training progress in epoch #16, step 178, discriminator loss=0.684 , generator loss=0.760\n",
      "Training progress in epoch #16, step 179, discriminator loss=0.680 , generator loss=0.739\n",
      "Training progress in epoch #16, step 180, discriminator loss=0.691 , generator loss=0.727\n",
      "Training progress in epoch #16, step 181, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #16, step 182, discriminator loss=0.682 , generator loss=0.710\n",
      "Training progress in epoch #16, step 183, discriminator loss=0.679 , generator loss=0.712\n",
      "Training progress in epoch #16, step 184, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #16, step 185, discriminator loss=0.689 , generator loss=0.662\n",
      "Training progress in epoch #16, step 186, discriminator loss=0.690 , generator loss=0.665\n",
      "Training progress in epoch #16, step 187, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #16, step 188, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #16, step 189, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #16, step 190, discriminator loss=0.680 , generator loss=0.706\n",
      "Training progress in epoch #16, step 191, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #16, step 192, discriminator loss=0.696 , generator loss=0.731\n",
      "Training progress in epoch #16, step 193, discriminator loss=0.691 , generator loss=0.732\n",
      "Training progress in epoch #16, step 194, discriminator loss=0.681 , generator loss=0.720\n",
      "Training progress in epoch #16, step 195, discriminator loss=0.688 , generator loss=0.745\n",
      "Training progress in epoch #16, step 196, discriminator loss=0.687 , generator loss=0.747\n",
      "Training progress in epoch #16, step 197, discriminator loss=0.683 , generator loss=0.761\n",
      "Training progress in epoch #16, step 198, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #16, step 199, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #16, step 200, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #16, step 201, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #16, step 202, discriminator loss=0.687 , generator loss=0.667\n",
      "Training progress in epoch #16, step 203, discriminator loss=0.677 , generator loss=0.681\n",
      "Training progress in epoch #16, step 204, discriminator loss=0.682 , generator loss=0.679\n",
      "Training progress in epoch #16, step 205, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #16, step 206, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #16, step 207, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #16, step 208, discriminator loss=0.685 , generator loss=0.726\n",
      "Training progress in epoch #16, step 209, discriminator loss=0.690 , generator loss=0.752\n",
      "Training progress in epoch #16, step 210, discriminator loss=0.686 , generator loss=0.751\n",
      "Training progress in epoch #16, step 211, discriminator loss=0.684 , generator loss=0.721\n",
      "Training progress in epoch #16, step 212, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #16, step 213, discriminator loss=0.686 , generator loss=0.735\n",
      "Training progress in epoch #16, step 214, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #16, step 215, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #16, step 216, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #16, step 217, discriminator loss=0.678 , generator loss=0.710\n",
      "Training progress in epoch #16, step 218, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #16, step 219, discriminator loss=0.685 , generator loss=0.678\n",
      "Training progress in epoch #16, step 220, discriminator loss=0.688 , generator loss=0.678\n",
      "Training progress in epoch #16, step 221, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #16, step 222, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #16, step 223, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #16, step 224, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #16, step 225, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #16, step 226, discriminator loss=0.690 , generator loss=0.759\n",
      "Training progress in epoch #16, step 227, discriminator loss=0.692 , generator loss=0.766\n",
      "Training progress in epoch #16, step 228, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #16, step 229, discriminator loss=0.684 , generator loss=0.697\n",
      "Training progress in epoch #16, step 230, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #16, step 231, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #16, step 232, discriminator loss=0.685 , generator loss=0.697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #16, step 233, discriminator loss=0.678 , generator loss=0.675\n",
      "Disciminator Accuracy on real images: 95%, on fake images: 21%\n",
      "Training progress in epoch #17, step 0, discriminator loss=0.681 , generator loss=0.686\n",
      "Training progress in epoch #17, step 1, discriminator loss=0.684 , generator loss=0.681\n",
      "Training progress in epoch #17, step 2, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #17, step 3, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #17, step 4, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #17, step 5, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #17, step 6, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #17, step 7, discriminator loss=0.690 , generator loss=0.742\n",
      "Training progress in epoch #17, step 8, discriminator loss=0.696 , generator loss=0.770\n",
      "Training progress in epoch #17, step 9, discriminator loss=0.691 , generator loss=0.777\n",
      "Training progress in epoch #17, step 10, discriminator loss=0.688 , generator loss=0.740\n",
      "Training progress in epoch #17, step 11, discriminator loss=0.686 , generator loss=0.757\n",
      "Training progress in epoch #17, step 12, discriminator loss=0.682 , generator loss=0.741\n",
      "Training progress in epoch #17, step 13, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #17, step 14, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #17, step 15, discriminator loss=0.693 , generator loss=0.655\n",
      "Training progress in epoch #17, step 16, discriminator loss=0.694 , generator loss=0.664\n",
      "Training progress in epoch #17, step 17, discriminator loss=0.685 , generator loss=0.677\n",
      "Training progress in epoch #17, step 18, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #17, step 19, discriminator loss=0.680 , generator loss=0.699\n",
      "Training progress in epoch #17, step 20, discriminator loss=0.692 , generator loss=0.747\n",
      "Training progress in epoch #17, step 21, discriminator loss=0.687 , generator loss=0.738\n",
      "Training progress in epoch #17, step 22, discriminator loss=0.681 , generator loss=0.735\n",
      "Training progress in epoch #17, step 23, discriminator loss=0.691 , generator loss=0.756\n",
      "Training progress in epoch #17, step 24, discriminator loss=0.696 , generator loss=0.727\n",
      "Training progress in epoch #17, step 25, discriminator loss=0.692 , generator loss=0.735\n",
      "Training progress in epoch #17, step 26, discriminator loss=0.694 , generator loss=0.744\n",
      "Training progress in epoch #17, step 27, discriminator loss=0.691 , generator loss=0.742\n",
      "Training progress in epoch #17, step 28, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #17, step 29, discriminator loss=0.699 , generator loss=0.688\n",
      "Training progress in epoch #17, step 30, discriminator loss=0.694 , generator loss=0.675\n",
      "Training progress in epoch #17, step 31, discriminator loss=0.684 , generator loss=0.662\n",
      "Training progress in epoch #17, step 32, discriminator loss=0.684 , generator loss=0.657\n",
      "Training progress in epoch #17, step 33, discriminator loss=0.686 , generator loss=0.677\n",
      "Training progress in epoch #17, step 34, discriminator loss=0.674 , generator loss=0.698\n",
      "Training progress in epoch #17, step 35, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #17, step 36, discriminator loss=0.680 , generator loss=0.689\n",
      "Training progress in epoch #17, step 37, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #17, step 38, discriminator loss=0.693 , generator loss=0.746\n",
      "Training progress in epoch #17, step 39, discriminator loss=0.690 , generator loss=0.768\n",
      "Training progress in epoch #17, step 40, discriminator loss=0.694 , generator loss=0.780\n",
      "Training progress in epoch #17, step 41, discriminator loss=0.685 , generator loss=0.748\n",
      "Training progress in epoch #17, step 42, discriminator loss=0.696 , generator loss=0.727\n",
      "Training progress in epoch #17, step 43, discriminator loss=0.693 , generator loss=0.735\n",
      "Training progress in epoch #17, step 44, discriminator loss=0.684 , generator loss=0.745\n",
      "Training progress in epoch #17, step 45, discriminator loss=0.689 , generator loss=0.732\n",
      "Training progress in epoch #17, step 46, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #17, step 47, discriminator loss=0.700 , generator loss=0.677\n",
      "Training progress in epoch #17, step 48, discriminator loss=0.690 , generator loss=0.674\n",
      "Training progress in epoch #17, step 49, discriminator loss=0.691 , generator loss=0.677\n",
      "Training progress in epoch #17, step 50, discriminator loss=0.683 , generator loss=0.701\n",
      "Training progress in epoch #17, step 51, discriminator loss=0.686 , generator loss=0.729\n",
      "Training progress in epoch #17, step 52, discriminator loss=0.689 , generator loss=0.732\n",
      "Training progress in epoch #17, step 53, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #17, step 54, discriminator loss=0.680 , generator loss=0.726\n",
      "Training progress in epoch #17, step 55, discriminator loss=0.683 , generator loss=0.733\n",
      "Training progress in epoch #17, step 56, discriminator loss=0.691 , generator loss=0.740\n",
      "Training progress in epoch #17, step 57, discriminator loss=0.699 , generator loss=0.747\n",
      "Training progress in epoch #17, step 58, discriminator loss=0.695 , generator loss=0.746\n",
      "Training progress in epoch #17, step 59, discriminator loss=0.697 , generator loss=0.713\n",
      "Training progress in epoch #17, step 60, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #17, step 61, discriminator loss=0.697 , generator loss=0.681\n",
      "Training progress in epoch #17, step 62, discriminator loss=0.698 , generator loss=0.690\n",
      "Training progress in epoch #17, step 63, discriminator loss=0.697 , generator loss=0.701\n",
      "Training progress in epoch #17, step 64, discriminator loss=0.698 , generator loss=0.709\n",
      "Training progress in epoch #17, step 65, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #17, step 66, discriminator loss=0.688 , generator loss=0.667\n",
      "Training progress in epoch #17, step 67, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #17, step 68, discriminator loss=0.682 , generator loss=0.684\n",
      "Training progress in epoch #17, step 69, discriminator loss=0.679 , generator loss=0.680\n",
      "Training progress in epoch #17, step 70, discriminator loss=0.678 , generator loss=0.678\n",
      "Training progress in epoch #17, step 71, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #17, step 72, discriminator loss=0.678 , generator loss=0.745\n",
      "Training progress in epoch #17, step 73, discriminator loss=0.693 , generator loss=0.748\n",
      "Training progress in epoch #17, step 74, discriminator loss=0.701 , generator loss=0.741\n",
      "Training progress in epoch #17, step 75, discriminator loss=0.698 , generator loss=0.734\n",
      "Training progress in epoch #17, step 76, discriminator loss=0.690 , generator loss=0.744\n",
      "Training progress in epoch #17, step 77, discriminator loss=0.693 , generator loss=0.730\n",
      "Training progress in epoch #17, step 78, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #17, step 79, discriminator loss=0.696 , generator loss=0.690\n",
      "Training progress in epoch #17, step 80, discriminator loss=0.702 , generator loss=0.690\n",
      "Training progress in epoch #17, step 81, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #17, step 82, discriminator loss=0.697 , generator loss=0.691\n",
      "Training progress in epoch #17, step 83, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #17, step 84, discriminator loss=0.685 , generator loss=0.688\n",
      "Training progress in epoch #17, step 85, discriminator loss=0.684 , generator loss=0.695\n",
      "Training progress in epoch #17, step 86, discriminator loss=0.685 , generator loss=0.729\n",
      "Training progress in epoch #17, step 87, discriminator loss=0.685 , generator loss=0.750\n",
      "Training progress in epoch #17, step 88, discriminator loss=0.684 , generator loss=0.745\n",
      "Training progress in epoch #17, step 89, discriminator loss=0.690 , generator loss=0.749\n",
      "Training progress in epoch #17, step 90, discriminator loss=0.690 , generator loss=0.746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #17, step 91, discriminator loss=0.690 , generator loss=0.749\n",
      "Training progress in epoch #17, step 92, discriminator loss=0.683 , generator loss=0.750\n",
      "Training progress in epoch #17, step 93, discriminator loss=0.680 , generator loss=0.738\n",
      "Training progress in epoch #17, step 94, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #17, step 95, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #17, step 96, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #17, step 97, discriminator loss=0.701 , generator loss=0.666\n",
      "Training progress in epoch #17, step 98, discriminator loss=0.695 , generator loss=0.676\n",
      "Training progress in epoch #17, step 99, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #17, step 100, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #17, step 101, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #17, step 102, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #17, step 103, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #17, step 104, discriminator loss=0.693 , generator loss=0.726\n",
      "Training progress in epoch #17, step 105, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #17, step 106, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #17, step 107, discriminator loss=0.682 , generator loss=0.733\n",
      "Training progress in epoch #17, step 108, discriminator loss=0.688 , generator loss=0.744\n",
      "Training progress in epoch #17, step 109, discriminator loss=0.682 , generator loss=0.749\n",
      "Training progress in epoch #17, step 110, discriminator loss=0.677 , generator loss=0.722\n",
      "Training progress in epoch #17, step 111, discriminator loss=0.685 , generator loss=0.690\n",
      "Training progress in epoch #17, step 112, discriminator loss=0.680 , generator loss=0.698\n",
      "Training progress in epoch #17, step 113, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #17, step 114, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #17, step 115, discriminator loss=0.686 , generator loss=0.685\n",
      "Training progress in epoch #17, step 116, discriminator loss=0.679 , generator loss=0.684\n",
      "Training progress in epoch #17, step 117, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #17, step 118, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #17, step 119, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #17, step 120, discriminator loss=0.681 , generator loss=0.737\n",
      "Training progress in epoch #17, step 121, discriminator loss=0.688 , generator loss=0.740\n",
      "Training progress in epoch #17, step 122, discriminator loss=0.697 , generator loss=0.766\n",
      "Training progress in epoch #17, step 123, discriminator loss=0.696 , generator loss=0.771\n",
      "Training progress in epoch #17, step 124, discriminator loss=0.689 , generator loss=0.779\n",
      "Training progress in epoch #17, step 125, discriminator loss=0.693 , generator loss=0.784\n",
      "Training progress in epoch #17, step 126, discriminator loss=0.685 , generator loss=0.789\n",
      "Training progress in epoch #17, step 127, discriminator loss=0.692 , generator loss=0.740\n",
      "Training progress in epoch #17, step 128, discriminator loss=0.699 , generator loss=0.692\n",
      "Training progress in epoch #17, step 129, discriminator loss=0.695 , generator loss=0.668\n",
      "Training progress in epoch #17, step 130, discriminator loss=0.691 , generator loss=0.672\n",
      "Training progress in epoch #17, step 131, discriminator loss=0.686 , generator loss=0.663\n",
      "Training progress in epoch #17, step 132, discriminator loss=0.682 , generator loss=0.665\n",
      "Training progress in epoch #17, step 133, discriminator loss=0.680 , generator loss=0.691\n",
      "Training progress in epoch #17, step 134, discriminator loss=0.677 , generator loss=0.691\n",
      "Training progress in epoch #17, step 135, discriminator loss=0.679 , generator loss=0.715\n",
      "Training progress in epoch #17, step 136, discriminator loss=0.681 , generator loss=0.704\n",
      "Training progress in epoch #17, step 137, discriminator loss=0.683 , generator loss=0.727\n",
      "Training progress in epoch #17, step 138, discriminator loss=0.695 , generator loss=0.764\n",
      "Training progress in epoch #17, step 139, discriminator loss=0.700 , generator loss=0.793\n",
      "Training progress in epoch #17, step 140, discriminator loss=0.707 , generator loss=0.767\n",
      "Training progress in epoch #17, step 141, discriminator loss=0.703 , generator loss=0.782\n",
      "Training progress in epoch #17, step 142, discriminator loss=0.697 , generator loss=0.792\n",
      "Training progress in epoch #17, step 143, discriminator loss=0.693 , generator loss=0.765\n",
      "Training progress in epoch #17, step 144, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #17, step 145, discriminator loss=0.699 , generator loss=0.696\n",
      "Training progress in epoch #17, step 146, discriminator loss=0.700 , generator loss=0.674\n",
      "Training progress in epoch #17, step 147, discriminator loss=0.702 , generator loss=0.640\n",
      "Training progress in epoch #17, step 148, discriminator loss=0.693 , generator loss=0.624\n",
      "Training progress in epoch #17, step 149, discriminator loss=0.691 , generator loss=0.639\n",
      "Training progress in epoch #17, step 150, discriminator loss=0.683 , generator loss=0.688\n",
      "Training progress in epoch #17, step 151, discriminator loss=0.669 , generator loss=0.713\n",
      "Training progress in epoch #17, step 152, discriminator loss=0.674 , generator loss=0.750\n",
      "Training progress in epoch #17, step 153, discriminator loss=0.674 , generator loss=0.763\n",
      "Training progress in epoch #17, step 154, discriminator loss=0.678 , generator loss=0.756\n",
      "Training progress in epoch #17, step 155, discriminator loss=0.688 , generator loss=0.771\n",
      "Training progress in epoch #17, step 156, discriminator loss=0.689 , generator loss=0.814\n",
      "Training progress in epoch #17, step 157, discriminator loss=0.686 , generator loss=0.791\n",
      "Training progress in epoch #17, step 158, discriminator loss=0.682 , generator loss=0.784\n",
      "Training progress in epoch #17, step 159, discriminator loss=0.692 , generator loss=0.757\n",
      "Training progress in epoch #17, step 160, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #17, step 161, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #17, step 162, discriminator loss=0.711 , generator loss=0.669\n",
      "Training progress in epoch #17, step 163, discriminator loss=0.713 , generator loss=0.644\n",
      "Training progress in epoch #17, step 164, discriminator loss=0.702 , generator loss=0.648\n",
      "Training progress in epoch #17, step 165, discriminator loss=0.701 , generator loss=0.659\n",
      "Training progress in epoch #17, step 166, discriminator loss=0.696 , generator loss=0.674\n",
      "Training progress in epoch #17, step 167, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #17, step 168, discriminator loss=0.687 , generator loss=0.740\n",
      "Training progress in epoch #17, step 169, discriminator loss=0.689 , generator loss=0.748\n",
      "Training progress in epoch #17, step 170, discriminator loss=0.686 , generator loss=0.756\n",
      "Training progress in epoch #17, step 171, discriminator loss=0.684 , generator loss=0.758\n",
      "Training progress in epoch #17, step 172, discriminator loss=0.693 , generator loss=0.764\n",
      "Training progress in epoch #17, step 173, discriminator loss=0.687 , generator loss=0.773\n",
      "Training progress in epoch #17, step 174, discriminator loss=0.685 , generator loss=0.781\n",
      "Training progress in epoch #17, step 175, discriminator loss=0.683 , generator loss=0.770\n",
      "Training progress in epoch #17, step 176, discriminator loss=0.685 , generator loss=0.735\n",
      "Training progress in epoch #17, step 177, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #17, step 178, discriminator loss=0.682 , generator loss=0.713\n",
      "Training progress in epoch #17, step 179, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #17, step 180, discriminator loss=0.691 , generator loss=0.673\n",
      "Training progress in epoch #17, step 181, discriminator loss=0.694 , generator loss=0.642\n",
      "Training progress in epoch #17, step 182, discriminator loss=0.698 , generator loss=0.653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #17, step 183, discriminator loss=0.686 , generator loss=0.659\n",
      "Training progress in epoch #17, step 184, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #17, step 185, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #17, step 186, discriminator loss=0.678 , generator loss=0.691\n",
      "Training progress in epoch #17, step 187, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #17, step 188, discriminator loss=0.691 , generator loss=0.744\n",
      "Training progress in epoch #17, step 189, discriminator loss=0.692 , generator loss=0.754\n",
      "Training progress in epoch #17, step 190, discriminator loss=0.691 , generator loss=0.767\n",
      "Training progress in epoch #17, step 191, discriminator loss=0.692 , generator loss=0.754\n",
      "Training progress in epoch #17, step 192, discriminator loss=0.684 , generator loss=0.753\n",
      "Training progress in epoch #17, step 193, discriminator loss=0.687 , generator loss=0.735\n",
      "Training progress in epoch #17, step 194, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #17, step 195, discriminator loss=0.697 , generator loss=0.676\n",
      "Training progress in epoch #17, step 196, discriminator loss=0.693 , generator loss=0.676\n",
      "Training progress in epoch #17, step 197, discriminator loss=0.695 , generator loss=0.668\n",
      "Training progress in epoch #17, step 198, discriminator loss=0.690 , generator loss=0.659\n",
      "Training progress in epoch #17, step 199, discriminator loss=0.688 , generator loss=0.668\n",
      "Training progress in epoch #17, step 200, discriminator loss=0.688 , generator loss=0.675\n",
      "Training progress in epoch #17, step 201, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #17, step 202, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #17, step 203, discriminator loss=0.682 , generator loss=0.737\n",
      "Training progress in epoch #17, step 204, discriminator loss=0.692 , generator loss=0.761\n",
      "Training progress in epoch #17, step 205, discriminator loss=0.692 , generator loss=0.753\n",
      "Training progress in epoch #17, step 206, discriminator loss=0.686 , generator loss=0.759\n",
      "Training progress in epoch #17, step 207, discriminator loss=0.687 , generator loss=0.749\n",
      "Training progress in epoch #17, step 208, discriminator loss=0.688 , generator loss=0.783\n",
      "Training progress in epoch #17, step 209, discriminator loss=0.685 , generator loss=0.781\n",
      "Training progress in epoch #17, step 210, discriminator loss=0.687 , generator loss=0.750\n",
      "Training progress in epoch #17, step 211, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #17, step 212, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #17, step 213, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #17, step 214, discriminator loss=0.697 , generator loss=0.690\n",
      "Training progress in epoch #17, step 215, discriminator loss=0.699 , generator loss=0.661\n",
      "Training progress in epoch #17, step 216, discriminator loss=0.691 , generator loss=0.654\n",
      "Training progress in epoch #17, step 217, discriminator loss=0.697 , generator loss=0.655\n",
      "Training progress in epoch #17, step 218, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #17, step 219, discriminator loss=0.683 , generator loss=0.700\n",
      "Training progress in epoch #17, step 220, discriminator loss=0.681 , generator loss=0.709\n",
      "Training progress in epoch #17, step 221, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #17, step 222, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #17, step 223, discriminator loss=0.689 , generator loss=0.746\n",
      "Training progress in epoch #17, step 224, discriminator loss=0.698 , generator loss=0.760\n",
      "Training progress in epoch #17, step 225, discriminator loss=0.694 , generator loss=0.752\n",
      "Training progress in epoch #17, step 226, discriminator loss=0.697 , generator loss=0.732\n",
      "Training progress in epoch #17, step 227, discriminator loss=0.694 , generator loss=0.752\n",
      "Training progress in epoch #17, step 228, discriminator loss=0.696 , generator loss=0.739\n",
      "Training progress in epoch #17, step 229, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #17, step 230, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #17, step 231, discriminator loss=0.683 , generator loss=0.711\n",
      "Training progress in epoch #17, step 232, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #17, step 233, discriminator loss=0.681 , generator loss=0.684\n",
      "Disciminator Accuracy on real images: 87%, on fake images: 20%\n",
      "Training progress in epoch #18, step 0, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #18, step 1, discriminator loss=0.685 , generator loss=0.672\n",
      "Training progress in epoch #18, step 2, discriminator loss=0.684 , generator loss=0.683\n",
      "Training progress in epoch #18, step 3, discriminator loss=0.675 , generator loss=0.690\n",
      "Training progress in epoch #18, step 4, discriminator loss=0.666 , generator loss=0.717\n",
      "Training progress in epoch #18, step 5, discriminator loss=0.677 , generator loss=0.702\n",
      "Training progress in epoch #18, step 6, discriminator loss=0.673 , generator loss=0.691\n",
      "Training progress in epoch #18, step 7, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #18, step 8, discriminator loss=0.685 , generator loss=0.729\n",
      "Training progress in epoch #18, step 9, discriminator loss=0.694 , generator loss=0.755\n",
      "Training progress in epoch #18, step 10, discriminator loss=0.693 , generator loss=0.776\n",
      "Training progress in epoch #18, step 11, discriminator loss=0.696 , generator loss=0.765\n",
      "Training progress in epoch #18, step 12, discriminator loss=0.694 , generator loss=0.739\n",
      "Training progress in epoch #18, step 13, discriminator loss=0.701 , generator loss=0.716\n",
      "Training progress in epoch #18, step 14, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #18, step 15, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #18, step 16, discriminator loss=0.700 , generator loss=0.707\n",
      "Training progress in epoch #18, step 17, discriminator loss=0.700 , generator loss=0.705\n",
      "Training progress in epoch #18, step 18, discriminator loss=0.689 , generator loss=0.677\n",
      "Training progress in epoch #18, step 19, discriminator loss=0.680 , generator loss=0.668\n",
      "Training progress in epoch #18, step 20, discriminator loss=0.678 , generator loss=0.701\n",
      "Training progress in epoch #18, step 21, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #18, step 22, discriminator loss=0.677 , generator loss=0.746\n",
      "Training progress in epoch #18, step 23, discriminator loss=0.683 , generator loss=0.738\n",
      "Training progress in epoch #18, step 24, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #18, step 25, discriminator loss=0.687 , generator loss=0.762\n",
      "Training progress in epoch #18, step 26, discriminator loss=0.688 , generator loss=0.765\n",
      "Training progress in epoch #18, step 27, discriminator loss=0.692 , generator loss=0.741\n",
      "Training progress in epoch #18, step 28, discriminator loss=0.690 , generator loss=0.750\n",
      "Training progress in epoch #18, step 29, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #18, step 30, discriminator loss=0.690 , generator loss=0.737\n",
      "Training progress in epoch #18, step 31, discriminator loss=0.688 , generator loss=0.725\n",
      "Training progress in epoch #18, step 32, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #18, step 33, discriminator loss=0.696 , generator loss=0.684\n",
      "Training progress in epoch #18, step 34, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #18, step 35, discriminator loss=0.696 , generator loss=0.674\n",
      "Training progress in epoch #18, step 36, discriminator loss=0.693 , generator loss=0.654\n",
      "Training progress in epoch #18, step 37, discriminator loss=0.691 , generator loss=0.673\n",
      "Training progress in epoch #18, step 38, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #18, step 39, discriminator loss=0.685 , generator loss=0.695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #18, step 40, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #18, step 41, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #18, step 42, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #18, step 43, discriminator loss=0.693 , generator loss=0.728\n",
      "Training progress in epoch #18, step 44, discriminator loss=0.690 , generator loss=0.734\n",
      "Training progress in epoch #18, step 45, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #18, step 46, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #18, step 47, discriminator loss=0.687 , generator loss=0.738\n",
      "Training progress in epoch #18, step 48, discriminator loss=0.690 , generator loss=0.742\n",
      "Training progress in epoch #18, step 49, discriminator loss=0.682 , generator loss=0.750\n",
      "Training progress in epoch #18, step 50, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #18, step 51, discriminator loss=0.689 , generator loss=0.678\n",
      "Training progress in epoch #18, step 52, discriminator loss=0.689 , generator loss=0.667\n",
      "Training progress in epoch #18, step 53, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #18, step 54, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #18, step 55, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #18, step 56, discriminator loss=0.679 , generator loss=0.698\n",
      "Training progress in epoch #18, step 57, discriminator loss=0.681 , generator loss=0.695\n",
      "Training progress in epoch #18, step 58, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #18, step 59, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #18, step 60, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #18, step 61, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #18, step 62, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #18, step 63, discriminator loss=0.689 , generator loss=0.742\n",
      "Training progress in epoch #18, step 64, discriminator loss=0.690 , generator loss=0.749\n",
      "Training progress in epoch #18, step 65, discriminator loss=0.691 , generator loss=0.734\n",
      "Training progress in epoch #18, step 66, discriminator loss=0.699 , generator loss=0.705\n",
      "Training progress in epoch #18, step 67, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #18, step 68, discriminator loss=0.693 , generator loss=0.737\n",
      "Training progress in epoch #18, step 69, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #18, step 70, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #18, step 71, discriminator loss=0.695 , generator loss=0.719\n",
      "Training progress in epoch #18, step 72, discriminator loss=0.698 , generator loss=0.731\n",
      "Training progress in epoch #18, step 73, discriminator loss=0.682 , generator loss=0.720\n",
      "Training progress in epoch #18, step 74, discriminator loss=0.683 , generator loss=0.674\n",
      "Training progress in epoch #18, step 75, discriminator loss=0.683 , generator loss=0.671\n",
      "Training progress in epoch #18, step 76, discriminator loss=0.682 , generator loss=0.709\n",
      "Training progress in epoch #18, step 77, discriminator loss=0.688 , generator loss=0.743\n",
      "Training progress in epoch #18, step 78, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #18, step 79, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #18, step 80, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #18, step 81, discriminator loss=0.682 , generator loss=0.705\n",
      "Training progress in epoch #18, step 82, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #18, step 83, discriminator loss=0.689 , generator loss=0.734\n",
      "Training progress in epoch #18, step 84, discriminator loss=0.696 , generator loss=0.735\n",
      "Training progress in epoch #18, step 85, discriminator loss=0.698 , generator loss=0.739\n",
      "Training progress in epoch #18, step 86, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #18, step 87, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #18, step 88, discriminator loss=0.679 , generator loss=0.710\n",
      "Training progress in epoch #18, step 89, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #18, step 90, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #18, step 91, discriminator loss=0.680 , generator loss=0.699\n",
      "Training progress in epoch #18, step 92, discriminator loss=0.685 , generator loss=0.679\n",
      "Training progress in epoch #18, step 93, discriminator loss=0.679 , generator loss=0.692\n",
      "Training progress in epoch #18, step 94, discriminator loss=0.683 , generator loss=0.711\n",
      "Training progress in epoch #18, step 95, discriminator loss=0.681 , generator loss=0.718\n",
      "Training progress in epoch #18, step 96, discriminator loss=0.682 , generator loss=0.737\n",
      "Training progress in epoch #18, step 97, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #18, step 98, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #18, step 99, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #18, step 100, discriminator loss=0.705 , generator loss=0.738\n",
      "Training progress in epoch #18, step 101, discriminator loss=0.696 , generator loss=0.749\n",
      "Training progress in epoch #18, step 102, discriminator loss=0.696 , generator loss=0.713\n",
      "Training progress in epoch #18, step 103, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #18, step 104, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #18, step 105, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #18, step 106, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #18, step 107, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #18, step 108, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #18, step 109, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #18, step 110, discriminator loss=0.682 , generator loss=0.709\n",
      "Training progress in epoch #18, step 111, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #18, step 112, discriminator loss=0.685 , generator loss=0.736\n",
      "Training progress in epoch #18, step 113, discriminator loss=0.683 , generator loss=0.739\n",
      "Training progress in epoch #18, step 114, discriminator loss=0.679 , generator loss=0.743\n",
      "Training progress in epoch #18, step 115, discriminator loss=0.693 , generator loss=0.753\n",
      "Training progress in epoch #18, step 116, discriminator loss=0.692 , generator loss=0.740\n",
      "Training progress in epoch #18, step 117, discriminator loss=0.682 , generator loss=0.725\n",
      "Training progress in epoch #18, step 118, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #18, step 119, discriminator loss=0.695 , generator loss=0.671\n",
      "Training progress in epoch #18, step 120, discriminator loss=0.697 , generator loss=0.672\n",
      "Training progress in epoch #18, step 121, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #18, step 122, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #18, step 123, discriminator loss=0.693 , generator loss=0.674\n",
      "Training progress in epoch #18, step 124, discriminator loss=0.689 , generator loss=0.668\n",
      "Training progress in epoch #18, step 125, discriminator loss=0.690 , generator loss=0.675\n",
      "Training progress in epoch #18, step 126, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #18, step 127, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #18, step 128, discriminator loss=0.689 , generator loss=0.754\n",
      "Training progress in epoch #18, step 129, discriminator loss=0.682 , generator loss=0.726\n",
      "Training progress in epoch #18, step 130, discriminator loss=0.682 , generator loss=0.718\n",
      "Training progress in epoch #18, step 131, discriminator loss=0.685 , generator loss=0.719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #18, step 132, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #18, step 133, discriminator loss=0.684 , generator loss=0.744\n",
      "Training progress in epoch #18, step 134, discriminator loss=0.677 , generator loss=0.754\n",
      "Training progress in epoch #18, step 135, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #18, step 136, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #18, step 137, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #18, step 138, discriminator loss=0.690 , generator loss=0.674\n",
      "Training progress in epoch #18, step 139, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #18, step 140, discriminator loss=0.687 , generator loss=0.676\n",
      "Training progress in epoch #18, step 141, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #18, step 142, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #18, step 143, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #18, step 144, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #18, step 145, discriminator loss=0.683 , generator loss=0.717\n",
      "Training progress in epoch #18, step 146, discriminator loss=0.683 , generator loss=0.740\n",
      "Training progress in epoch #18, step 147, discriminator loss=0.692 , generator loss=0.741\n",
      "Training progress in epoch #18, step 148, discriminator loss=0.682 , generator loss=0.743\n",
      "Training progress in epoch #18, step 149, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #18, step 150, discriminator loss=0.680 , generator loss=0.709\n",
      "Training progress in epoch #18, step 151, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #18, step 152, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #18, step 153, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #18, step 154, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #18, step 155, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #18, step 156, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #18, step 157, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #18, step 158, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #18, step 159, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #18, step 160, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #18, step 161, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #18, step 162, discriminator loss=0.684 , generator loss=0.725\n",
      "Training progress in epoch #18, step 163, discriminator loss=0.690 , generator loss=0.738\n",
      "Training progress in epoch #18, step 164, discriminator loss=0.687 , generator loss=0.738\n",
      "Training progress in epoch #18, step 165, discriminator loss=0.683 , generator loss=0.732\n",
      "Training progress in epoch #18, step 166, discriminator loss=0.687 , generator loss=0.749\n",
      "Training progress in epoch #18, step 167, discriminator loss=0.680 , generator loss=0.739\n",
      "Training progress in epoch #18, step 168, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #18, step 169, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #18, step 170, discriminator loss=0.688 , generator loss=0.680\n",
      "Training progress in epoch #18, step 171, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #18, step 172, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #18, step 173, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #18, step 174, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #18, step 175, discriminator loss=0.696 , generator loss=0.679\n",
      "Training progress in epoch #18, step 176, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #18, step 177, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #18, step 178, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #18, step 179, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #18, step 180, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #18, step 181, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #18, step 182, discriminator loss=0.684 , generator loss=0.753\n",
      "Training progress in epoch #18, step 183, discriminator loss=0.679 , generator loss=0.758\n",
      "Training progress in epoch #18, step 184, discriminator loss=0.680 , generator loss=0.729\n",
      "Training progress in epoch #18, step 185, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #18, step 186, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #18, step 187, discriminator loss=0.680 , generator loss=0.680\n",
      "Training progress in epoch #18, step 188, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #18, step 189, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #18, step 190, discriminator loss=0.681 , generator loss=0.681\n",
      "Training progress in epoch #18, step 191, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #18, step 192, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #18, step 193, discriminator loss=0.685 , generator loss=0.730\n",
      "Training progress in epoch #18, step 194, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #18, step 195, discriminator loss=0.699 , generator loss=0.695\n",
      "Training progress in epoch #18, step 196, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #18, step 197, discriminator loss=0.697 , generator loss=0.733\n",
      "Training progress in epoch #18, step 198, discriminator loss=0.696 , generator loss=0.751\n",
      "Training progress in epoch #18, step 199, discriminator loss=0.683 , generator loss=0.739\n",
      "Training progress in epoch #18, step 200, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #18, step 201, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #18, step 202, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #18, step 203, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #18, step 204, discriminator loss=0.685 , generator loss=0.730\n",
      "Training progress in epoch #18, step 205, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #18, step 206, discriminator loss=0.681 , generator loss=0.698\n",
      "Training progress in epoch #18, step 207, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #18, step 208, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #18, step 209, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #18, step 210, discriminator loss=0.686 , generator loss=0.736\n",
      "Training progress in epoch #18, step 211, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #18, step 212, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #18, step 213, discriminator loss=0.692 , generator loss=0.729\n",
      "Training progress in epoch #18, step 214, discriminator loss=0.684 , generator loss=0.755\n",
      "Training progress in epoch #18, step 215, discriminator loss=0.683 , generator loss=0.738\n",
      "Training progress in epoch #18, step 216, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #18, step 217, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #18, step 218, discriminator loss=0.685 , generator loss=0.711\n",
      "Training progress in epoch #18, step 219, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #18, step 220, discriminator loss=0.681 , generator loss=0.709\n",
      "Training progress in epoch #18, step 221, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #18, step 222, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #18, step 223, discriminator loss=0.691 , generator loss=0.703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #18, step 224, discriminator loss=0.679 , generator loss=0.691\n",
      "Training progress in epoch #18, step 225, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #18, step 226, discriminator loss=0.684 , generator loss=0.707\n",
      "Training progress in epoch #18, step 227, discriminator loss=0.682 , generator loss=0.701\n",
      "Training progress in epoch #18, step 228, discriminator loss=0.681 , generator loss=0.683\n",
      "Training progress in epoch #18, step 229, discriminator loss=0.687 , generator loss=0.682\n",
      "Training progress in epoch #18, step 230, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #18, step 231, discriminator loss=0.682 , generator loss=0.710\n",
      "Training progress in epoch #18, step 232, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #18, step 233, discriminator loss=0.687 , generator loss=0.700\n",
      "Disciminator Accuracy on real images: 70%, on fake images: 57%\n",
      "Training progress in epoch #19, step 0, discriminator loss=0.694 , generator loss=0.736\n",
      "Training progress in epoch #19, step 1, discriminator loss=0.686 , generator loss=0.745\n",
      "Training progress in epoch #19, step 2, discriminator loss=0.700 , generator loss=0.751\n",
      "Training progress in epoch #19, step 3, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #19, step 4, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #19, step 5, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #19, step 6, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #19, step 7, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #19, step 8, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #19, step 9, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #19, step 10, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #19, step 11, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #19, step 12, discriminator loss=0.693 , generator loss=0.741\n",
      "Training progress in epoch #19, step 13, discriminator loss=0.686 , generator loss=0.731\n",
      "Training progress in epoch #19, step 14, discriminator loss=0.680 , generator loss=0.714\n",
      "Training progress in epoch #19, step 15, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #19, step 16, discriminator loss=0.686 , generator loss=0.724\n",
      "Training progress in epoch #19, step 17, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #19, step 18, discriminator loss=0.679 , generator loss=0.719\n",
      "Training progress in epoch #19, step 19, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #19, step 20, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #19, step 21, discriminator loss=0.689 , generator loss=0.732\n",
      "Training progress in epoch #19, step 22, discriminator loss=0.685 , generator loss=0.726\n",
      "Training progress in epoch #19, step 23, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #19, step 24, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #19, step 25, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #19, step 26, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #19, step 27, discriminator loss=0.685 , generator loss=0.710\n",
      "Training progress in epoch #19, step 28, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #19, step 29, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #19, step 30, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #19, step 31, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #19, step 32, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #19, step 33, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #19, step 34, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #19, step 35, discriminator loss=0.687 , generator loss=0.720\n",
      "Training progress in epoch #19, step 36, discriminator loss=0.684 , generator loss=0.721\n",
      "Training progress in epoch #19, step 37, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #19, step 38, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #19, step 39, discriminator loss=0.683 , generator loss=0.723\n",
      "Training progress in epoch #19, step 40, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #19, step 41, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #19, step 42, discriminator loss=0.687 , generator loss=0.727\n",
      "Training progress in epoch #19, step 43, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #19, step 44, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #19, step 45, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #19, step 46, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #19, step 47, discriminator loss=0.684 , generator loss=0.694\n",
      "Training progress in epoch #19, step 48, discriminator loss=0.682 , generator loss=0.685\n",
      "Training progress in epoch #19, step 49, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #19, step 50, discriminator loss=0.694 , generator loss=0.724\n",
      "Training progress in epoch #19, step 51, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #19, step 52, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #19, step 53, discriminator loss=0.686 , generator loss=0.733\n",
      "Training progress in epoch #19, step 54, discriminator loss=0.693 , generator loss=0.736\n",
      "Training progress in epoch #19, step 55, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #19, step 56, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #19, step 57, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #19, step 58, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #19, step 59, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #19, step 60, discriminator loss=0.685 , generator loss=0.678\n",
      "Training progress in epoch #19, step 61, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #19, step 62, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #19, step 63, discriminator loss=0.682 , generator loss=0.704\n",
      "Training progress in epoch #19, step 64, discriminator loss=0.691 , generator loss=0.737\n",
      "Training progress in epoch #19, step 65, discriminator loss=0.692 , generator loss=0.732\n",
      "Training progress in epoch #19, step 66, discriminator loss=0.688 , generator loss=0.743\n",
      "Training progress in epoch #19, step 67, discriminator loss=0.694 , generator loss=0.752\n",
      "Training progress in epoch #19, step 68, discriminator loss=0.690 , generator loss=0.739\n",
      "Training progress in epoch #19, step 69, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #19, step 70, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #19, step 71, discriminator loss=0.694 , generator loss=0.674\n",
      "Training progress in epoch #19, step 72, discriminator loss=0.701 , generator loss=0.685\n",
      "Training progress in epoch #19, step 73, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #19, step 74, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #19, step 75, discriminator loss=0.683 , generator loss=0.688\n",
      "Training progress in epoch #19, step 76, discriminator loss=0.691 , generator loss=0.672\n",
      "Training progress in epoch #19, step 77, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #19, step 78, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #19, step 79, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #19, step 80, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #19, step 81, discriminator loss=0.689 , generator loss=0.723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #19, step 82, discriminator loss=0.686 , generator loss=0.729\n",
      "Training progress in epoch #19, step 83, discriminator loss=0.690 , generator loss=0.736\n",
      "Training progress in epoch #19, step 84, discriminator loss=0.683 , generator loss=0.749\n",
      "Training progress in epoch #19, step 85, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #19, step 86, discriminator loss=0.682 , generator loss=0.707\n",
      "Training progress in epoch #19, step 87, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #19, step 88, discriminator loss=0.694 , generator loss=0.673\n",
      "Training progress in epoch #19, step 89, discriminator loss=0.686 , generator loss=0.662\n",
      "Training progress in epoch #19, step 90, discriminator loss=0.688 , generator loss=0.680\n",
      "Training progress in epoch #19, step 91, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #19, step 92, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #19, step 93, discriminator loss=0.684 , generator loss=0.703\n",
      "Training progress in epoch #19, step 94, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #19, step 95, discriminator loss=0.687 , generator loss=0.727\n",
      "Training progress in epoch #19, step 96, discriminator loss=0.690 , generator loss=0.737\n",
      "Training progress in epoch #19, step 97, discriminator loss=0.691 , generator loss=0.747\n",
      "Training progress in epoch #19, step 98, discriminator loss=0.690 , generator loss=0.736\n",
      "Training progress in epoch #19, step 99, discriminator loss=0.675 , generator loss=0.763\n",
      "Training progress in epoch #19, step 100, discriminator loss=0.686 , generator loss=0.762\n",
      "Training progress in epoch #19, step 101, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #19, step 102, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #19, step 103, discriminator loss=0.697 , generator loss=0.693\n",
      "Training progress in epoch #19, step 104, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #19, step 105, discriminator loss=0.689 , generator loss=0.652\n",
      "Training progress in epoch #19, step 106, discriminator loss=0.689 , generator loss=0.660\n",
      "Training progress in epoch #19, step 107, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #19, step 108, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #19, step 109, discriminator loss=0.686 , generator loss=0.681\n",
      "Training progress in epoch #19, step 110, discriminator loss=0.679 , generator loss=0.684\n",
      "Training progress in epoch #19, step 111, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #19, step 112, discriminator loss=0.692 , generator loss=0.790\n",
      "Training progress in epoch #19, step 113, discriminator loss=0.689 , generator loss=0.754\n",
      "Training progress in epoch #19, step 114, discriminator loss=0.695 , generator loss=0.735\n",
      "Training progress in epoch #19, step 115, discriminator loss=0.682 , generator loss=0.731\n",
      "Training progress in epoch #19, step 116, discriminator loss=0.685 , generator loss=0.743\n",
      "Training progress in epoch #19, step 117, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #19, step 118, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #19, step 119, discriminator loss=0.682 , generator loss=0.687\n",
      "Training progress in epoch #19, step 120, discriminator loss=0.689 , generator loss=0.671\n",
      "Training progress in epoch #19, step 121, discriminator loss=0.686 , generator loss=0.666\n",
      "Training progress in epoch #19, step 122, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #19, step 123, discriminator loss=0.688 , generator loss=0.673\n",
      "Training progress in epoch #19, step 124, discriminator loss=0.693 , generator loss=0.667\n",
      "Training progress in epoch #19, step 125, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #19, step 126, discriminator loss=0.698 , generator loss=0.723\n",
      "Training progress in epoch #19, step 127, discriminator loss=0.693 , generator loss=0.743\n",
      "Training progress in epoch #19, step 128, discriminator loss=0.693 , generator loss=0.736\n",
      "Training progress in epoch #19, step 129, discriminator loss=0.692 , generator loss=0.731\n",
      "Training progress in epoch #19, step 130, discriminator loss=0.690 , generator loss=0.745\n",
      "Training progress in epoch #19, step 131, discriminator loss=0.689 , generator loss=0.747\n",
      "Training progress in epoch #19, step 132, discriminator loss=0.680 , generator loss=0.742\n",
      "Training progress in epoch #19, step 133, discriminator loss=0.680 , generator loss=0.711\n",
      "Training progress in epoch #19, step 134, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #19, step 135, discriminator loss=0.694 , generator loss=0.668\n",
      "Training progress in epoch #19, step 136, discriminator loss=0.683 , generator loss=0.665\n",
      "Training progress in epoch #19, step 137, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #19, step 138, discriminator loss=0.683 , generator loss=0.684\n",
      "Training progress in epoch #19, step 139, discriminator loss=0.685 , generator loss=0.681\n",
      "Training progress in epoch #19, step 140, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #19, step 141, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #19, step 142, discriminator loss=0.691 , generator loss=0.738\n",
      "Training progress in epoch #19, step 143, discriminator loss=0.687 , generator loss=0.761\n",
      "Training progress in epoch #19, step 144, discriminator loss=0.687 , generator loss=0.747\n",
      "Training progress in epoch #19, step 145, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #19, step 146, discriminator loss=0.687 , generator loss=0.753\n",
      "Training progress in epoch #19, step 147, discriminator loss=0.687 , generator loss=0.759\n",
      "Training progress in epoch #19, step 148, discriminator loss=0.695 , generator loss=0.727\n",
      "Training progress in epoch #19, step 149, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #19, step 150, discriminator loss=0.690 , generator loss=0.674\n",
      "Training progress in epoch #19, step 151, discriminator loss=0.690 , generator loss=0.667\n",
      "Training progress in epoch #19, step 152, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #19, step 153, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #19, step 154, discriminator loss=0.678 , generator loss=0.674\n",
      "Training progress in epoch #19, step 155, discriminator loss=0.677 , generator loss=0.682\n",
      "Training progress in epoch #19, step 156, discriminator loss=0.676 , generator loss=0.699\n",
      "Training progress in epoch #19, step 157, discriminator loss=0.680 , generator loss=0.716\n",
      "Training progress in epoch #19, step 158, discriminator loss=0.689 , generator loss=0.769\n",
      "Training progress in epoch #19, step 159, discriminator loss=0.688 , generator loss=0.781\n",
      "Training progress in epoch #19, step 160, discriminator loss=0.698 , generator loss=0.762\n",
      "Training progress in epoch #19, step 161, discriminator loss=0.688 , generator loss=0.751\n",
      "Training progress in epoch #19, step 162, discriminator loss=0.693 , generator loss=0.763\n",
      "Training progress in epoch #19, step 163, discriminator loss=0.700 , generator loss=0.722\n",
      "Training progress in epoch #19, step 164, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #19, step 165, discriminator loss=0.701 , generator loss=0.694\n",
      "Training progress in epoch #19, step 166, discriminator loss=0.697 , generator loss=0.657\n",
      "Training progress in epoch #19, step 167, discriminator loss=0.695 , generator loss=0.638\n",
      "Training progress in epoch #19, step 168, discriminator loss=0.692 , generator loss=0.634\n",
      "Training progress in epoch #19, step 169, discriminator loss=0.690 , generator loss=0.660\n",
      "Training progress in epoch #19, step 170, discriminator loss=0.681 , generator loss=0.698\n",
      "Training progress in epoch #19, step 171, discriminator loss=0.684 , generator loss=0.723\n",
      "Training progress in epoch #19, step 172, discriminator loss=0.682 , generator loss=0.734\n",
      "Training progress in epoch #19, step 173, discriminator loss=0.688 , generator loss=0.724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #19, step 174, discriminator loss=0.686 , generator loss=0.740\n",
      "Training progress in epoch #19, step 175, discriminator loss=0.692 , generator loss=0.766\n",
      "Training progress in epoch #19, step 176, discriminator loss=0.686 , generator loss=0.768\n",
      "Training progress in epoch #19, step 177, discriminator loss=0.692 , generator loss=0.763\n",
      "Training progress in epoch #19, step 178, discriminator loss=0.687 , generator loss=0.751\n",
      "Training progress in epoch #19, step 179, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #19, step 180, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #19, step 181, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #19, step 182, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #19, step 183, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #19, step 184, discriminator loss=0.694 , generator loss=0.665\n",
      "Training progress in epoch #19, step 185, discriminator loss=0.688 , generator loss=0.644\n",
      "Training progress in epoch #19, step 186, discriminator loss=0.684 , generator loss=0.659\n",
      "Training progress in epoch #19, step 187, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #19, step 188, discriminator loss=0.681 , generator loss=0.732\n",
      "Training progress in epoch #19, step 189, discriminator loss=0.687 , generator loss=0.745\n",
      "Training progress in epoch #19, step 190, discriminator loss=0.689 , generator loss=0.757\n",
      "Training progress in epoch #19, step 191, discriminator loss=0.697 , generator loss=0.750\n",
      "Training progress in epoch #19, step 192, discriminator loss=0.688 , generator loss=0.757\n",
      "Training progress in epoch #19, step 193, discriminator loss=0.693 , generator loss=0.793\n",
      "Training progress in epoch #19, step 194, discriminator loss=0.697 , generator loss=0.759\n",
      "Training progress in epoch #19, step 195, discriminator loss=0.685 , generator loss=0.727\n",
      "Training progress in epoch #19, step 196, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #19, step 197, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #19, step 198, discriminator loss=0.696 , generator loss=0.676\n",
      "Training progress in epoch #19, step 199, discriminator loss=0.691 , generator loss=0.668\n",
      "Training progress in epoch #19, step 200, discriminator loss=0.694 , generator loss=0.663\n",
      "Training progress in epoch #19, step 201, discriminator loss=0.682 , generator loss=0.677\n",
      "Training progress in epoch #19, step 202, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #19, step 203, discriminator loss=0.682 , generator loss=0.708\n",
      "Training progress in epoch #19, step 204, discriminator loss=0.676 , generator loss=0.726\n",
      "Training progress in epoch #19, step 205, discriminator loss=0.683 , generator loss=0.744\n",
      "Training progress in epoch #19, step 206, discriminator loss=0.690 , generator loss=0.749\n",
      "Training progress in epoch #19, step 207, discriminator loss=0.687 , generator loss=0.746\n",
      "Training progress in epoch #19, step 208, discriminator loss=0.691 , generator loss=0.743\n",
      "Training progress in epoch #19, step 209, discriminator loss=0.693 , generator loss=0.754\n",
      "Training progress in epoch #19, step 210, discriminator loss=0.691 , generator loss=0.750\n",
      "Training progress in epoch #19, step 211, discriminator loss=0.694 , generator loss=0.721\n",
      "Training progress in epoch #19, step 212, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #19, step 213, discriminator loss=0.693 , generator loss=0.726\n",
      "Training progress in epoch #19, step 214, discriminator loss=0.700 , generator loss=0.670\n",
      "Training progress in epoch #19, step 215, discriminator loss=0.694 , generator loss=0.643\n",
      "Training progress in epoch #19, step 216, discriminator loss=0.698 , generator loss=0.642\n",
      "Training progress in epoch #19, step 217, discriminator loss=0.691 , generator loss=0.649\n",
      "Training progress in epoch #19, step 218, discriminator loss=0.685 , generator loss=0.666\n",
      "Training progress in epoch #19, step 219, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #19, step 220, discriminator loss=0.675 , generator loss=0.717\n",
      "Training progress in epoch #19, step 221, discriminator loss=0.686 , generator loss=0.741\n",
      "Training progress in epoch #19, step 222, discriminator loss=0.686 , generator loss=0.753\n",
      "Training progress in epoch #19, step 223, discriminator loss=0.681 , generator loss=0.775\n",
      "Training progress in epoch #19, step 224, discriminator loss=0.689 , generator loss=0.784\n",
      "Training progress in epoch #19, step 225, discriminator loss=0.681 , generator loss=0.779\n",
      "Training progress in epoch #19, step 226, discriminator loss=0.684 , generator loss=0.779\n",
      "Training progress in epoch #19, step 227, discriminator loss=0.681 , generator loss=0.731\n",
      "Training progress in epoch #19, step 228, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #19, step 229, discriminator loss=0.689 , generator loss=0.670\n",
      "Training progress in epoch #19, step 230, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #19, step 231, discriminator loss=0.694 , generator loss=0.678\n",
      "Training progress in epoch #19, step 232, discriminator loss=0.696 , generator loss=0.667\n",
      "Training progress in epoch #19, step 233, discriminator loss=0.698 , generator loss=0.642\n",
      "Disciminator Accuracy on real images: 99%, on fake images: 3%\n",
      "Training progress in epoch #20, step 0, discriminator loss=0.691 , generator loss=0.663\n",
      "Training progress in epoch #20, step 1, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #20, step 2, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #20, step 3, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #20, step 4, discriminator loss=0.693 , generator loss=0.738\n",
      "Training progress in epoch #20, step 5, discriminator loss=0.693 , generator loss=0.744\n",
      "Training progress in epoch #20, step 6, discriminator loss=0.698 , generator loss=0.784\n",
      "Training progress in epoch #20, step 7, discriminator loss=0.689 , generator loss=0.764\n",
      "Training progress in epoch #20, step 8, discriminator loss=0.688 , generator loss=0.760\n",
      "Training progress in epoch #20, step 9, discriminator loss=0.691 , generator loss=0.733\n",
      "Training progress in epoch #20, step 10, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #20, step 11, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #20, step 12, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #20, step 13, discriminator loss=0.697 , generator loss=0.686\n",
      "Training progress in epoch #20, step 14, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #20, step 15, discriminator loss=0.691 , generator loss=0.667\n",
      "Training progress in epoch #20, step 16, discriminator loss=0.698 , generator loss=0.688\n",
      "Training progress in epoch #20, step 17, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #20, step 18, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #20, step 19, discriminator loss=0.700 , generator loss=0.711\n",
      "Training progress in epoch #20, step 20, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #20, step 21, discriminator loss=0.685 , generator loss=0.738\n",
      "Training progress in epoch #20, step 22, discriminator loss=0.691 , generator loss=0.761\n",
      "Training progress in epoch #20, step 23, discriminator loss=0.685 , generator loss=0.748\n",
      "Training progress in epoch #20, step 24, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #20, step 25, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #20, step 26, discriminator loss=0.683 , generator loss=0.731\n",
      "Training progress in epoch #20, step 27, discriminator loss=0.688 , generator loss=0.743\n",
      "Training progress in epoch #20, step 28, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #20, step 29, discriminator loss=0.694 , generator loss=0.671\n",
      "Training progress in epoch #20, step 30, discriminator loss=0.692 , generator loss=0.659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #20, step 31, discriminator loss=0.695 , generator loss=0.674\n",
      "Training progress in epoch #20, step 32, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #20, step 33, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #20, step 34, discriminator loss=0.694 , generator loss=0.677\n",
      "Training progress in epoch #20, step 35, discriminator loss=0.685 , generator loss=0.687\n",
      "Training progress in epoch #20, step 36, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #20, step 37, discriminator loss=0.685 , generator loss=0.726\n",
      "Training progress in epoch #20, step 38, discriminator loss=0.692 , generator loss=0.742\n",
      "Training progress in epoch #20, step 39, discriminator loss=0.680 , generator loss=0.745\n",
      "Training progress in epoch #20, step 40, discriminator loss=0.693 , generator loss=0.750\n",
      "Training progress in epoch #20, step 41, discriminator loss=0.685 , generator loss=0.726\n",
      "Training progress in epoch #20, step 42, discriminator loss=0.680 , generator loss=0.721\n",
      "Training progress in epoch #20, step 43, discriminator loss=0.685 , generator loss=0.714\n",
      "Training progress in epoch #20, step 44, discriminator loss=0.686 , generator loss=0.727\n",
      "Training progress in epoch #20, step 45, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #20, step 46, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #20, step 47, discriminator loss=0.682 , generator loss=0.701\n",
      "Training progress in epoch #20, step 48, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #20, step 49, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #20, step 50, discriminator loss=0.690 , generator loss=0.670\n",
      "Training progress in epoch #20, step 51, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #20, step 52, discriminator loss=0.697 , generator loss=0.709\n",
      "Training progress in epoch #20, step 53, discriminator loss=0.698 , generator loss=0.731\n",
      "Training progress in epoch #20, step 54, discriminator loss=0.691 , generator loss=0.723\n",
      "Training progress in epoch #20, step 55, discriminator loss=0.696 , generator loss=0.744\n",
      "Training progress in epoch #20, step 56, discriminator loss=0.695 , generator loss=0.751\n",
      "Training progress in epoch #20, step 57, discriminator loss=0.683 , generator loss=0.752\n",
      "Training progress in epoch #20, step 58, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #20, step 59, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #20, step 60, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #20, step 61, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #20, step 62, discriminator loss=0.680 , generator loss=0.701\n",
      "Training progress in epoch #20, step 63, discriminator loss=0.682 , generator loss=0.723\n",
      "Training progress in epoch #20, step 64, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #20, step 65, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #20, step 66, discriminator loss=0.678 , generator loss=0.705\n",
      "Training progress in epoch #20, step 67, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #20, step 68, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #20, step 69, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #20, step 70, discriminator loss=0.692 , generator loss=0.733\n",
      "Training progress in epoch #20, step 71, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #20, step 72, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #20, step 73, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #20, step 74, discriminator loss=0.688 , generator loss=0.731\n",
      "Training progress in epoch #20, step 75, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #20, step 76, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #20, step 77, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #20, step 78, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #20, step 79, discriminator loss=0.684 , generator loss=0.691\n",
      "Training progress in epoch #20, step 80, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #20, step 81, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #20, step 82, discriminator loss=0.684 , generator loss=0.712\n",
      "Training progress in epoch #20, step 83, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #20, step 84, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #20, step 85, discriminator loss=0.680 , generator loss=0.720\n",
      "Training progress in epoch #20, step 86, discriminator loss=0.698 , generator loss=0.728\n",
      "Training progress in epoch #20, step 87, discriminator loss=0.683 , generator loss=0.735\n",
      "Training progress in epoch #20, step 88, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #20, step 89, discriminator loss=0.684 , generator loss=0.724\n",
      "Training progress in epoch #20, step 90, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #20, step 91, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #20, step 92, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #20, step 93, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #20, step 94, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #20, step 95, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #20, step 96, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #20, step 97, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #20, step 98, discriminator loss=0.680 , generator loss=0.710\n",
      "Training progress in epoch #20, step 99, discriminator loss=0.684 , generator loss=0.692\n",
      "Training progress in epoch #20, step 100, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #20, step 101, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #20, step 102, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #20, step 103, discriminator loss=0.692 , generator loss=0.747\n",
      "Training progress in epoch #20, step 104, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #20, step 105, discriminator loss=0.687 , generator loss=0.727\n",
      "Training progress in epoch #20, step 106, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #20, step 107, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #20, step 108, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #20, step 109, discriminator loss=0.680 , generator loss=0.706\n",
      "Training progress in epoch #20, step 110, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #20, step 111, discriminator loss=0.682 , generator loss=0.730\n",
      "Training progress in epoch #20, step 112, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #20, step 113, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #20, step 114, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #20, step 115, discriminator loss=0.695 , generator loss=0.682\n",
      "Training progress in epoch #20, step 116, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #20, step 117, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #20, step 118, discriminator loss=0.697 , generator loss=0.700\n",
      "Training progress in epoch #20, step 119, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #20, step 120, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #20, step 121, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #20, step 122, discriminator loss=0.688 , generator loss=0.728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #20, step 123, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #20, step 124, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #20, step 125, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #20, step 126, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #20, step 127, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #20, step 128, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #20, step 129, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #20, step 130, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #20, step 131, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #20, step 132, discriminator loss=0.680 , generator loss=0.705\n",
      "Training progress in epoch #20, step 133, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #20, step 134, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #20, step 135, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #20, step 136, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #20, step 137, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #20, step 138, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #20, step 139, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #20, step 140, discriminator loss=0.691 , generator loss=0.739\n",
      "Training progress in epoch #20, step 141, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #20, step 142, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #20, step 143, discriminator loss=0.684 , generator loss=0.702\n",
      "Training progress in epoch #20, step 144, discriminator loss=0.685 , generator loss=0.685\n",
      "Training progress in epoch #20, step 145, discriminator loss=0.690 , generator loss=0.670\n",
      "Training progress in epoch #20, step 146, discriminator loss=0.695 , generator loss=0.680\n",
      "Training progress in epoch #20, step 147, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #20, step 148, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #20, step 149, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #20, step 150, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #20, step 151, discriminator loss=0.682 , generator loss=0.731\n",
      "Training progress in epoch #20, step 152, discriminator loss=0.688 , generator loss=0.737\n",
      "Training progress in epoch #20, step 153, discriminator loss=0.679 , generator loss=0.744\n",
      "Training progress in epoch #20, step 154, discriminator loss=0.688 , generator loss=0.724\n",
      "Training progress in epoch #20, step 155, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #20, step 156, discriminator loss=0.693 , generator loss=0.726\n",
      "Training progress in epoch #20, step 157, discriminator loss=0.684 , generator loss=0.700\n",
      "Training progress in epoch #20, step 158, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #20, step 159, discriminator loss=0.690 , generator loss=0.678\n",
      "Training progress in epoch #20, step 160, discriminator loss=0.690 , generator loss=0.669\n",
      "Training progress in epoch #20, step 161, discriminator loss=0.688 , generator loss=0.673\n",
      "Training progress in epoch #20, step 162, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #20, step 163, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #20, step 164, discriminator loss=0.687 , generator loss=0.749\n",
      "Training progress in epoch #20, step 165, discriminator loss=0.692 , generator loss=0.754\n",
      "Training progress in epoch #20, step 166, discriminator loss=0.689 , generator loss=0.758\n",
      "Training progress in epoch #20, step 167, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #20, step 168, discriminator loss=0.684 , generator loss=0.735\n",
      "Training progress in epoch #20, step 169, discriminator loss=0.689 , generator loss=0.754\n",
      "Training progress in epoch #20, step 170, discriminator loss=0.683 , generator loss=0.749\n",
      "Training progress in epoch #20, step 171, discriminator loss=0.685 , generator loss=0.733\n",
      "Training progress in epoch #20, step 172, discriminator loss=0.679 , generator loss=0.691\n",
      "Training progress in epoch #20, step 173, discriminator loss=0.679 , generator loss=0.690\n",
      "Training progress in epoch #20, step 174, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #20, step 175, discriminator loss=0.685 , generator loss=0.684\n",
      "Training progress in epoch #20, step 176, discriminator loss=0.682 , generator loss=0.688\n",
      "Training progress in epoch #20, step 177, discriminator loss=0.684 , generator loss=0.694\n",
      "Training progress in epoch #20, step 178, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #20, step 179, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #20, step 180, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #20, step 181, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #20, step 182, discriminator loss=0.697 , generator loss=0.702\n",
      "Training progress in epoch #20, step 183, discriminator loss=0.695 , generator loss=0.734\n",
      "Training progress in epoch #20, step 184, discriminator loss=0.705 , generator loss=0.767\n",
      "Training progress in epoch #20, step 185, discriminator loss=0.694 , generator loss=0.759\n",
      "Training progress in epoch #20, step 186, discriminator loss=0.684 , generator loss=0.731\n",
      "Training progress in epoch #20, step 187, discriminator loss=0.693 , generator loss=0.726\n",
      "Training progress in epoch #20, step 188, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #20, step 189, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #20, step 190, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #20, step 191, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #20, step 192, discriminator loss=0.681 , generator loss=0.675\n",
      "Training progress in epoch #20, step 193, discriminator loss=0.683 , generator loss=0.683\n",
      "Training progress in epoch #20, step 194, discriminator loss=0.667 , generator loss=0.681\n",
      "Training progress in epoch #20, step 195, discriminator loss=0.684 , generator loss=0.683\n",
      "Training progress in epoch #20, step 196, discriminator loss=0.684 , generator loss=0.692\n",
      "Training progress in epoch #20, step 197, discriminator loss=0.680 , generator loss=0.722\n",
      "Training progress in epoch #20, step 198, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #20, step 199, discriminator loss=0.687 , generator loss=0.720\n",
      "Training progress in epoch #20, step 200, discriminator loss=0.698 , generator loss=0.718\n",
      "Training progress in epoch #20, step 201, discriminator loss=0.698 , generator loss=0.727\n",
      "Training progress in epoch #20, step 202, discriminator loss=0.695 , generator loss=0.754\n",
      "Training progress in epoch #20, step 203, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #20, step 204, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #20, step 205, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #20, step 206, discriminator loss=0.701 , generator loss=0.688\n",
      "Training progress in epoch #20, step 207, discriminator loss=0.699 , generator loss=0.681\n",
      "Training progress in epoch #20, step 208, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #20, step 209, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #20, step 210, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #20, step 211, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #20, step 212, discriminator loss=0.684 , generator loss=0.743\n",
      "Training progress in epoch #20, step 213, discriminator loss=0.688 , generator loss=0.745\n",
      "Training progress in epoch #20, step 214, discriminator loss=0.686 , generator loss=0.735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #20, step 215, discriminator loss=0.690 , generator loss=0.745\n",
      "Training progress in epoch #20, step 216, discriminator loss=0.686 , generator loss=0.750\n",
      "Training progress in epoch #20, step 217, discriminator loss=0.686 , generator loss=0.737\n",
      "Training progress in epoch #20, step 218, discriminator loss=0.673 , generator loss=0.714\n",
      "Training progress in epoch #20, step 219, discriminator loss=0.687 , generator loss=0.725\n",
      "Training progress in epoch #20, step 220, discriminator loss=0.684 , generator loss=0.721\n",
      "Training progress in epoch #20, step 221, discriminator loss=0.683 , generator loss=0.686\n",
      "Training progress in epoch #20, step 222, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #20, step 223, discriminator loss=0.690 , generator loss=0.666\n",
      "Training progress in epoch #20, step 224, discriminator loss=0.692 , generator loss=0.662\n",
      "Training progress in epoch #20, step 225, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #20, step 226, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #20, step 227, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #20, step 228, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #20, step 229, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #20, step 230, discriminator loss=0.707 , generator loss=0.726\n",
      "Training progress in epoch #20, step 231, discriminator loss=0.701 , generator loss=0.740\n",
      "Training progress in epoch #20, step 232, discriminator loss=0.696 , generator loss=0.722\n",
      "Training progress in epoch #20, step 233, discriminator loss=0.695 , generator loss=0.706\n",
      "Disciminator Accuracy on real images: 59%, on fake images: 63%\n",
      "Training progress in epoch #21, step 0, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #21, step 1, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #21, step 2, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #21, step 3, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #21, step 4, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #21, step 5, discriminator loss=0.678 , generator loss=0.701\n",
      "Training progress in epoch #21, step 6, discriminator loss=0.682 , generator loss=0.680\n",
      "Training progress in epoch #21, step 7, discriminator loss=0.678 , generator loss=0.698\n",
      "Training progress in epoch #21, step 8, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #21, step 9, discriminator loss=0.682 , generator loss=0.719\n",
      "Training progress in epoch #21, step 10, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #21, step 11, discriminator loss=0.681 , generator loss=0.716\n",
      "Training progress in epoch #21, step 12, discriminator loss=0.682 , generator loss=0.719\n",
      "Training progress in epoch #21, step 13, discriminator loss=0.680 , generator loss=0.734\n",
      "Training progress in epoch #21, step 14, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #21, step 15, discriminator loss=0.688 , generator loss=0.743\n",
      "Training progress in epoch #21, step 16, discriminator loss=0.695 , generator loss=0.733\n",
      "Training progress in epoch #21, step 17, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #21, step 18, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #21, step 19, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #21, step 20, discriminator loss=0.701 , generator loss=0.679\n",
      "Training progress in epoch #21, step 21, discriminator loss=0.693 , generator loss=0.671\n",
      "Training progress in epoch #21, step 22, discriminator loss=0.698 , generator loss=0.680\n",
      "Training progress in epoch #21, step 23, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #21, step 24, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #21, step 25, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #21, step 26, discriminator loss=0.693 , generator loss=0.744\n",
      "Training progress in epoch #21, step 27, discriminator loss=0.686 , generator loss=0.727\n",
      "Training progress in epoch #21, step 28, discriminator loss=0.686 , generator loss=0.728\n",
      "Training progress in epoch #21, step 29, discriminator loss=0.680 , generator loss=0.770\n",
      "Training progress in epoch #21, step 30, discriminator loss=0.670 , generator loss=0.784\n",
      "Training progress in epoch #21, step 31, discriminator loss=0.681 , generator loss=0.743\n",
      "Training progress in epoch #21, step 32, discriminator loss=0.684 , generator loss=0.723\n",
      "Training progress in epoch #21, step 33, discriminator loss=0.684 , generator loss=0.693\n",
      "Training progress in epoch #21, step 34, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #21, step 35, discriminator loss=0.684 , generator loss=0.683\n",
      "Training progress in epoch #21, step 36, discriminator loss=0.693 , generator loss=0.678\n",
      "Training progress in epoch #21, step 37, discriminator loss=0.694 , generator loss=0.658\n",
      "Training progress in epoch #21, step 38, discriminator loss=0.685 , generator loss=0.646\n",
      "Training progress in epoch #21, step 39, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #21, step 40, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #21, step 41, discriminator loss=0.695 , generator loss=0.724\n",
      "Training progress in epoch #21, step 42, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #21, step 43, discriminator loss=0.693 , generator loss=0.742\n",
      "Training progress in epoch #21, step 44, discriminator loss=0.699 , generator loss=0.726\n",
      "Training progress in epoch #21, step 45, discriminator loss=0.703 , generator loss=0.728\n",
      "Training progress in epoch #21, step 46, discriminator loss=0.694 , generator loss=0.734\n",
      "Training progress in epoch #21, step 47, discriminator loss=0.691 , generator loss=0.740\n",
      "Training progress in epoch #21, step 48, discriminator loss=0.684 , generator loss=0.731\n",
      "Training progress in epoch #21, step 49, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #21, step 50, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #21, step 51, discriminator loss=0.691 , generator loss=0.655\n",
      "Training progress in epoch #21, step 52, discriminator loss=0.683 , generator loss=0.650\n",
      "Training progress in epoch #21, step 53, discriminator loss=0.680 , generator loss=0.674\n",
      "Training progress in epoch #21, step 54, discriminator loss=0.680 , generator loss=0.694\n",
      "Training progress in epoch #21, step 55, discriminator loss=0.683 , generator loss=0.708\n",
      "Training progress in epoch #21, step 56, discriminator loss=0.684 , generator loss=0.732\n",
      "Training progress in epoch #21, step 57, discriminator loss=0.689 , generator loss=0.744\n",
      "Training progress in epoch #21, step 58, discriminator loss=0.689 , generator loss=0.755\n",
      "Training progress in epoch #21, step 59, discriminator loss=0.689 , generator loss=0.760\n",
      "Training progress in epoch #21, step 60, discriminator loss=0.692 , generator loss=0.744\n",
      "Training progress in epoch #21, step 61, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #21, step 62, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #21, step 63, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #21, step 64, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #21, step 65, discriminator loss=0.702 , generator loss=0.691\n",
      "Training progress in epoch #21, step 66, discriminator loss=0.700 , generator loss=0.691\n",
      "Training progress in epoch #21, step 67, discriminator loss=0.698 , generator loss=0.677\n",
      "Training progress in epoch #21, step 68, discriminator loss=0.701 , generator loss=0.673\n",
      "Training progress in epoch #21, step 69, discriminator loss=0.690 , generator loss=0.678\n",
      "Training progress in epoch #21, step 70, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #21, step 71, discriminator loss=0.683 , generator loss=0.735\n",
      "Training progress in epoch #21, step 72, discriminator loss=0.683 , generator loss=0.739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #21, step 73, discriminator loss=0.687 , generator loss=0.736\n",
      "Training progress in epoch #21, step 74, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #21, step 75, discriminator loss=0.691 , generator loss=0.750\n",
      "Training progress in epoch #21, step 76, discriminator loss=0.688 , generator loss=0.740\n",
      "Training progress in epoch #21, step 77, discriminator loss=0.683 , generator loss=0.735\n",
      "Training progress in epoch #21, step 78, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #21, step 79, discriminator loss=0.684 , generator loss=0.720\n",
      "Training progress in epoch #21, step 80, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #21, step 81, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #21, step 82, discriminator loss=0.679 , generator loss=0.674\n",
      "Training progress in epoch #21, step 83, discriminator loss=0.690 , generator loss=0.673\n",
      "Training progress in epoch #21, step 84, discriminator loss=0.694 , generator loss=0.676\n",
      "Training progress in epoch #21, step 85, discriminator loss=0.697 , generator loss=0.681\n",
      "Training progress in epoch #21, step 86, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #21, step 87, discriminator loss=0.682 , generator loss=0.693\n",
      "Training progress in epoch #21, step 88, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #21, step 89, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #21, step 90, discriminator loss=0.693 , generator loss=0.733\n",
      "Training progress in epoch #21, step 91, discriminator loss=0.688 , generator loss=0.725\n",
      "Training progress in epoch #21, step 92, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #21, step 93, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #21, step 94, discriminator loss=0.680 , generator loss=0.717\n",
      "Training progress in epoch #21, step 95, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #21, step 96, discriminator loss=0.683 , generator loss=0.720\n",
      "Training progress in epoch #21, step 97, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #21, step 98, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #21, step 99, discriminator loss=0.687 , generator loss=0.677\n",
      "Training progress in epoch #21, step 100, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #21, step 101, discriminator loss=0.688 , generator loss=0.682\n",
      "Training progress in epoch #21, step 102, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #21, step 103, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #21, step 104, discriminator loss=0.691 , generator loss=0.741\n",
      "Training progress in epoch #21, step 105, discriminator loss=0.683 , generator loss=0.738\n",
      "Training progress in epoch #21, step 106, discriminator loss=0.693 , generator loss=0.758\n",
      "Training progress in epoch #21, step 107, discriminator loss=0.683 , generator loss=0.750\n",
      "Training progress in epoch #21, step 108, discriminator loss=0.683 , generator loss=0.730\n",
      "Training progress in epoch #21, step 109, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #21, step 110, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #21, step 111, discriminator loss=0.684 , generator loss=0.703\n",
      "Training progress in epoch #21, step 112, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #21, step 113, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #21, step 114, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #21, step 115, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #21, step 116, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #21, step 117, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #21, step 118, discriminator loss=0.691 , generator loss=0.733\n",
      "Training progress in epoch #21, step 119, discriminator loss=0.684 , generator loss=0.720\n",
      "Training progress in epoch #21, step 120, discriminator loss=0.684 , generator loss=0.735\n",
      "Training progress in epoch #21, step 121, discriminator loss=0.691 , generator loss=0.752\n",
      "Training progress in epoch #21, step 122, discriminator loss=0.679 , generator loss=0.720\n",
      "Training progress in epoch #21, step 123, discriminator loss=0.700 , generator loss=0.727\n",
      "Training progress in epoch #21, step 124, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #21, step 125, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #21, step 126, discriminator loss=0.685 , generator loss=0.730\n",
      "Training progress in epoch #21, step 127, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #21, step 128, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #21, step 129, discriminator loss=0.683 , generator loss=0.677\n",
      "Training progress in epoch #21, step 130, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #21, step 131, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #21, step 132, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #21, step 133, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #21, step 134, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #21, step 135, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #21, step 136, discriminator loss=0.692 , generator loss=0.672\n",
      "Training progress in epoch #21, step 137, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #21, step 138, discriminator loss=0.695 , generator loss=0.726\n",
      "Training progress in epoch #21, step 139, discriminator loss=0.695 , generator loss=0.732\n",
      "Training progress in epoch #21, step 140, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #21, step 141, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #21, step 142, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #21, step 143, discriminator loss=0.679 , generator loss=0.699\n",
      "Training progress in epoch #21, step 144, discriminator loss=0.683 , generator loss=0.711\n",
      "Training progress in epoch #21, step 145, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #21, step 146, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #21, step 147, discriminator loss=0.682 , generator loss=0.695\n",
      "Training progress in epoch #21, step 148, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #21, step 149, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #21, step 150, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #21, step 151, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #21, step 152, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #21, step 153, discriminator loss=0.689 , generator loss=0.740\n",
      "Training progress in epoch #21, step 154, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #21, step 155, discriminator loss=0.700 , generator loss=0.727\n",
      "Training progress in epoch #21, step 156, discriminator loss=0.694 , generator loss=0.726\n",
      "Training progress in epoch #21, step 157, discriminator loss=0.692 , generator loss=0.733\n",
      "Training progress in epoch #21, step 158, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #21, step 159, discriminator loss=0.683 , generator loss=0.730\n",
      "Training progress in epoch #21, step 160, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #21, step 161, discriminator loss=0.680 , generator loss=0.687\n",
      "Training progress in epoch #21, step 162, discriminator loss=0.684 , generator loss=0.693\n",
      "Training progress in epoch #21, step 163, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #21, step 164, discriminator loss=0.684 , generator loss=0.724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #21, step 165, discriminator loss=0.687 , generator loss=0.725\n",
      "Training progress in epoch #21, step 166, discriminator loss=0.679 , generator loss=0.731\n",
      "Training progress in epoch #21, step 167, discriminator loss=0.680 , generator loss=0.722\n",
      "Training progress in epoch #21, step 168, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #21, step 169, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #21, step 170, discriminator loss=0.687 , generator loss=0.735\n",
      "Training progress in epoch #21, step 171, discriminator loss=0.694 , generator loss=0.746\n",
      "Training progress in epoch #21, step 172, discriminator loss=0.696 , generator loss=0.722\n",
      "Training progress in epoch #21, step 173, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #21, step 174, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #21, step 175, discriminator loss=0.700 , generator loss=0.665\n",
      "Training progress in epoch #21, step 176, discriminator loss=0.699 , generator loss=0.662\n",
      "Training progress in epoch #21, step 177, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #21, step 178, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #21, step 179, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #21, step 180, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #21, step 181, discriminator loss=0.677 , generator loss=0.724\n",
      "Training progress in epoch #21, step 182, discriminator loss=0.697 , generator loss=0.737\n",
      "Training progress in epoch #21, step 183, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #21, step 184, discriminator loss=0.688 , generator loss=0.752\n",
      "Training progress in epoch #21, step 185, discriminator loss=0.686 , generator loss=0.751\n",
      "Training progress in epoch #21, step 186, discriminator loss=0.692 , generator loss=0.760\n",
      "Training progress in epoch #21, step 187, discriminator loss=0.680 , generator loss=0.752\n",
      "Training progress in epoch #21, step 188, discriminator loss=0.682 , generator loss=0.695\n",
      "Training progress in epoch #21, step 189, discriminator loss=0.693 , generator loss=0.666\n",
      "Training progress in epoch #21, step 190, discriminator loss=0.696 , generator loss=0.650\n",
      "Training progress in epoch #21, step 191, discriminator loss=0.688 , generator loss=0.656\n",
      "Training progress in epoch #21, step 192, discriminator loss=0.685 , generator loss=0.679\n",
      "Training progress in epoch #21, step 193, discriminator loss=0.694 , generator loss=0.680\n",
      "Training progress in epoch #21, step 194, discriminator loss=0.684 , generator loss=0.705\n",
      "Training progress in epoch #21, step 195, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #21, step 196, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #21, step 197, discriminator loss=0.701 , generator loss=0.771\n",
      "Training progress in epoch #21, step 198, discriminator loss=0.683 , generator loss=0.773\n",
      "Training progress in epoch #21, step 199, discriminator loss=0.698 , generator loss=0.738\n",
      "Training progress in epoch #21, step 200, discriminator loss=0.688 , generator loss=0.746\n",
      "Training progress in epoch #21, step 201, discriminator loss=0.689 , generator loss=0.769\n",
      "Training progress in epoch #21, step 202, discriminator loss=0.685 , generator loss=0.733\n",
      "Training progress in epoch #21, step 203, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #21, step 204, discriminator loss=0.695 , generator loss=0.674\n",
      "Training progress in epoch #21, step 205, discriminator loss=0.694 , generator loss=0.646\n",
      "Training progress in epoch #21, step 206, discriminator loss=0.696 , generator loss=0.626\n",
      "Training progress in epoch #21, step 207, discriminator loss=0.685 , generator loss=0.640\n",
      "Training progress in epoch #21, step 208, discriminator loss=0.689 , generator loss=0.675\n",
      "Training progress in epoch #21, step 209, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #21, step 210, discriminator loss=0.696 , generator loss=0.756\n",
      "Training progress in epoch #21, step 211, discriminator loss=0.698 , generator loss=0.749\n",
      "Training progress in epoch #21, step 212, discriminator loss=0.687 , generator loss=0.749\n",
      "Training progress in epoch #21, step 213, discriminator loss=0.683 , generator loss=0.749\n",
      "Training progress in epoch #21, step 214, discriminator loss=0.683 , generator loss=0.742\n",
      "Training progress in epoch #21, step 215, discriminator loss=0.686 , generator loss=0.747\n",
      "Training progress in epoch #21, step 216, discriminator loss=0.684 , generator loss=0.764\n",
      "Training progress in epoch #21, step 217, discriminator loss=0.688 , generator loss=0.733\n",
      "Training progress in epoch #21, step 218, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #21, step 219, discriminator loss=0.694 , generator loss=0.674\n",
      "Training progress in epoch #21, step 220, discriminator loss=0.698 , generator loss=0.659\n",
      "Training progress in epoch #21, step 221, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #21, step 222, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #21, step 223, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #21, step 224, discriminator loss=0.687 , generator loss=0.661\n",
      "Training progress in epoch #21, step 225, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #21, step 226, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #21, step 227, discriminator loss=0.700 , generator loss=0.757\n",
      "Training progress in epoch #21, step 228, discriminator loss=0.692 , generator loss=0.743\n",
      "Training progress in epoch #21, step 229, discriminator loss=0.696 , generator loss=0.752\n",
      "Training progress in epoch #21, step 230, discriminator loss=0.691 , generator loss=0.746\n",
      "Training progress in epoch #21, step 231, discriminator loss=0.697 , generator loss=0.756\n",
      "Training progress in epoch #21, step 232, discriminator loss=0.688 , generator loss=0.724\n",
      "Training progress in epoch #21, step 233, discriminator loss=0.681 , generator loss=0.695\n",
      "Disciminator Accuracy on real images: 74%, on fake images: 51%\n",
      "Training progress in epoch #22, step 0, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #22, step 1, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #22, step 2, discriminator loss=0.699 , generator loss=0.666\n",
      "Training progress in epoch #22, step 3, discriminator loss=0.693 , generator loss=0.664\n",
      "Training progress in epoch #22, step 4, discriminator loss=0.684 , generator loss=0.673\n",
      "Training progress in epoch #22, step 5, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #22, step 6, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #22, step 7, discriminator loss=0.690 , generator loss=0.738\n",
      "Training progress in epoch #22, step 8, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #22, step 9, discriminator loss=0.694 , generator loss=0.760\n",
      "Training progress in epoch #22, step 10, discriminator loss=0.684 , generator loss=0.747\n",
      "Training progress in epoch #22, step 11, discriminator loss=0.692 , generator loss=0.743\n",
      "Training progress in epoch #22, step 12, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #22, step 13, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #22, step 14, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #22, step 15, discriminator loss=0.696 , generator loss=0.695\n",
      "Training progress in epoch #22, step 16, discriminator loss=0.693 , generator loss=0.667\n",
      "Training progress in epoch #22, step 17, discriminator loss=0.695 , generator loss=0.655\n",
      "Training progress in epoch #22, step 18, discriminator loss=0.686 , generator loss=0.667\n",
      "Training progress in epoch #22, step 19, discriminator loss=0.682 , generator loss=0.677\n",
      "Training progress in epoch #22, step 20, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #22, step 21, discriminator loss=0.689 , generator loss=0.733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #22, step 22, discriminator loss=0.690 , generator loss=0.758\n",
      "Training progress in epoch #22, step 23, discriminator loss=0.684 , generator loss=0.761\n",
      "Training progress in epoch #22, step 24, discriminator loss=0.684 , generator loss=0.770\n",
      "Training progress in epoch #22, step 25, discriminator loss=0.685 , generator loss=0.725\n",
      "Training progress in epoch #22, step 26, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #22, step 27, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #22, step 28, discriminator loss=0.683 , generator loss=0.718\n",
      "Training progress in epoch #22, step 29, discriminator loss=0.683 , generator loss=0.708\n",
      "Training progress in epoch #22, step 30, discriminator loss=0.693 , generator loss=0.668\n",
      "Training progress in epoch #22, step 31, discriminator loss=0.691 , generator loss=0.662\n",
      "Training progress in epoch #22, step 32, discriminator loss=0.696 , generator loss=0.676\n",
      "Training progress in epoch #22, step 33, discriminator loss=0.684 , generator loss=0.680\n",
      "Training progress in epoch #22, step 34, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #22, step 35, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #22, step 36, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #22, step 37, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #22, step 38, discriminator loss=0.691 , generator loss=0.735\n",
      "Training progress in epoch #22, step 39, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #22, step 40, discriminator loss=0.696 , generator loss=0.742\n",
      "Training progress in epoch #22, step 41, discriminator loss=0.689 , generator loss=0.743\n",
      "Training progress in epoch #22, step 42, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #22, step 43, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #22, step 44, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #22, step 45, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #22, step 46, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #22, step 47, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #22, step 48, discriminator loss=0.690 , generator loss=0.675\n",
      "Training progress in epoch #22, step 49, discriminator loss=0.674 , generator loss=0.680\n",
      "Training progress in epoch #22, step 50, discriminator loss=0.681 , generator loss=0.696\n",
      "Training progress in epoch #22, step 51, discriminator loss=0.687 , generator loss=0.727\n",
      "Training progress in epoch #22, step 52, discriminator loss=0.696 , generator loss=0.736\n",
      "Training progress in epoch #22, step 53, discriminator loss=0.685 , generator loss=0.735\n",
      "Training progress in epoch #22, step 54, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #22, step 55, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #22, step 56, discriminator loss=0.693 , generator loss=0.728\n",
      "Training progress in epoch #22, step 57, discriminator loss=0.685 , generator loss=0.728\n",
      "Training progress in epoch #22, step 58, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #22, step 59, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #22, step 60, discriminator loss=0.696 , generator loss=0.678\n",
      "Training progress in epoch #22, step 61, discriminator loss=0.696 , generator loss=0.652\n",
      "Training progress in epoch #22, step 62, discriminator loss=0.693 , generator loss=0.660\n",
      "Training progress in epoch #22, step 63, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #22, step 64, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #22, step 65, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #22, step 66, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #22, step 67, discriminator loss=0.689 , generator loss=0.728\n",
      "Training progress in epoch #22, step 68, discriminator loss=0.687 , generator loss=0.756\n",
      "Training progress in epoch #22, step 69, discriminator loss=0.699 , generator loss=0.740\n",
      "Training progress in epoch #22, step 70, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #22, step 71, discriminator loss=0.682 , generator loss=0.714\n",
      "Training progress in epoch #22, step 72, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #22, step 73, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #22, step 74, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #22, step 75, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #22, step 76, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #22, step 77, discriminator loss=0.680 , generator loss=0.683\n",
      "Training progress in epoch #22, step 78, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #22, step 79, discriminator loss=0.679 , generator loss=0.704\n",
      "Training progress in epoch #22, step 80, discriminator loss=0.684 , generator loss=0.701\n",
      "Training progress in epoch #22, step 81, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #22, step 82, discriminator loss=0.692 , generator loss=0.745\n",
      "Training progress in epoch #22, step 83, discriminator loss=0.685 , generator loss=0.723\n",
      "Training progress in epoch #22, step 84, discriminator loss=0.690 , generator loss=0.729\n",
      "Training progress in epoch #22, step 85, discriminator loss=0.685 , generator loss=0.721\n",
      "Training progress in epoch #22, step 86, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #22, step 87, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #22, step 88, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #22, step 89, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #22, step 90, discriminator loss=0.695 , generator loss=0.660\n",
      "Training progress in epoch #22, step 91, discriminator loss=0.702 , generator loss=0.663\n",
      "Training progress in epoch #22, step 92, discriminator loss=0.688 , generator loss=0.673\n",
      "Training progress in epoch #22, step 93, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #22, step 94, discriminator loss=0.680 , generator loss=0.701\n",
      "Training progress in epoch #22, step 95, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #22, step 96, discriminator loss=0.692 , generator loss=0.731\n",
      "Training progress in epoch #22, step 97, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #22, step 98, discriminator loss=0.676 , generator loss=0.726\n",
      "Training progress in epoch #22, step 99, discriminator loss=0.688 , generator loss=0.741\n",
      "Training progress in epoch #22, step 100, discriminator loss=0.684 , generator loss=0.764\n",
      "Training progress in epoch #22, step 101, discriminator loss=0.688 , generator loss=0.771\n",
      "Training progress in epoch #22, step 102, discriminator loss=0.682 , generator loss=0.742\n",
      "Training progress in epoch #22, step 103, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #22, step 104, discriminator loss=0.685 , generator loss=0.712\n",
      "Training progress in epoch #22, step 105, discriminator loss=0.683 , generator loss=0.674\n",
      "Training progress in epoch #22, step 106, discriminator loss=0.696 , generator loss=0.677\n",
      "Training progress in epoch #22, step 107, discriminator loss=0.690 , generator loss=0.656\n",
      "Training progress in epoch #22, step 108, discriminator loss=0.689 , generator loss=0.648\n",
      "Training progress in epoch #22, step 109, discriminator loss=0.690 , generator loss=0.672\n",
      "Training progress in epoch #22, step 110, discriminator loss=0.695 , generator loss=0.689\n",
      "Training progress in epoch #22, step 111, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #22, step 112, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #22, step 113, discriminator loss=0.689 , generator loss=0.720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #22, step 114, discriminator loss=0.703 , generator loss=0.767\n",
      "Training progress in epoch #22, step 115, discriminator loss=0.692 , generator loss=0.766\n",
      "Training progress in epoch #22, step 116, discriminator loss=0.695 , generator loss=0.767\n",
      "Training progress in epoch #22, step 117, discriminator loss=0.688 , generator loss=0.767\n",
      "Training progress in epoch #22, step 118, discriminator loss=0.683 , generator loss=0.742\n",
      "Training progress in epoch #22, step 119, discriminator loss=0.688 , generator loss=0.737\n",
      "Training progress in epoch #22, step 120, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #22, step 121, discriminator loss=0.692 , generator loss=0.643\n",
      "Training progress in epoch #22, step 122, discriminator loss=0.695 , generator loss=0.628\n",
      "Training progress in epoch #22, step 123, discriminator loss=0.680 , generator loss=0.655\n",
      "Training progress in epoch #22, step 124, discriminator loss=0.679 , generator loss=0.669\n",
      "Training progress in epoch #22, step 125, discriminator loss=0.684 , generator loss=0.688\n",
      "Training progress in epoch #22, step 126, discriminator loss=0.676 , generator loss=0.704\n",
      "Training progress in epoch #22, step 127, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #22, step 128, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #22, step 129, discriminator loss=0.683 , generator loss=0.754\n",
      "Training progress in epoch #22, step 130, discriminator loss=0.693 , generator loss=0.760\n",
      "Training progress in epoch #22, step 131, discriminator loss=0.693 , generator loss=0.765\n",
      "Training progress in epoch #22, step 132, discriminator loss=0.691 , generator loss=0.773\n",
      "Training progress in epoch #22, step 133, discriminator loss=0.689 , generator loss=0.775\n",
      "Training progress in epoch #22, step 134, discriminator loss=0.699 , generator loss=0.714\n",
      "Training progress in epoch #22, step 135, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #22, step 136, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #22, step 137, discriminator loss=0.692 , generator loss=0.661\n",
      "Training progress in epoch #22, step 138, discriminator loss=0.697 , generator loss=0.636\n",
      "Training progress in epoch #22, step 139, discriminator loss=0.691 , generator loss=0.640\n",
      "Training progress in epoch #22, step 140, discriminator loss=0.687 , generator loss=0.665\n",
      "Training progress in epoch #22, step 141, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #22, step 142, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #22, step 143, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #22, step 144, discriminator loss=0.690 , generator loss=0.744\n",
      "Training progress in epoch #22, step 145, discriminator loss=0.689 , generator loss=0.770\n",
      "Training progress in epoch #22, step 146, discriminator loss=0.694 , generator loss=0.760\n",
      "Training progress in epoch #22, step 147, discriminator loss=0.686 , generator loss=0.775\n",
      "Training progress in epoch #22, step 148, discriminator loss=0.682 , generator loss=0.767\n",
      "Training progress in epoch #22, step 149, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #22, step 150, discriminator loss=0.683 , generator loss=0.687\n",
      "Training progress in epoch #22, step 151, discriminator loss=0.687 , generator loss=0.675\n",
      "Training progress in epoch #22, step 152, discriminator loss=0.693 , generator loss=0.658\n",
      "Training progress in epoch #22, step 153, discriminator loss=0.690 , generator loss=0.656\n",
      "Training progress in epoch #22, step 154, discriminator loss=0.690 , generator loss=0.656\n",
      "Training progress in epoch #22, step 155, discriminator loss=0.684 , generator loss=0.655\n",
      "Training progress in epoch #22, step 156, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #22, step 157, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #22, step 158, discriminator loss=0.690 , generator loss=0.734\n",
      "Training progress in epoch #22, step 159, discriminator loss=0.685 , generator loss=0.730\n",
      "Training progress in epoch #22, step 160, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #22, step 161, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #22, step 162, discriminator loss=0.689 , generator loss=0.750\n",
      "Training progress in epoch #22, step 163, discriminator loss=0.684 , generator loss=0.765\n",
      "Training progress in epoch #22, step 164, discriminator loss=0.695 , generator loss=0.740\n",
      "Training progress in epoch #22, step 165, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #22, step 166, discriminator loss=0.698 , generator loss=0.695\n",
      "Training progress in epoch #22, step 167, discriminator loss=0.685 , generator loss=0.683\n",
      "Training progress in epoch #22, step 168, discriminator loss=0.690 , generator loss=0.661\n",
      "Training progress in epoch #22, step 169, discriminator loss=0.697 , generator loss=0.658\n",
      "Training progress in epoch #22, step 170, discriminator loss=0.690 , generator loss=0.667\n",
      "Training progress in epoch #22, step 171, discriminator loss=0.681 , generator loss=0.686\n",
      "Training progress in epoch #22, step 172, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #22, step 173, discriminator loss=0.696 , generator loss=0.706\n",
      "Training progress in epoch #22, step 174, discriminator loss=0.685 , generator loss=0.715\n",
      "Training progress in epoch #22, step 175, discriminator loss=0.688 , generator loss=0.732\n",
      "Training progress in epoch #22, step 176, discriminator loss=0.690 , generator loss=0.742\n",
      "Training progress in epoch #22, step 177, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #22, step 178, discriminator loss=0.686 , generator loss=0.748\n",
      "Training progress in epoch #22, step 179, discriminator loss=0.688 , generator loss=0.744\n",
      "Training progress in epoch #22, step 180, discriminator loss=0.689 , generator loss=0.734\n",
      "Training progress in epoch #22, step 181, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #22, step 182, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #22, step 183, discriminator loss=0.694 , generator loss=0.673\n",
      "Training progress in epoch #22, step 184, discriminator loss=0.690 , generator loss=0.672\n",
      "Training progress in epoch #22, step 185, discriminator loss=0.693 , generator loss=0.664\n",
      "Training progress in epoch #22, step 186, discriminator loss=0.686 , generator loss=0.656\n",
      "Training progress in epoch #22, step 187, discriminator loss=0.695 , generator loss=0.667\n",
      "Training progress in epoch #22, step 188, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #22, step 189, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #22, step 190, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #22, step 191, discriminator loss=0.687 , generator loss=0.740\n",
      "Training progress in epoch #22, step 192, discriminator loss=0.695 , generator loss=0.754\n",
      "Training progress in epoch #22, step 193, discriminator loss=0.692 , generator loss=0.767\n",
      "Training progress in epoch #22, step 194, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #22, step 195, discriminator loss=0.683 , generator loss=0.713\n",
      "Training progress in epoch #22, step 196, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #22, step 197, discriminator loss=0.685 , generator loss=0.732\n",
      "Training progress in epoch #22, step 198, discriminator loss=0.680 , generator loss=0.716\n",
      "Training progress in epoch #22, step 199, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #22, step 200, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #22, step 201, discriminator loss=0.695 , generator loss=0.669\n",
      "Training progress in epoch #22, step 202, discriminator loss=0.690 , generator loss=0.663\n",
      "Training progress in epoch #22, step 203, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #22, step 204, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #22, step 205, discriminator loss=0.692 , generator loss=0.716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #22, step 206, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #22, step 207, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #22, step 208, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #22, step 209, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #22, step 210, discriminator loss=0.695 , generator loss=0.731\n",
      "Training progress in epoch #22, step 211, discriminator loss=0.684 , generator loss=0.755\n",
      "Training progress in epoch #22, step 212, discriminator loss=0.685 , generator loss=0.752\n",
      "Training progress in epoch #22, step 213, discriminator loss=0.692 , generator loss=0.732\n",
      "Training progress in epoch #22, step 214, discriminator loss=0.687 , generator loss=0.726\n",
      "Training progress in epoch #22, step 215, discriminator loss=0.683 , generator loss=0.696\n",
      "Training progress in epoch #22, step 216, discriminator loss=0.684 , generator loss=0.688\n",
      "Training progress in epoch #22, step 217, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #22, step 218, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #22, step 219, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #22, step 220, discriminator loss=0.685 , generator loss=0.677\n",
      "Training progress in epoch #22, step 221, discriminator loss=0.691 , generator loss=0.673\n",
      "Training progress in epoch #22, step 222, discriminator loss=0.683 , generator loss=0.719\n",
      "Training progress in epoch #22, step 223, discriminator loss=0.685 , generator loss=0.751\n",
      "Training progress in epoch #22, step 224, discriminator loss=0.692 , generator loss=0.742\n",
      "Training progress in epoch #22, step 225, discriminator loss=0.697 , generator loss=0.743\n",
      "Training progress in epoch #22, step 226, discriminator loss=0.685 , generator loss=0.729\n",
      "Training progress in epoch #22, step 227, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #22, step 228, discriminator loss=0.684 , generator loss=0.728\n",
      "Training progress in epoch #22, step 229, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #22, step 230, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #22, step 231, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #22, step 232, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #22, step 233, discriminator loss=0.689 , generator loss=0.666\n",
      "Disciminator Accuracy on real images: 92%, on fake images: 17%\n",
      "Training progress in epoch #23, step 0, discriminator loss=0.689 , generator loss=0.662\n",
      "Training progress in epoch #23, step 1, discriminator loss=0.692 , generator loss=0.666\n",
      "Training progress in epoch #23, step 2, discriminator loss=0.694 , generator loss=0.666\n",
      "Training progress in epoch #23, step 3, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #23, step 4, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #23, step 5, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #23, step 6, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #23, step 7, discriminator loss=0.697 , generator loss=0.744\n",
      "Training progress in epoch #23, step 8, discriminator loss=0.697 , generator loss=0.735\n",
      "Training progress in epoch #23, step 9, discriminator loss=0.691 , generator loss=0.748\n",
      "Training progress in epoch #23, step 10, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #23, step 11, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #23, step 12, discriminator loss=0.687 , generator loss=0.720\n",
      "Training progress in epoch #23, step 13, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #23, step 14, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #23, step 15, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #23, step 16, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #23, step 17, discriminator loss=0.680 , generator loss=0.660\n",
      "Training progress in epoch #23, step 18, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #23, step 19, discriminator loss=0.681 , generator loss=0.697\n",
      "Training progress in epoch #23, step 20, discriminator loss=0.684 , generator loss=0.707\n",
      "Training progress in epoch #23, step 21, discriminator loss=0.681 , generator loss=0.713\n",
      "Training progress in epoch #23, step 22, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #23, step 23, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #23, step 24, discriminator loss=0.696 , generator loss=0.743\n",
      "Training progress in epoch #23, step 25, discriminator loss=0.685 , generator loss=0.745\n",
      "Training progress in epoch #23, step 26, discriminator loss=0.691 , generator loss=0.735\n",
      "Training progress in epoch #23, step 27, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #23, step 28, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #23, step 29, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #23, step 30, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #23, step 31, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #23, step 32, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #23, step 33, discriminator loss=0.684 , generator loss=0.678\n",
      "Training progress in epoch #23, step 34, discriminator loss=0.678 , generator loss=0.683\n",
      "Training progress in epoch #23, step 35, discriminator loss=0.678 , generator loss=0.698\n",
      "Training progress in epoch #23, step 36, discriminator loss=0.685 , generator loss=0.714\n",
      "Training progress in epoch #23, step 37, discriminator loss=0.689 , generator loss=0.748\n",
      "Training progress in epoch #23, step 38, discriminator loss=0.693 , generator loss=0.756\n",
      "Training progress in epoch #23, step 39, discriminator loss=0.689 , generator loss=0.743\n",
      "Training progress in epoch #23, step 40, discriminator loss=0.684 , generator loss=0.714\n",
      "Training progress in epoch #23, step 41, discriminator loss=0.686 , generator loss=0.741\n",
      "Training progress in epoch #23, step 42, discriminator loss=0.695 , generator loss=0.743\n",
      "Training progress in epoch #23, step 43, discriminator loss=0.693 , generator loss=0.740\n",
      "Training progress in epoch #23, step 44, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #23, step 45, discriminator loss=0.700 , generator loss=0.668\n",
      "Training progress in epoch #23, step 46, discriminator loss=0.691 , generator loss=0.645\n",
      "Training progress in epoch #23, step 47, discriminator loss=0.694 , generator loss=0.655\n",
      "Training progress in epoch #23, step 48, discriminator loss=0.696 , generator loss=0.679\n",
      "Training progress in epoch #23, step 49, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #23, step 50, discriminator loss=0.690 , generator loss=0.729\n",
      "Training progress in epoch #23, step 51, discriminator loss=0.692 , generator loss=0.734\n",
      "Training progress in epoch #23, step 52, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #23, step 53, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #23, step 54, discriminator loss=0.691 , generator loss=0.757\n",
      "Training progress in epoch #23, step 55, discriminator loss=0.683 , generator loss=0.773\n",
      "Training progress in epoch #23, step 56, discriminator loss=0.684 , generator loss=0.742\n",
      "Training progress in epoch #23, step 57, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #23, step 58, discriminator loss=0.683 , generator loss=0.709\n",
      "Training progress in epoch #23, step 59, discriminator loss=0.684 , generator loss=0.721\n",
      "Training progress in epoch #23, step 60, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #23, step 61, discriminator loss=0.690 , generator loss=0.672\n",
      "Training progress in epoch #23, step 62, discriminator loss=0.689 , generator loss=0.662\n",
      "Training progress in epoch #23, step 63, discriminator loss=0.696 , generator loss=0.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #23, step 64, discriminator loss=0.690 , generator loss=0.660\n",
      "Training progress in epoch #23, step 65, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #23, step 66, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #23, step 67, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #23, step 68, discriminator loss=0.691 , generator loss=0.743\n",
      "Training progress in epoch #23, step 69, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #23, step 70, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #23, step 71, discriminator loss=0.690 , generator loss=0.746\n",
      "Training progress in epoch #23, step 72, discriminator loss=0.682 , generator loss=0.727\n",
      "Training progress in epoch #23, step 73, discriminator loss=0.683 , generator loss=0.738\n",
      "Training progress in epoch #23, step 74, discriminator loss=0.689 , generator loss=0.735\n",
      "Training progress in epoch #23, step 75, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #23, step 76, discriminator loss=0.683 , generator loss=0.679\n",
      "Training progress in epoch #23, step 77, discriminator loss=0.687 , generator loss=0.668\n",
      "Training progress in epoch #23, step 78, discriminator loss=0.694 , generator loss=0.678\n",
      "Training progress in epoch #23, step 79, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #23, step 80, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #23, step 81, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #23, step 82, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #23, step 83, discriminator loss=0.690 , generator loss=0.743\n",
      "Training progress in epoch #23, step 84, discriminator loss=0.694 , generator loss=0.749\n",
      "Training progress in epoch #23, step 85, discriminator loss=0.691 , generator loss=0.764\n",
      "Training progress in epoch #23, step 86, discriminator loss=0.691 , generator loss=0.738\n",
      "Training progress in epoch #23, step 87, discriminator loss=0.683 , generator loss=0.726\n",
      "Training progress in epoch #23, step 88, discriminator loss=0.676 , generator loss=0.732\n",
      "Training progress in epoch #23, step 89, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #23, step 90, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #23, step 91, discriminator loss=0.687 , generator loss=0.673\n",
      "Training progress in epoch #23, step 92, discriminator loss=0.687 , generator loss=0.660\n",
      "Training progress in epoch #23, step 93, discriminator loss=0.684 , generator loss=0.693\n",
      "Training progress in epoch #23, step 94, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #23, step 95, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #23, step 96, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #23, step 97, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #23, step 98, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #23, step 99, discriminator loss=0.695 , generator loss=0.725\n",
      "Training progress in epoch #23, step 100, discriminator loss=0.699 , generator loss=0.735\n",
      "Training progress in epoch #23, step 101, discriminator loss=0.701 , generator loss=0.732\n",
      "Training progress in epoch #23, step 102, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #23, step 103, discriminator loss=0.685 , generator loss=0.740\n",
      "Training progress in epoch #23, step 104, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #23, step 105, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #23, step 106, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #23, step 107, discriminator loss=0.678 , generator loss=0.712\n",
      "Training progress in epoch #23, step 108, discriminator loss=0.681 , generator loss=0.697\n",
      "Training progress in epoch #23, step 109, discriminator loss=0.676 , generator loss=0.689\n",
      "Training progress in epoch #23, step 110, discriminator loss=0.684 , generator loss=0.682\n",
      "Training progress in epoch #23, step 111, discriminator loss=0.684 , generator loss=0.700\n",
      "Training progress in epoch #23, step 112, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #23, step 113, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #23, step 114, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #23, step 115, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #23, step 116, discriminator loss=0.690 , generator loss=0.736\n",
      "Training progress in epoch #23, step 117, discriminator loss=0.688 , generator loss=0.737\n",
      "Training progress in epoch #23, step 118, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #23, step 119, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #23, step 120, discriminator loss=0.691 , generator loss=0.674\n",
      "Training progress in epoch #23, step 121, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #23, step 122, discriminator loss=0.689 , generator loss=0.666\n",
      "Training progress in epoch #23, step 123, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #23, step 124, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #23, step 125, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #23, step 126, discriminator loss=0.680 , generator loss=0.705\n",
      "Training progress in epoch #23, step 127, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #23, step 128, discriminator loss=0.682 , generator loss=0.732\n",
      "Training progress in epoch #23, step 129, discriminator loss=0.682 , generator loss=0.703\n",
      "Training progress in epoch #23, step 130, discriminator loss=0.687 , generator loss=0.744\n",
      "Training progress in epoch #23, step 131, discriminator loss=0.688 , generator loss=0.748\n",
      "Training progress in epoch #23, step 132, discriminator loss=0.686 , generator loss=0.741\n",
      "Training progress in epoch #23, step 133, discriminator loss=0.687 , generator loss=0.734\n",
      "Training progress in epoch #23, step 134, discriminator loss=0.685 , generator loss=0.720\n",
      "Training progress in epoch #23, step 135, discriminator loss=0.693 , generator loss=0.671\n",
      "Training progress in epoch #23, step 136, discriminator loss=0.695 , generator loss=0.683\n",
      "Training progress in epoch #23, step 137, discriminator loss=0.690 , generator loss=0.671\n",
      "Training progress in epoch #23, step 138, discriminator loss=0.691 , generator loss=0.650\n",
      "Training progress in epoch #23, step 139, discriminator loss=0.699 , generator loss=0.649\n",
      "Training progress in epoch #23, step 140, discriminator loss=0.690 , generator loss=0.673\n",
      "Training progress in epoch #23, step 141, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #23, step 142, discriminator loss=0.693 , generator loss=0.732\n",
      "Training progress in epoch #23, step 143, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #23, step 144, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #23, step 145, discriminator loss=0.695 , generator loss=0.771\n",
      "Training progress in epoch #23, step 146, discriminator loss=0.693 , generator loss=0.782\n",
      "Training progress in epoch #23, step 147, discriminator loss=0.688 , generator loss=0.751\n",
      "Training progress in epoch #23, step 148, discriminator loss=0.684 , generator loss=0.739\n",
      "Training progress in epoch #23, step 149, discriminator loss=0.678 , generator loss=0.715\n",
      "Training progress in epoch #23, step 150, discriminator loss=0.680 , generator loss=0.695\n",
      "Training progress in epoch #23, step 151, discriminator loss=0.692 , generator loss=0.672\n",
      "Training progress in epoch #23, step 152, discriminator loss=0.688 , generator loss=0.669\n",
      "Training progress in epoch #23, step 153, discriminator loss=0.684 , generator loss=0.660\n",
      "Training progress in epoch #23, step 154, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #23, step 155, discriminator loss=0.687 , generator loss=0.696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #23, step 156, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #23, step 157, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #23, step 158, discriminator loss=0.697 , generator loss=0.715\n",
      "Training progress in epoch #23, step 159, discriminator loss=0.697 , generator loss=0.762\n",
      "Training progress in epoch #23, step 160, discriminator loss=0.694 , generator loss=0.740\n",
      "Training progress in epoch #23, step 161, discriminator loss=0.693 , generator loss=0.775\n",
      "Training progress in epoch #23, step 162, discriminator loss=0.688 , generator loss=0.757\n",
      "Training progress in epoch #23, step 163, discriminator loss=0.690 , generator loss=0.756\n",
      "Training progress in epoch #23, step 164, discriminator loss=0.676 , generator loss=0.739\n",
      "Training progress in epoch #23, step 165, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #23, step 166, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #23, step 167, discriminator loss=0.696 , generator loss=0.672\n",
      "Training progress in epoch #23, step 168, discriminator loss=0.691 , generator loss=0.640\n",
      "Training progress in epoch #23, step 169, discriminator loss=0.685 , generator loss=0.626\n",
      "Training progress in epoch #23, step 170, discriminator loss=0.695 , generator loss=0.652\n",
      "Training progress in epoch #23, step 171, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #23, step 172, discriminator loss=0.678 , generator loss=0.704\n",
      "Training progress in epoch #23, step 173, discriminator loss=0.679 , generator loss=0.717\n",
      "Training progress in epoch #23, step 174, discriminator loss=0.695 , generator loss=0.734\n",
      "Training progress in epoch #23, step 175, discriminator loss=0.691 , generator loss=0.762\n",
      "Training progress in epoch #23, step 176, discriminator loss=0.691 , generator loss=0.801\n",
      "Training progress in epoch #23, step 177, discriminator loss=0.684 , generator loss=0.785\n",
      "Training progress in epoch #23, step 178, discriminator loss=0.689 , generator loss=0.742\n",
      "Training progress in epoch #23, step 179, discriminator loss=0.680 , generator loss=0.720\n",
      "Training progress in epoch #23, step 180, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #23, step 181, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #23, step 182, discriminator loss=0.696 , generator loss=0.685\n",
      "Training progress in epoch #23, step 183, discriminator loss=0.694 , generator loss=0.663\n",
      "Training progress in epoch #23, step 184, discriminator loss=0.694 , generator loss=0.642\n",
      "Training progress in epoch #23, step 185, discriminator loss=0.682 , generator loss=0.657\n",
      "Training progress in epoch #23, step 186, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #23, step 187, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #23, step 188, discriminator loss=0.686 , generator loss=0.717\n",
      "Training progress in epoch #23, step 189, discriminator loss=0.690 , generator loss=0.748\n",
      "Training progress in epoch #23, step 190, discriminator loss=0.695 , generator loss=0.776\n",
      "Training progress in epoch #23, step 191, discriminator loss=0.689 , generator loss=0.750\n",
      "Training progress in epoch #23, step 192, discriminator loss=0.683 , generator loss=0.727\n",
      "Training progress in epoch #23, step 193, discriminator loss=0.699 , generator loss=0.731\n",
      "Training progress in epoch #23, step 194, discriminator loss=0.694 , generator loss=0.731\n",
      "Training progress in epoch #23, step 195, discriminator loss=0.680 , generator loss=0.730\n",
      "Training progress in epoch #23, step 196, discriminator loss=0.698 , generator loss=0.688\n",
      "Training progress in epoch #23, step 197, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #23, step 198, discriminator loss=0.693 , generator loss=0.674\n",
      "Training progress in epoch #23, step 199, discriminator loss=0.694 , generator loss=0.668\n",
      "Training progress in epoch #23, step 200, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #23, step 201, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #23, step 202, discriminator loss=0.682 , generator loss=0.709\n",
      "Training progress in epoch #23, step 203, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #23, step 204, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #23, step 205, discriminator loss=0.691 , generator loss=0.750\n",
      "Training progress in epoch #23, step 206, discriminator loss=0.683 , generator loss=0.715\n",
      "Training progress in epoch #23, step 207, discriminator loss=0.684 , generator loss=0.728\n",
      "Training progress in epoch #23, step 208, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #23, step 209, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #23, step 210, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #23, step 211, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #23, step 212, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #23, step 213, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #23, step 214, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #23, step 215, discriminator loss=0.698 , generator loss=0.685\n",
      "Training progress in epoch #23, step 216, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #23, step 217, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #23, step 218, discriminator loss=0.693 , generator loss=0.737\n",
      "Training progress in epoch #23, step 219, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #23, step 220, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #23, step 221, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #23, step 222, discriminator loss=0.688 , generator loss=0.746\n",
      "Training progress in epoch #23, step 223, discriminator loss=0.683 , generator loss=0.728\n",
      "Training progress in epoch #23, step 224, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #23, step 225, discriminator loss=0.680 , generator loss=0.695\n",
      "Training progress in epoch #23, step 226, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #23, step 227, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #23, step 228, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #23, step 229, discriminator loss=0.693 , generator loss=0.664\n",
      "Training progress in epoch #23, step 230, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #23, step 231, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #23, step 232, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #23, step 233, discriminator loss=0.694 , generator loss=0.716\n",
      "Disciminator Accuracy on real images: 27%, on fake images: 89%\n",
      "Training progress in epoch #24, step 0, discriminator loss=0.689 , generator loss=0.742\n",
      "Training progress in epoch #24, step 1, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #24, step 2, discriminator loss=0.699 , generator loss=0.692\n",
      "Training progress in epoch #24, step 3, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #24, step 4, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #24, step 5, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #24, step 6, discriminator loss=0.682 , generator loss=0.719\n",
      "Training progress in epoch #24, step 7, discriminator loss=0.680 , generator loss=0.702\n",
      "Training progress in epoch #24, step 8, discriminator loss=0.681 , generator loss=0.721\n",
      "Training progress in epoch #24, step 9, discriminator loss=0.684 , generator loss=0.749\n",
      "Training progress in epoch #24, step 10, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #24, step 11, discriminator loss=0.683 , generator loss=0.689\n",
      "Training progress in epoch #24, step 12, discriminator loss=0.684 , generator loss=0.690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #24, step 13, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #24, step 14, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #24, step 15, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #24, step 16, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #24, step 17, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #24, step 18, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #24, step 19, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #24, step 20, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #24, step 21, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #24, step 22, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #24, step 23, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #24, step 24, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #24, step 25, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #24, step 26, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #24, step 27, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #24, step 28, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #24, step 29, discriminator loss=0.690 , generator loss=0.727\n",
      "Training progress in epoch #24, step 30, discriminator loss=0.685 , generator loss=0.720\n",
      "Training progress in epoch #24, step 31, discriminator loss=0.682 , generator loss=0.730\n",
      "Training progress in epoch #24, step 32, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #24, step 33, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #24, step 34, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #24, step 35, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #24, step 36, discriminator loss=0.684 , generator loss=0.694\n",
      "Training progress in epoch #24, step 37, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #24, step 38, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #24, step 39, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #24, step 40, discriminator loss=0.692 , generator loss=0.666\n",
      "Training progress in epoch #24, step 41, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #24, step 42, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #24, step 43, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #24, step 44, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #24, step 45, discriminator loss=0.696 , generator loss=0.694\n",
      "Training progress in epoch #24, step 46, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #24, step 47, discriminator loss=0.694 , generator loss=0.728\n",
      "Training progress in epoch #24, step 48, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #24, step 49, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #24, step 50, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #24, step 51, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #24, step 52, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #24, step 53, discriminator loss=0.684 , generator loss=0.705\n",
      "Training progress in epoch #24, step 54, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #24, step 55, discriminator loss=0.693 , generator loss=0.672\n",
      "Training progress in epoch #24, step 56, discriminator loss=0.691 , generator loss=0.677\n",
      "Training progress in epoch #24, step 57, discriminator loss=0.684 , generator loss=0.676\n",
      "Training progress in epoch #24, step 58, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #24, step 59, discriminator loss=0.693 , generator loss=0.732\n",
      "Training progress in epoch #24, step 60, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #24, step 61, discriminator loss=0.686 , generator loss=0.736\n",
      "Training progress in epoch #24, step 62, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #24, step 63, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #24, step 64, discriminator loss=0.694 , generator loss=0.768\n",
      "Training progress in epoch #24, step 65, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #24, step 66, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #24, step 67, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #24, step 68, discriminator loss=0.697 , generator loss=0.676\n",
      "Training progress in epoch #24, step 69, discriminator loss=0.684 , generator loss=0.670\n",
      "Training progress in epoch #24, step 70, discriminator loss=0.695 , generator loss=0.666\n",
      "Training progress in epoch #24, step 71, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #24, step 72, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #24, step 73, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #24, step 74, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #24, step 75, discriminator loss=0.695 , generator loss=0.731\n",
      "Training progress in epoch #24, step 76, discriminator loss=0.689 , generator loss=0.742\n",
      "Training progress in epoch #24, step 77, discriminator loss=0.687 , generator loss=0.736\n",
      "Training progress in epoch #24, step 78, discriminator loss=0.687 , generator loss=0.754\n",
      "Training progress in epoch #24, step 79, discriminator loss=0.687 , generator loss=0.756\n",
      "Training progress in epoch #24, step 80, discriminator loss=0.682 , generator loss=0.762\n",
      "Training progress in epoch #24, step 81, discriminator loss=0.683 , generator loss=0.749\n",
      "Training progress in epoch #24, step 82, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #24, step 83, discriminator loss=0.680 , generator loss=0.685\n",
      "Training progress in epoch #24, step 84, discriminator loss=0.688 , generator loss=0.677\n",
      "Training progress in epoch #24, step 85, discriminator loss=0.686 , generator loss=0.667\n",
      "Training progress in epoch #24, step 86, discriminator loss=0.685 , generator loss=0.665\n",
      "Training progress in epoch #24, step 87, discriminator loss=0.687 , generator loss=0.679\n",
      "Training progress in epoch #24, step 88, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #24, step 89, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #24, step 90, discriminator loss=0.693 , generator loss=0.748\n",
      "Training progress in epoch #24, step 91, discriminator loss=0.697 , generator loss=0.758\n",
      "Training progress in epoch #24, step 92, discriminator loss=0.698 , generator loss=0.745\n",
      "Training progress in epoch #24, step 93, discriminator loss=0.688 , generator loss=0.758\n",
      "Training progress in epoch #24, step 94, discriminator loss=0.692 , generator loss=0.756\n",
      "Training progress in epoch #24, step 95, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #24, step 96, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #24, step 97, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #24, step 98, discriminator loss=0.680 , generator loss=0.672\n",
      "Training progress in epoch #24, step 99, discriminator loss=0.703 , generator loss=0.637\n",
      "Training progress in epoch #24, step 100, discriminator loss=0.687 , generator loss=0.639\n",
      "Training progress in epoch #24, step 101, discriminator loss=0.692 , generator loss=0.672\n",
      "Training progress in epoch #24, step 102, discriminator loss=0.678 , generator loss=0.696\n",
      "Training progress in epoch #24, step 103, discriminator loss=0.681 , generator loss=0.692\n",
      "Training progress in epoch #24, step 104, discriminator loss=0.684 , generator loss=0.727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #24, step 105, discriminator loss=0.692 , generator loss=0.768\n",
      "Training progress in epoch #24, step 106, discriminator loss=0.671 , generator loss=0.741\n",
      "Training progress in epoch #24, step 107, discriminator loss=0.680 , generator loss=0.729\n",
      "Training progress in epoch #24, step 108, discriminator loss=0.685 , generator loss=0.756\n",
      "Training progress in epoch #24, step 109, discriminator loss=0.686 , generator loss=0.782\n",
      "Training progress in epoch #24, step 110, discriminator loss=0.691 , generator loss=0.739\n",
      "Training progress in epoch #24, step 111, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #24, step 112, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #24, step 113, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #24, step 114, discriminator loss=0.703 , generator loss=0.657\n",
      "Training progress in epoch #24, step 115, discriminator loss=0.703 , generator loss=0.645\n",
      "Training progress in epoch #24, step 116, discriminator loss=0.697 , generator loss=0.666\n",
      "Training progress in epoch #24, step 117, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #24, step 118, discriminator loss=0.698 , generator loss=0.704\n",
      "Training progress in epoch #24, step 119, discriminator loss=0.687 , generator loss=0.731\n",
      "Training progress in epoch #24, step 120, discriminator loss=0.695 , generator loss=0.732\n",
      "Training progress in epoch #24, step 121, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #24, step 122, discriminator loss=0.693 , generator loss=0.758\n",
      "Training progress in epoch #24, step 123, discriminator loss=0.684 , generator loss=0.751\n",
      "Training progress in epoch #24, step 124, discriminator loss=0.689 , generator loss=0.763\n",
      "Training progress in epoch #24, step 125, discriminator loss=0.682 , generator loss=0.738\n",
      "Training progress in epoch #24, step 126, discriminator loss=0.684 , generator loss=0.750\n",
      "Training progress in epoch #24, step 127, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #24, step 128, discriminator loss=0.678 , generator loss=0.701\n",
      "Training progress in epoch #24, step 129, discriminator loss=0.685 , generator loss=0.683\n",
      "Training progress in epoch #24, step 130, discriminator loss=0.685 , generator loss=0.674\n",
      "Training progress in epoch #24, step 131, discriminator loss=0.676 , generator loss=0.663\n",
      "Training progress in epoch #24, step 132, discriminator loss=0.681 , generator loss=0.691\n",
      "Training progress in epoch #24, step 133, discriminator loss=0.678 , generator loss=0.715\n",
      "Training progress in epoch #24, step 134, discriminator loss=0.682 , generator loss=0.709\n",
      "Training progress in epoch #24, step 135, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #24, step 136, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #24, step 137, discriminator loss=0.694 , generator loss=0.758\n",
      "Training progress in epoch #24, step 138, discriminator loss=0.696 , generator loss=0.759\n",
      "Training progress in epoch #24, step 139, discriminator loss=0.688 , generator loss=0.738\n",
      "Training progress in epoch #24, step 140, discriminator loss=0.696 , generator loss=0.734\n",
      "Training progress in epoch #24, step 141, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #24, step 142, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #24, step 143, discriminator loss=0.696 , generator loss=0.679\n",
      "Training progress in epoch #24, step 144, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #24, step 145, discriminator loss=0.700 , generator loss=0.666\n",
      "Training progress in epoch #24, step 146, discriminator loss=0.688 , generator loss=0.675\n",
      "Training progress in epoch #24, step 147, discriminator loss=0.689 , generator loss=0.677\n",
      "Training progress in epoch #24, step 148, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #24, step 149, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #24, step 150, discriminator loss=0.689 , generator loss=0.737\n",
      "Training progress in epoch #24, step 151, discriminator loss=0.682 , generator loss=0.719\n",
      "Training progress in epoch #24, step 152, discriminator loss=0.686 , generator loss=0.739\n",
      "Training progress in epoch #24, step 153, discriminator loss=0.690 , generator loss=0.734\n",
      "Training progress in epoch #24, step 154, discriminator loss=0.680 , generator loss=0.722\n",
      "Training progress in epoch #24, step 155, discriminator loss=0.686 , generator loss=0.729\n",
      "Training progress in epoch #24, step 156, discriminator loss=0.695 , generator loss=0.727\n",
      "Training progress in epoch #24, step 157, discriminator loss=0.682 , generator loss=0.698\n",
      "Training progress in epoch #24, step 158, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #24, step 159, discriminator loss=0.685 , generator loss=0.663\n",
      "Training progress in epoch #24, step 160, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #24, step 161, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #24, step 162, discriminator loss=0.690 , generator loss=0.665\n",
      "Training progress in epoch #24, step 163, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #24, step 164, discriminator loss=0.693 , generator loss=0.730\n",
      "Training progress in epoch #24, step 165, discriminator loss=0.697 , generator loss=0.728\n",
      "Training progress in epoch #24, step 166, discriminator loss=0.693 , generator loss=0.731\n",
      "Training progress in epoch #24, step 167, discriminator loss=0.688 , generator loss=0.732\n",
      "Training progress in epoch #24, step 168, discriminator loss=0.694 , generator loss=0.752\n",
      "Training progress in epoch #24, step 169, discriminator loss=0.695 , generator loss=0.777\n",
      "Training progress in epoch #24, step 170, discriminator loss=0.691 , generator loss=0.785\n",
      "Training progress in epoch #24, step 171, discriminator loss=0.686 , generator loss=0.735\n",
      "Training progress in epoch #24, step 172, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #24, step 173, discriminator loss=0.687 , generator loss=0.648\n",
      "Training progress in epoch #24, step 174, discriminator loss=0.694 , generator loss=0.660\n",
      "Training progress in epoch #24, step 175, discriminator loss=0.685 , generator loss=0.655\n",
      "Training progress in epoch #24, step 176, discriminator loss=0.686 , generator loss=0.665\n",
      "Training progress in epoch #24, step 177, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #24, step 178, discriminator loss=0.683 , generator loss=0.719\n",
      "Training progress in epoch #24, step 179, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #24, step 180, discriminator loss=0.680 , generator loss=0.736\n",
      "Training progress in epoch #24, step 181, discriminator loss=0.678 , generator loss=0.731\n",
      "Training progress in epoch #24, step 182, discriminator loss=0.692 , generator loss=0.774\n",
      "Training progress in epoch #24, step 183, discriminator loss=0.693 , generator loss=0.757\n",
      "Training progress in epoch #24, step 184, discriminator loss=0.678 , generator loss=0.723\n",
      "Training progress in epoch #24, step 185, discriminator loss=0.686 , generator loss=0.754\n",
      "Training progress in epoch #24, step 186, discriminator loss=0.686 , generator loss=0.746\n",
      "Training progress in epoch #24, step 187, discriminator loss=0.696 , generator loss=0.694\n",
      "Training progress in epoch #24, step 188, discriminator loss=0.686 , generator loss=0.653\n",
      "Training progress in epoch #24, step 189, discriminator loss=0.695 , generator loss=0.659\n",
      "Training progress in epoch #24, step 190, discriminator loss=0.692 , generator loss=0.664\n",
      "Training progress in epoch #24, step 191, discriminator loss=0.697 , generator loss=0.656\n",
      "Training progress in epoch #24, step 192, discriminator loss=0.692 , generator loss=0.666\n",
      "Training progress in epoch #24, step 193, discriminator loss=0.697 , generator loss=0.703\n",
      "Training progress in epoch #24, step 194, discriminator loss=0.695 , generator loss=0.725\n",
      "Training progress in epoch #24, step 195, discriminator loss=0.700 , generator loss=0.736\n",
      "Training progress in epoch #24, step 196, discriminator loss=0.691 , generator loss=0.722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #24, step 197, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #24, step 198, discriminator loss=0.692 , generator loss=0.765\n",
      "Training progress in epoch #24, step 199, discriminator loss=0.676 , generator loss=0.764\n",
      "Training progress in epoch #24, step 200, discriminator loss=0.682 , generator loss=0.733\n",
      "Training progress in epoch #24, step 201, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #24, step 202, discriminator loss=0.681 , generator loss=0.691\n",
      "Training progress in epoch #24, step 203, discriminator loss=0.685 , generator loss=0.683\n",
      "Training progress in epoch #24, step 204, discriminator loss=0.682 , generator loss=0.688\n",
      "Training progress in epoch #24, step 205, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #24, step 206, discriminator loss=0.683 , generator loss=0.674\n",
      "Training progress in epoch #24, step 207, discriminator loss=0.683 , generator loss=0.676\n",
      "Training progress in epoch #24, step 208, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #24, step 209, discriminator loss=0.681 , generator loss=0.707\n",
      "Training progress in epoch #24, step 210, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #24, step 211, discriminator loss=0.697 , generator loss=0.762\n",
      "Training progress in epoch #24, step 212, discriminator loss=0.696 , generator loss=0.772\n",
      "Training progress in epoch #24, step 213, discriminator loss=0.693 , generator loss=0.749\n",
      "Training progress in epoch #24, step 214, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #24, step 215, discriminator loss=0.700 , generator loss=0.716\n",
      "Training progress in epoch #24, step 216, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #24, step 217, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #24, step 218, discriminator loss=0.695 , generator loss=0.673\n",
      "Training progress in epoch #24, step 219, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #24, step 220, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #24, step 221, discriminator loss=0.680 , generator loss=0.703\n",
      "Training progress in epoch #24, step 222, discriminator loss=0.680 , generator loss=0.706\n",
      "Training progress in epoch #24, step 223, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #24, step 224, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #24, step 225, discriminator loss=0.685 , generator loss=0.724\n",
      "Training progress in epoch #24, step 226, discriminator loss=0.682 , generator loss=0.716\n",
      "Training progress in epoch #24, step 227, discriminator loss=0.674 , generator loss=0.727\n",
      "Training progress in epoch #24, step 228, discriminator loss=0.691 , generator loss=0.743\n",
      "Training progress in epoch #24, step 229, discriminator loss=0.682 , generator loss=0.741\n",
      "Training progress in epoch #24, step 230, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #24, step 231, discriminator loss=0.685 , generator loss=0.724\n",
      "Training progress in epoch #24, step 232, discriminator loss=0.682 , generator loss=0.717\n",
      "Training progress in epoch #24, step 233, discriminator loss=0.695 , generator loss=0.717\n",
      "Disciminator Accuracy on real images: 40%, on fake images: 76%\n",
      "Training progress in epoch #25, step 0, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #25, step 1, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #25, step 2, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #25, step 3, discriminator loss=0.691 , generator loss=0.667\n",
      "Training progress in epoch #25, step 4, discriminator loss=0.696 , generator loss=0.672\n",
      "Training progress in epoch #25, step 5, discriminator loss=0.689 , generator loss=0.670\n",
      "Training progress in epoch #25, step 6, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #25, step 7, discriminator loss=0.700 , generator loss=0.740\n",
      "Training progress in epoch #25, step 8, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #25, step 9, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #25, step 10, discriminator loss=0.688 , generator loss=0.742\n",
      "Training progress in epoch #25, step 11, discriminator loss=0.689 , generator loss=0.762\n",
      "Training progress in epoch #25, step 12, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #25, step 13, discriminator loss=0.684 , generator loss=0.684\n",
      "Training progress in epoch #25, step 14, discriminator loss=0.690 , generator loss=0.674\n",
      "Training progress in epoch #25, step 15, discriminator loss=0.683 , generator loss=0.711\n",
      "Training progress in epoch #25, step 16, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #25, step 17, discriminator loss=0.680 , generator loss=0.688\n",
      "Training progress in epoch #25, step 18, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #25, step 19, discriminator loss=0.682 , generator loss=0.677\n",
      "Training progress in epoch #25, step 20, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #25, step 21, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #25, step 22, discriminator loss=0.694 , generator loss=0.721\n",
      "Training progress in epoch #25, step 23, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #25, step 24, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #25, step 25, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #25, step 26, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #25, step 27, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #25, step 28, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #25, step 29, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #25, step 30, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #25, step 31, discriminator loss=0.695 , generator loss=0.676\n",
      "Training progress in epoch #25, step 32, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #25, step 33, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #25, step 34, discriminator loss=0.684 , generator loss=0.734\n",
      "Training progress in epoch #25, step 35, discriminator loss=0.682 , generator loss=0.741\n",
      "Training progress in epoch #25, step 36, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #25, step 37, discriminator loss=0.688 , generator loss=0.746\n",
      "Training progress in epoch #25, step 38, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #25, step 39, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #25, step 40, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #25, step 41, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #25, step 42, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #25, step 43, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #25, step 44, discriminator loss=0.678 , generator loss=0.698\n",
      "Training progress in epoch #25, step 45, discriminator loss=0.684 , generator loss=0.687\n",
      "Training progress in epoch #25, step 46, discriminator loss=0.683 , generator loss=0.701\n",
      "Training progress in epoch #25, step 47, discriminator loss=0.679 , generator loss=0.717\n",
      "Training progress in epoch #25, step 48, discriminator loss=0.682 , generator loss=0.719\n",
      "Training progress in epoch #25, step 49, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #25, step 50, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #25, step 51, discriminator loss=0.685 , generator loss=0.730\n",
      "Training progress in epoch #25, step 52, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #25, step 53, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #25, step 54, discriminator loss=0.693 , generator loss=0.718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #25, step 55, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #25, step 56, discriminator loss=0.693 , generator loss=0.674\n",
      "Training progress in epoch #25, step 57, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #25, step 58, discriminator loss=0.692 , generator loss=0.676\n",
      "Training progress in epoch #25, step 59, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #25, step 60, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #25, step 61, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #25, step 62, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #25, step 63, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #25, step 64, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #25, step 65, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #25, step 66, discriminator loss=0.683 , generator loss=0.721\n",
      "Training progress in epoch #25, step 67, discriminator loss=0.683 , generator loss=0.715\n",
      "Training progress in epoch #25, step 68, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #25, step 69, discriminator loss=0.681 , generator loss=0.729\n",
      "Training progress in epoch #25, step 70, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #25, step 71, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #25, step 72, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #25, step 73, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #25, step 74, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #25, step 75, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #25, step 76, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #25, step 77, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #25, step 78, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #25, step 79, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #25, step 80, discriminator loss=0.696 , generator loss=0.748\n",
      "Training progress in epoch #25, step 81, discriminator loss=0.702 , generator loss=0.719\n",
      "Training progress in epoch #25, step 82, discriminator loss=0.696 , generator loss=0.685\n",
      "Training progress in epoch #25, step 83, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #25, step 84, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #25, step 85, discriminator loss=0.695 , generator loss=0.731\n",
      "Training progress in epoch #25, step 86, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #25, step 87, discriminator loss=0.684 , generator loss=0.710\n",
      "Training progress in epoch #25, step 88, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #25, step 89, discriminator loss=0.681 , generator loss=0.686\n",
      "Training progress in epoch #25, step 90, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #25, step 91, discriminator loss=0.682 , generator loss=0.725\n",
      "Training progress in epoch #25, step 92, discriminator loss=0.683 , generator loss=0.746\n",
      "Training progress in epoch #25, step 93, discriminator loss=0.684 , generator loss=0.755\n",
      "Training progress in epoch #25, step 94, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #25, step 95, discriminator loss=0.692 , generator loss=0.734\n",
      "Training progress in epoch #25, step 96, discriminator loss=0.688 , generator loss=0.752\n",
      "Training progress in epoch #25, step 97, discriminator loss=0.683 , generator loss=0.737\n",
      "Training progress in epoch #25, step 98, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #25, step 99, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #25, step 100, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #25, step 101, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #25, step 102, discriminator loss=0.694 , generator loss=0.668\n",
      "Training progress in epoch #25, step 103, discriminator loss=0.683 , generator loss=0.651\n",
      "Training progress in epoch #25, step 104, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #25, step 105, discriminator loss=0.687 , generator loss=0.731\n",
      "Training progress in epoch #25, step 106, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #25, step 107, discriminator loss=0.687 , generator loss=0.742\n",
      "Training progress in epoch #25, step 108, discriminator loss=0.691 , generator loss=0.727\n",
      "Training progress in epoch #25, step 109, discriminator loss=0.689 , generator loss=0.749\n",
      "Training progress in epoch #25, step 110, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #25, step 111, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #25, step 112, discriminator loss=0.676 , generator loss=0.709\n",
      "Training progress in epoch #25, step 113, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #25, step 114, discriminator loss=0.689 , generator loss=0.668\n",
      "Training progress in epoch #25, step 115, discriminator loss=0.685 , generator loss=0.681\n",
      "Training progress in epoch #25, step 116, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #25, step 117, discriminator loss=0.683 , generator loss=0.691\n",
      "Training progress in epoch #25, step 118, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #25, step 119, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #25, step 120, discriminator loss=0.685 , generator loss=0.725\n",
      "Training progress in epoch #25, step 121, discriminator loss=0.697 , generator loss=0.762\n",
      "Training progress in epoch #25, step 122, discriminator loss=0.690 , generator loss=0.742\n",
      "Training progress in epoch #25, step 123, discriminator loss=0.686 , generator loss=0.742\n",
      "Training progress in epoch #25, step 124, discriminator loss=0.685 , generator loss=0.738\n",
      "Training progress in epoch #25, step 125, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #25, step 126, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #25, step 127, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #25, step 128, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #25, step 129, discriminator loss=0.690 , generator loss=0.667\n",
      "Training progress in epoch #25, step 130, discriminator loss=0.688 , generator loss=0.671\n",
      "Training progress in epoch #25, step 131, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #25, step 132, discriminator loss=0.688 , generator loss=0.737\n",
      "Training progress in epoch #25, step 133, discriminator loss=0.683 , generator loss=0.751\n",
      "Training progress in epoch #25, step 134, discriminator loss=0.690 , generator loss=0.742\n",
      "Training progress in epoch #25, step 135, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #25, step 136, discriminator loss=0.687 , generator loss=0.720\n",
      "Training progress in epoch #25, step 137, discriminator loss=0.691 , generator loss=0.732\n",
      "Training progress in epoch #25, step 138, discriminator loss=0.684 , generator loss=0.731\n",
      "Training progress in epoch #25, step 139, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #25, step 140, discriminator loss=0.684 , generator loss=0.694\n",
      "Training progress in epoch #25, step 141, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #25, step 142, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #25, step 143, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #25, step 144, discriminator loss=0.685 , generator loss=0.760\n",
      "Training progress in epoch #25, step 145, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #25, step 146, discriminator loss=0.685 , generator loss=0.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #25, step 147, discriminator loss=0.684 , generator loss=0.688\n",
      "Training progress in epoch #25, step 148, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #25, step 149, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #25, step 150, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #25, step 151, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #25, step 152, discriminator loss=0.682 , generator loss=0.692\n",
      "Training progress in epoch #25, step 153, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #25, step 154, discriminator loss=0.683 , generator loss=0.733\n",
      "Training progress in epoch #25, step 155, discriminator loss=0.691 , generator loss=0.731\n",
      "Training progress in epoch #25, step 156, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #25, step 157, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #25, step 158, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #25, step 159, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #25, step 160, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #25, step 161, discriminator loss=0.697 , generator loss=0.699\n",
      "Training progress in epoch #25, step 162, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #25, step 163, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #25, step 164, discriminator loss=0.680 , generator loss=0.681\n",
      "Training progress in epoch #25, step 165, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #25, step 166, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #25, step 167, discriminator loss=0.681 , generator loss=0.724\n",
      "Training progress in epoch #25, step 168, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #25, step 169, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #25, step 170, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #25, step 171, discriminator loss=0.696 , generator loss=0.690\n",
      "Training progress in epoch #25, step 172, discriminator loss=0.679 , generator loss=0.686\n",
      "Training progress in epoch #25, step 173, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #25, step 174, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #25, step 175, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #25, step 176, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #25, step 177, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #25, step 178, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #25, step 179, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #25, step 180, discriminator loss=0.694 , generator loss=0.677\n",
      "Training progress in epoch #25, step 181, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #25, step 182, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #25, step 183, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #25, step 184, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #25, step 185, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #25, step 186, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #25, step 187, discriminator loss=0.685 , generator loss=0.744\n",
      "Training progress in epoch #25, step 188, discriminator loss=0.686 , generator loss=0.724\n",
      "Training progress in epoch #25, step 189, discriminator loss=0.682 , generator loss=0.737\n",
      "Training progress in epoch #25, step 190, discriminator loss=0.684 , generator loss=0.750\n",
      "Training progress in epoch #25, step 191, discriminator loss=0.680 , generator loss=0.728\n",
      "Training progress in epoch #25, step 192, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #25, step 193, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #25, step 194, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #25, step 195, discriminator loss=0.691 , generator loss=0.666\n",
      "Training progress in epoch #25, step 196, discriminator loss=0.702 , generator loss=0.651\n",
      "Training progress in epoch #25, step 197, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #25, step 198, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #25, step 199, discriminator loss=0.693 , generator loss=0.740\n",
      "Training progress in epoch #25, step 200, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #25, step 201, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #25, step 202, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #25, step 203, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #25, step 204, discriminator loss=0.693 , generator loss=0.746\n",
      "Training progress in epoch #25, step 205, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #25, step 206, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #25, step 207, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #25, step 208, discriminator loss=0.687 , generator loss=0.680\n",
      "Training progress in epoch #25, step 209, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #25, step 210, discriminator loss=0.677 , generator loss=0.703\n",
      "Training progress in epoch #25, step 211, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #25, step 212, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #25, step 213, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #25, step 214, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #25, step 215, discriminator loss=0.688 , generator loss=0.724\n",
      "Training progress in epoch #25, step 216, discriminator loss=0.683 , generator loss=0.707\n",
      "Training progress in epoch #25, step 217, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #25, step 218, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #25, step 219, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #25, step 220, discriminator loss=0.701 , generator loss=0.681\n",
      "Training progress in epoch #25, step 221, discriminator loss=0.697 , generator loss=0.688\n",
      "Training progress in epoch #25, step 222, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #25, step 223, discriminator loss=0.687 , generator loss=0.739\n",
      "Training progress in epoch #25, step 224, discriminator loss=0.685 , generator loss=0.728\n",
      "Training progress in epoch #25, step 225, discriminator loss=0.689 , generator loss=0.727\n",
      "Training progress in epoch #25, step 226, discriminator loss=0.683 , generator loss=0.733\n",
      "Training progress in epoch #25, step 227, discriminator loss=0.694 , generator loss=0.751\n",
      "Training progress in epoch #25, step 228, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #25, step 229, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #25, step 230, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #25, step 231, discriminator loss=0.697 , generator loss=0.669\n",
      "Training progress in epoch #25, step 232, discriminator loss=0.694 , generator loss=0.679\n",
      "Training progress in epoch #25, step 233, discriminator loss=0.691 , generator loss=0.675\n",
      "Disciminator Accuracy on real images: 89%, on fake images: 21%\n",
      "Training progress in epoch #26, step 0, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #26, step 1, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #26, step 2, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #26, step 3, discriminator loss=0.691 , generator loss=0.733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #26, step 4, discriminator loss=0.695 , generator loss=0.749\n",
      "Training progress in epoch #26, step 5, discriminator loss=0.687 , generator loss=0.737\n",
      "Training progress in epoch #26, step 6, discriminator loss=0.693 , generator loss=0.732\n",
      "Training progress in epoch #26, step 7, discriminator loss=0.683 , generator loss=0.711\n",
      "Training progress in epoch #26, step 8, discriminator loss=0.677 , generator loss=0.699\n",
      "Training progress in epoch #26, step 9, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #26, step 10, discriminator loss=0.683 , generator loss=0.699\n",
      "Training progress in epoch #26, step 11, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #26, step 12, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #26, step 13, discriminator loss=0.681 , generator loss=0.712\n",
      "Training progress in epoch #26, step 14, discriminator loss=0.686 , generator loss=0.737\n",
      "Training progress in epoch #26, step 15, discriminator loss=0.682 , generator loss=0.739\n",
      "Training progress in epoch #26, step 16, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #26, step 17, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #26, step 18, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #26, step 19, discriminator loss=0.696 , generator loss=0.729\n",
      "Training progress in epoch #26, step 20, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #26, step 21, discriminator loss=0.696 , generator loss=0.666\n",
      "Training progress in epoch #26, step 22, discriminator loss=0.692 , generator loss=0.655\n",
      "Training progress in epoch #26, step 23, discriminator loss=0.696 , generator loss=0.686\n",
      "Training progress in epoch #26, step 24, discriminator loss=0.697 , generator loss=0.704\n",
      "Training progress in epoch #26, step 25, discriminator loss=0.684 , generator loss=0.695\n",
      "Training progress in epoch #26, step 26, discriminator loss=0.684 , generator loss=0.716\n",
      "Training progress in epoch #26, step 27, discriminator loss=0.686 , generator loss=0.730\n",
      "Training progress in epoch #26, step 28, discriminator loss=0.692 , generator loss=0.751\n",
      "Training progress in epoch #26, step 29, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #26, step 30, discriminator loss=0.684 , generator loss=0.714\n",
      "Training progress in epoch #26, step 31, discriminator loss=0.677 , generator loss=0.714\n",
      "Training progress in epoch #26, step 32, discriminator loss=0.681 , generator loss=0.735\n",
      "Training progress in epoch #26, step 33, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #26, step 34, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #26, step 35, discriminator loss=0.687 , generator loss=0.676\n",
      "Training progress in epoch #26, step 36, discriminator loss=0.691 , generator loss=0.677\n",
      "Training progress in epoch #26, step 37, discriminator loss=0.695 , generator loss=0.677\n",
      "Training progress in epoch #26, step 38, discriminator loss=0.689 , generator loss=0.672\n",
      "Training progress in epoch #26, step 39, discriminator loss=0.685 , generator loss=0.685\n",
      "Training progress in epoch #26, step 40, discriminator loss=0.696 , generator loss=0.722\n",
      "Training progress in epoch #26, step 41, discriminator loss=0.699 , generator loss=0.735\n",
      "Training progress in epoch #26, step 42, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #26, step 43, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #26, step 44, discriminator loss=0.698 , generator loss=0.744\n",
      "Training progress in epoch #26, step 45, discriminator loss=0.695 , generator loss=0.726\n",
      "Training progress in epoch #26, step 46, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #26, step 47, discriminator loss=0.682 , generator loss=0.723\n",
      "Training progress in epoch #26, step 48, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #26, step 49, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #26, step 50, discriminator loss=0.691 , generator loss=0.661\n",
      "Training progress in epoch #26, step 51, discriminator loss=0.676 , generator loss=0.661\n",
      "Training progress in epoch #26, step 52, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #26, step 53, discriminator loss=0.681 , generator loss=0.708\n",
      "Training progress in epoch #26, step 54, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #26, step 55, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #26, step 56, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #26, step 57, discriminator loss=0.687 , generator loss=0.742\n",
      "Training progress in epoch #26, step 58, discriminator loss=0.690 , generator loss=0.749\n",
      "Training progress in epoch #26, step 59, discriminator loss=0.689 , generator loss=0.750\n",
      "Training progress in epoch #26, step 60, discriminator loss=0.684 , generator loss=0.750\n",
      "Training progress in epoch #26, step 61, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #26, step 62, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #26, step 63, discriminator loss=0.691 , generator loss=0.658\n",
      "Training progress in epoch #26, step 64, discriminator loss=0.696 , generator loss=0.665\n",
      "Training progress in epoch #26, step 65, discriminator loss=0.687 , generator loss=0.663\n",
      "Training progress in epoch #26, step 66, discriminator loss=0.684 , generator loss=0.687\n",
      "Training progress in epoch #26, step 67, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #26, step 68, discriminator loss=0.692 , generator loss=0.729\n",
      "Training progress in epoch #26, step 69, discriminator loss=0.683 , generator loss=0.723\n",
      "Training progress in epoch #26, step 70, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #26, step 71, discriminator loss=0.687 , generator loss=0.731\n",
      "Training progress in epoch #26, step 72, discriminator loss=0.685 , generator loss=0.755\n",
      "Training progress in epoch #26, step 73, discriminator loss=0.686 , generator loss=0.765\n",
      "Training progress in epoch #26, step 74, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #26, step 75, discriminator loss=0.686 , generator loss=0.678\n",
      "Training progress in epoch #26, step 76, discriminator loss=0.690 , generator loss=0.665\n",
      "Training progress in epoch #26, step 77, discriminator loss=0.689 , generator loss=0.670\n",
      "Training progress in epoch #26, step 78, discriminator loss=0.700 , generator loss=0.687\n",
      "Training progress in epoch #26, step 79, discriminator loss=0.693 , generator loss=0.677\n",
      "Training progress in epoch #26, step 80, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #26, step 81, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #26, step 82, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #26, step 83, discriminator loss=0.695 , generator loss=0.724\n",
      "Training progress in epoch #26, step 84, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #26, step 85, discriminator loss=0.689 , generator loss=0.764\n",
      "Training progress in epoch #26, step 86, discriminator loss=0.687 , generator loss=0.740\n",
      "Training progress in epoch #26, step 87, discriminator loss=0.682 , generator loss=0.749\n",
      "Training progress in epoch #26, step 88, discriminator loss=0.691 , generator loss=0.754\n",
      "Training progress in epoch #26, step 89, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #26, step 90, discriminator loss=0.698 , generator loss=0.658\n",
      "Training progress in epoch #26, step 91, discriminator loss=0.694 , generator loss=0.660\n",
      "Training progress in epoch #26, step 92, discriminator loss=0.687 , generator loss=0.676\n",
      "Training progress in epoch #26, step 93, discriminator loss=0.690 , generator loss=0.678\n",
      "Training progress in epoch #26, step 94, discriminator loss=0.696 , generator loss=0.689\n",
      "Training progress in epoch #26, step 95, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #26, step 96, discriminator loss=0.688 , generator loss=0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #26, step 97, discriminator loss=0.694 , generator loss=0.732\n",
      "Training progress in epoch #26, step 98, discriminator loss=0.691 , generator loss=0.739\n",
      "Training progress in epoch #26, step 99, discriminator loss=0.693 , generator loss=0.751\n",
      "Training progress in epoch #26, step 100, discriminator loss=0.689 , generator loss=0.765\n",
      "Training progress in epoch #26, step 101, discriminator loss=0.689 , generator loss=0.735\n",
      "Training progress in epoch #26, step 102, discriminator loss=0.690 , generator loss=0.752\n",
      "Training progress in epoch #26, step 103, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #26, step 104, discriminator loss=0.696 , generator loss=0.678\n",
      "Training progress in epoch #26, step 105, discriminator loss=0.690 , generator loss=0.673\n",
      "Training progress in epoch #26, step 106, discriminator loss=0.690 , generator loss=0.651\n",
      "Training progress in epoch #26, step 107, discriminator loss=0.680 , generator loss=0.659\n",
      "Training progress in epoch #26, step 108, discriminator loss=0.683 , generator loss=0.669\n",
      "Training progress in epoch #26, step 109, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #26, step 110, discriminator loss=0.682 , generator loss=0.722\n",
      "Training progress in epoch #26, step 111, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #26, step 112, discriminator loss=0.691 , generator loss=0.756\n",
      "Training progress in epoch #26, step 113, discriminator loss=0.690 , generator loss=0.766\n",
      "Training progress in epoch #26, step 114, discriminator loss=0.684 , generator loss=0.765\n",
      "Training progress in epoch #26, step 115, discriminator loss=0.685 , generator loss=0.732\n",
      "Training progress in epoch #26, step 116, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #26, step 117, discriminator loss=0.687 , generator loss=0.740\n",
      "Training progress in epoch #26, step 118, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #26, step 119, discriminator loss=0.690 , generator loss=0.662\n",
      "Training progress in epoch #26, step 120, discriminator loss=0.692 , generator loss=0.626\n",
      "Training progress in epoch #26, step 121, discriminator loss=0.694 , generator loss=0.654\n",
      "Training progress in epoch #26, step 122, discriminator loss=0.690 , generator loss=0.664\n",
      "Training progress in epoch #26, step 123, discriminator loss=0.692 , generator loss=0.674\n",
      "Training progress in epoch #26, step 124, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #26, step 125, discriminator loss=0.687 , generator loss=0.725\n",
      "Training progress in epoch #26, step 126, discriminator loss=0.693 , generator loss=0.743\n",
      "Training progress in epoch #26, step 127, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #26, step 128, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #26, step 129, discriminator loss=0.688 , generator loss=0.763\n",
      "Training progress in epoch #26, step 130, discriminator loss=0.693 , generator loss=0.753\n",
      "Training progress in epoch #26, step 131, discriminator loss=0.683 , generator loss=0.753\n",
      "Training progress in epoch #26, step 132, discriminator loss=0.685 , generator loss=0.738\n",
      "Training progress in epoch #26, step 133, discriminator loss=0.675 , generator loss=0.705\n",
      "Training progress in epoch #26, step 134, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #26, step 135, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #26, step 136, discriminator loss=0.692 , generator loss=0.668\n",
      "Training progress in epoch #26, step 137, discriminator loss=0.686 , generator loss=0.679\n",
      "Training progress in epoch #26, step 138, discriminator loss=0.679 , generator loss=0.695\n",
      "Training progress in epoch #26, step 139, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #26, step 140, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #26, step 141, discriminator loss=0.696 , generator loss=0.733\n",
      "Training progress in epoch #26, step 142, discriminator loss=0.695 , generator loss=0.764\n",
      "Training progress in epoch #26, step 143, discriminator loss=0.695 , generator loss=0.737\n",
      "Training progress in epoch #26, step 144, discriminator loss=0.682 , generator loss=0.722\n",
      "Training progress in epoch #26, step 145, discriminator loss=0.683 , generator loss=0.733\n",
      "Training progress in epoch #26, step 146, discriminator loss=0.694 , generator loss=0.751\n",
      "Training progress in epoch #26, step 147, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #26, step 148, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #26, step 149, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #26, step 150, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #26, step 151, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #26, step 152, discriminator loss=0.689 , generator loss=0.669\n",
      "Training progress in epoch #26, step 153, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #26, step 154, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #26, step 155, discriminator loss=0.682 , generator loss=0.728\n",
      "Training progress in epoch #26, step 156, discriminator loss=0.682 , generator loss=0.716\n",
      "Training progress in epoch #26, step 157, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #26, step 158, discriminator loss=0.693 , generator loss=0.747\n",
      "Training progress in epoch #26, step 159, discriminator loss=0.686 , generator loss=0.752\n",
      "Training progress in epoch #26, step 160, discriminator loss=0.696 , generator loss=0.741\n",
      "Training progress in epoch #26, step 161, discriminator loss=0.682 , generator loss=0.726\n",
      "Training progress in epoch #26, step 162, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #26, step 163, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #26, step 164, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #26, step 165, discriminator loss=0.697 , generator loss=0.665\n",
      "Training progress in epoch #26, step 166, discriminator loss=0.689 , generator loss=0.672\n",
      "Training progress in epoch #26, step 167, discriminator loss=0.683 , generator loss=0.678\n",
      "Training progress in epoch #26, step 168, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #26, step 169, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #26, step 170, discriminator loss=0.682 , generator loss=0.720\n",
      "Training progress in epoch #26, step 171, discriminator loss=0.686 , generator loss=0.727\n",
      "Training progress in epoch #26, step 172, discriminator loss=0.687 , generator loss=0.747\n",
      "Training progress in epoch #26, step 173, discriminator loss=0.687 , generator loss=0.755\n",
      "Training progress in epoch #26, step 174, discriminator loss=0.685 , generator loss=0.729\n",
      "Training progress in epoch #26, step 175, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #26, step 176, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #26, step 177, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #26, step 178, discriminator loss=0.691 , generator loss=0.677\n",
      "Training progress in epoch #26, step 179, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #26, step 180, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #26, step 181, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #26, step 182, discriminator loss=0.687 , generator loss=0.668\n",
      "Training progress in epoch #26, step 183, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #26, step 184, discriminator loss=0.699 , generator loss=0.737\n",
      "Training progress in epoch #26, step 185, discriminator loss=0.689 , generator loss=0.740\n",
      "Training progress in epoch #26, step 186, discriminator loss=0.696 , generator loss=0.746\n",
      "Training progress in epoch #26, step 187, discriminator loss=0.683 , generator loss=0.729\n",
      "Training progress in epoch #26, step 188, discriminator loss=0.685 , generator loss=0.737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #26, step 189, discriminator loss=0.686 , generator loss=0.729\n",
      "Training progress in epoch #26, step 190, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #26, step 191, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #26, step 192, discriminator loss=0.685 , generator loss=0.678\n",
      "Training progress in epoch #26, step 193, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #26, step 194, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #26, step 195, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #26, step 196, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #26, step 197, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #26, step 198, discriminator loss=0.696 , generator loss=0.719\n",
      "Training progress in epoch #26, step 199, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #26, step 200, discriminator loss=0.692 , generator loss=0.728\n",
      "Training progress in epoch #26, step 201, discriminator loss=0.698 , generator loss=0.741\n",
      "Training progress in epoch #26, step 202, discriminator loss=0.690 , generator loss=0.742\n",
      "Training progress in epoch #26, step 203, discriminator loss=0.702 , generator loss=0.718\n",
      "Training progress in epoch #26, step 204, discriminator loss=0.697 , generator loss=0.711\n",
      "Training progress in epoch #26, step 205, discriminator loss=0.697 , generator loss=0.676\n",
      "Training progress in epoch #26, step 206, discriminator loss=0.690 , generator loss=0.669\n",
      "Training progress in epoch #26, step 207, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #26, step 208, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #26, step 209, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #26, step 210, discriminator loss=0.683 , generator loss=0.725\n",
      "Training progress in epoch #26, step 211, discriminator loss=0.688 , generator loss=0.749\n",
      "Training progress in epoch #26, step 212, discriminator loss=0.679 , generator loss=0.722\n",
      "Training progress in epoch #26, step 213, discriminator loss=0.682 , generator loss=0.725\n",
      "Training progress in epoch #26, step 214, discriminator loss=0.687 , generator loss=0.745\n",
      "Training progress in epoch #26, step 215, discriminator loss=0.688 , generator loss=0.760\n",
      "Training progress in epoch #26, step 216, discriminator loss=0.687 , generator loss=0.726\n",
      "Training progress in epoch #26, step 217, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #26, step 218, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #26, step 219, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #26, step 220, discriminator loss=0.698 , generator loss=0.673\n",
      "Training progress in epoch #26, step 221, discriminator loss=0.695 , generator loss=0.672\n",
      "Training progress in epoch #26, step 222, discriminator loss=0.693 , generator loss=0.655\n",
      "Training progress in epoch #26, step 223, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #26, step 224, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #26, step 225, discriminator loss=0.693 , generator loss=0.731\n",
      "Training progress in epoch #26, step 226, discriminator loss=0.697 , generator loss=0.718\n",
      "Training progress in epoch #26, step 227, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #26, step 228, discriminator loss=0.693 , generator loss=0.736\n",
      "Training progress in epoch #26, step 229, discriminator loss=0.686 , generator loss=0.760\n",
      "Training progress in epoch #26, step 230, discriminator loss=0.689 , generator loss=0.737\n",
      "Training progress in epoch #26, step 231, discriminator loss=0.685 , generator loss=0.710\n",
      "Training progress in epoch #26, step 232, discriminator loss=0.686 , generator loss=0.683\n",
      "Training progress in epoch #26, step 233, discriminator loss=0.683 , generator loss=0.686\n",
      "Disciminator Accuracy on real images: 82%, on fake images: 27%\n",
      "Training progress in epoch #27, step 0, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #27, step 1, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #27, step 2, discriminator loss=0.678 , generator loss=0.677\n",
      "Training progress in epoch #27, step 3, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #27, step 4, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #27, step 5, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #27, step 6, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #27, step 7, discriminator loss=0.686 , generator loss=0.742\n",
      "Training progress in epoch #27, step 8, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #27, step 9, discriminator loss=0.694 , generator loss=0.724\n",
      "Training progress in epoch #27, step 10, discriminator loss=0.696 , generator loss=0.706\n",
      "Training progress in epoch #27, step 11, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #27, step 12, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #27, step 13, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #27, step 14, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #27, step 15, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #27, step 16, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #27, step 17, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #27, step 18, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #27, step 19, discriminator loss=0.682 , generator loss=0.697\n",
      "Training progress in epoch #27, step 20, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #27, step 21, discriminator loss=0.685 , generator loss=0.724\n",
      "Training progress in epoch #27, step 22, discriminator loss=0.690 , generator loss=0.741\n",
      "Training progress in epoch #27, step 23, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #27, step 24, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #27, step 25, discriminator loss=0.678 , generator loss=0.710\n",
      "Training progress in epoch #27, step 26, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #27, step 27, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #27, step 28, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #27, step 29, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #27, step 30, discriminator loss=0.684 , generator loss=0.702\n",
      "Training progress in epoch #27, step 31, discriminator loss=0.699 , generator loss=0.696\n",
      "Training progress in epoch #27, step 32, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #27, step 33, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #27, step 34, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #27, step 35, discriminator loss=0.680 , generator loss=0.716\n",
      "Training progress in epoch #27, step 36, discriminator loss=0.684 , generator loss=0.710\n",
      "Training progress in epoch #27, step 37, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #27, step 38, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #27, step 39, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #27, step 40, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #27, step 41, discriminator loss=0.684 , generator loss=0.712\n",
      "Training progress in epoch #27, step 42, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #27, step 43, discriminator loss=0.683 , generator loss=0.731\n",
      "Training progress in epoch #27, step 44, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #27, step 45, discriminator loss=0.687 , generator loss=0.690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #27, step 46, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #27, step 47, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #27, step 48, discriminator loss=0.682 , generator loss=0.693\n",
      "Training progress in epoch #27, step 49, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #27, step 50, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #27, step 51, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #27, step 52, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #27, step 53, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #27, step 54, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #27, step 55, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #27, step 56, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #27, step 57, discriminator loss=0.682 , generator loss=0.710\n",
      "Training progress in epoch #27, step 58, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #27, step 59, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #27, step 60, discriminator loss=0.687 , generator loss=0.670\n",
      "Training progress in epoch #27, step 61, discriminator loss=0.682 , generator loss=0.676\n",
      "Training progress in epoch #27, step 62, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #27, step 63, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #27, step 64, discriminator loss=0.682 , generator loss=0.725\n",
      "Training progress in epoch #27, step 65, discriminator loss=0.697 , generator loss=0.725\n",
      "Training progress in epoch #27, step 66, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #27, step 67, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #27, step 68, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #27, step 69, discriminator loss=0.684 , generator loss=0.701\n",
      "Training progress in epoch #27, step 70, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #27, step 71, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #27, step 72, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #27, step 73, discriminator loss=0.683 , generator loss=0.685\n",
      "Training progress in epoch #27, step 74, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #27, step 75, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #27, step 76, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #27, step 77, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #27, step 78, discriminator loss=0.682 , generator loss=0.732\n",
      "Training progress in epoch #27, step 79, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #27, step 80, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #27, step 81, discriminator loss=0.682 , generator loss=0.728\n",
      "Training progress in epoch #27, step 82, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #27, step 83, discriminator loss=0.689 , generator loss=0.673\n",
      "Training progress in epoch #27, step 84, discriminator loss=0.686 , generator loss=0.677\n",
      "Training progress in epoch #27, step 85, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #27, step 86, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #27, step 87, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #27, step 88, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #27, step 89, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #27, step 90, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #27, step 91, discriminator loss=0.685 , generator loss=0.723\n",
      "Training progress in epoch #27, step 92, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #27, step 93, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #27, step 94, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #27, step 95, discriminator loss=0.699 , generator loss=0.687\n",
      "Training progress in epoch #27, step 96, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #27, step 97, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #27, step 98, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #27, step 99, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #27, step 100, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #27, step 101, discriminator loss=0.686 , generator loss=0.730\n",
      "Training progress in epoch #27, step 102, discriminator loss=0.685 , generator loss=0.739\n",
      "Training progress in epoch #27, step 103, discriminator loss=0.681 , generator loss=0.744\n",
      "Training progress in epoch #27, step 104, discriminator loss=0.682 , generator loss=0.735\n",
      "Training progress in epoch #27, step 105, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #27, step 106, discriminator loss=0.684 , generator loss=0.687\n",
      "Training progress in epoch #27, step 107, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #27, step 108, discriminator loss=0.698 , generator loss=0.690\n",
      "Training progress in epoch #27, step 109, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #27, step 110, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #27, step 111, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #27, step 112, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #27, step 113, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #27, step 114, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #27, step 115, discriminator loss=0.697 , generator loss=0.713\n",
      "Training progress in epoch #27, step 116, discriminator loss=0.683 , generator loss=0.721\n",
      "Training progress in epoch #27, step 117, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #27, step 118, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #27, step 119, discriminator loss=0.686 , generator loss=0.717\n",
      "Training progress in epoch #27, step 120, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #27, step 121, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #27, step 122, discriminator loss=0.681 , generator loss=0.688\n",
      "Training progress in epoch #27, step 123, discriminator loss=0.689 , generator loss=0.668\n",
      "Training progress in epoch #27, step 124, discriminator loss=0.687 , generator loss=0.682\n",
      "Training progress in epoch #27, step 125, discriminator loss=0.684 , generator loss=0.714\n",
      "Training progress in epoch #27, step 126, discriminator loss=0.690 , generator loss=0.737\n",
      "Training progress in epoch #27, step 127, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #27, step 128, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #27, step 129, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #27, step 130, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #27, step 131, discriminator loss=0.683 , generator loss=0.716\n",
      "Training progress in epoch #27, step 132, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #27, step 133, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #27, step 134, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #27, step 135, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #27, step 136, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #27, step 137, discriminator loss=0.693 , generator loss=0.699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #27, step 138, discriminator loss=0.681 , generator loss=0.682\n",
      "Training progress in epoch #27, step 139, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #27, step 140, discriminator loss=0.686 , generator loss=0.740\n",
      "Training progress in epoch #27, step 141, discriminator loss=0.690 , generator loss=0.739\n",
      "Training progress in epoch #27, step 142, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #27, step 143, discriminator loss=0.685 , generator loss=0.742\n",
      "Training progress in epoch #27, step 144, discriminator loss=0.686 , generator loss=0.737\n",
      "Training progress in epoch #27, step 145, discriminator loss=0.686 , generator loss=0.735\n",
      "Training progress in epoch #27, step 146, discriminator loss=0.683 , generator loss=0.719\n",
      "Training progress in epoch #27, step 147, discriminator loss=0.683 , generator loss=0.702\n",
      "Training progress in epoch #27, step 148, discriminator loss=0.689 , generator loss=0.673\n",
      "Training progress in epoch #27, step 149, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #27, step 150, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #27, step 151, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #27, step 152, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #27, step 153, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #27, step 154, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #27, step 155, discriminator loss=0.689 , generator loss=0.727\n",
      "Training progress in epoch #27, step 156, discriminator loss=0.689 , generator loss=0.759\n",
      "Training progress in epoch #27, step 157, discriminator loss=0.685 , generator loss=0.762\n",
      "Training progress in epoch #27, step 158, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #27, step 159, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #27, step 160, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #27, step 161, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #27, step 162, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #27, step 163, discriminator loss=0.693 , generator loss=0.673\n",
      "Training progress in epoch #27, step 164, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #27, step 165, discriminator loss=0.687 , generator loss=0.674\n",
      "Training progress in epoch #27, step 166, discriminator loss=0.684 , generator loss=0.698\n",
      "Training progress in epoch #27, step 167, discriminator loss=0.689 , generator loss=0.737\n",
      "Training progress in epoch #27, step 168, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #27, step 169, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #27, step 170, discriminator loss=0.692 , generator loss=0.731\n",
      "Training progress in epoch #27, step 171, discriminator loss=0.689 , generator loss=0.743\n",
      "Training progress in epoch #27, step 172, discriminator loss=0.693 , generator loss=0.731\n",
      "Training progress in epoch #27, step 173, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #27, step 174, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #27, step 175, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #27, step 176, discriminator loss=0.697 , generator loss=0.672\n",
      "Training progress in epoch #27, step 177, discriminator loss=0.686 , generator loss=0.666\n",
      "Training progress in epoch #27, step 178, discriminator loss=0.688 , generator loss=0.677\n",
      "Training progress in epoch #27, step 179, discriminator loss=0.683 , generator loss=0.683\n",
      "Training progress in epoch #27, step 180, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #27, step 181, discriminator loss=0.691 , generator loss=0.723\n",
      "Training progress in epoch #27, step 182, discriminator loss=0.696 , generator loss=0.716\n",
      "Training progress in epoch #27, step 183, discriminator loss=0.681 , generator loss=0.741\n",
      "Training progress in epoch #27, step 184, discriminator loss=0.690 , generator loss=0.748\n",
      "Training progress in epoch #27, step 185, discriminator loss=0.688 , generator loss=0.733\n",
      "Training progress in epoch #27, step 186, discriminator loss=0.684 , generator loss=0.713\n",
      "Training progress in epoch #27, step 187, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #27, step 188, discriminator loss=0.687 , generator loss=0.682\n",
      "Training progress in epoch #27, step 189, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #27, step 190, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #27, step 191, discriminator loss=0.691 , generator loss=0.665\n",
      "Training progress in epoch #27, step 192, discriminator loss=0.684 , generator loss=0.688\n",
      "Training progress in epoch #27, step 193, discriminator loss=0.678 , generator loss=0.710\n",
      "Training progress in epoch #27, step 194, discriminator loss=0.694 , generator loss=0.736\n",
      "Training progress in epoch #27, step 195, discriminator loss=0.683 , generator loss=0.753\n",
      "Training progress in epoch #27, step 196, discriminator loss=0.690 , generator loss=0.732\n",
      "Training progress in epoch #27, step 197, discriminator loss=0.691 , generator loss=0.734\n",
      "Training progress in epoch #27, step 198, discriminator loss=0.697 , generator loss=0.740\n",
      "Training progress in epoch #27, step 199, discriminator loss=0.687 , generator loss=0.736\n",
      "Training progress in epoch #27, step 200, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #27, step 201, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #27, step 202, discriminator loss=0.685 , generator loss=0.683\n",
      "Training progress in epoch #27, step 203, discriminator loss=0.696 , generator loss=0.668\n",
      "Training progress in epoch #27, step 204, discriminator loss=0.688 , generator loss=0.678\n",
      "Training progress in epoch #27, step 205, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #27, step 206, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #27, step 207, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #27, step 208, discriminator loss=0.681 , generator loss=0.695\n",
      "Training progress in epoch #27, step 209, discriminator loss=0.687 , generator loss=0.750\n",
      "Training progress in epoch #27, step 210, discriminator loss=0.692 , generator loss=0.764\n",
      "Training progress in epoch #27, step 211, discriminator loss=0.683 , generator loss=0.774\n",
      "Training progress in epoch #27, step 212, discriminator loss=0.686 , generator loss=0.754\n",
      "Training progress in epoch #27, step 213, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #27, step 214, discriminator loss=0.681 , generator loss=0.716\n",
      "Training progress in epoch #27, step 215, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #27, step 216, discriminator loss=0.699 , generator loss=0.638\n",
      "Training progress in epoch #27, step 217, discriminator loss=0.687 , generator loss=0.637\n",
      "Training progress in epoch #27, step 218, discriminator loss=0.697 , generator loss=0.636\n",
      "Training progress in epoch #27, step 219, discriminator loss=0.697 , generator loss=0.673\n",
      "Training progress in epoch #27, step 220, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #27, step 221, discriminator loss=0.681 , generator loss=0.707\n",
      "Training progress in epoch #27, step 222, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #27, step 223, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #27, step 224, discriminator loss=0.694 , generator loss=0.763\n",
      "Training progress in epoch #27, step 225, discriminator loss=0.696 , generator loss=0.764\n",
      "Training progress in epoch #27, step 226, discriminator loss=0.686 , generator loss=0.747\n",
      "Training progress in epoch #27, step 227, discriminator loss=0.679 , generator loss=0.761\n",
      "Training progress in epoch #27, step 228, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #27, step 229, discriminator loss=0.685 , generator loss=0.663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #27, step 230, discriminator loss=0.690 , generator loss=0.661\n",
      "Training progress in epoch #27, step 231, discriminator loss=0.685 , generator loss=0.664\n",
      "Training progress in epoch #27, step 232, discriminator loss=0.683 , generator loss=0.704\n",
      "Training progress in epoch #27, step 233, discriminator loss=0.688 , generator loss=0.684\n",
      "Disciminator Accuracy on real images: 79%, on fake images: 53%\n",
      "Training progress in epoch #28, step 0, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #28, step 1, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #28, step 2, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #28, step 3, discriminator loss=0.681 , generator loss=0.730\n",
      "Training progress in epoch #28, step 4, discriminator loss=0.690 , generator loss=0.742\n",
      "Training progress in epoch #28, step 5, discriminator loss=0.693 , generator loss=0.742\n",
      "Training progress in epoch #28, step 6, discriminator loss=0.693 , generator loss=0.758\n",
      "Training progress in epoch #28, step 7, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #28, step 8, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #28, step 9, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #28, step 10, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #28, step 11, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #28, step 12, discriminator loss=0.699 , generator loss=0.676\n",
      "Training progress in epoch #28, step 13, discriminator loss=0.692 , generator loss=0.658\n",
      "Training progress in epoch #28, step 14, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #28, step 15, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #28, step 16, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #28, step 17, discriminator loss=0.682 , generator loss=0.739\n",
      "Training progress in epoch #28, step 18, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #28, step 19, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #28, step 20, discriminator loss=0.683 , generator loss=0.762\n",
      "Training progress in epoch #28, step 21, discriminator loss=0.686 , generator loss=0.775\n",
      "Training progress in epoch #28, step 22, discriminator loss=0.679 , generator loss=0.734\n",
      "Training progress in epoch #28, step 23, discriminator loss=0.680 , generator loss=0.705\n",
      "Training progress in epoch #28, step 24, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #28, step 25, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #28, step 26, discriminator loss=0.689 , generator loss=0.663\n",
      "Training progress in epoch #28, step 27, discriminator loss=0.687 , generator loss=0.653\n",
      "Training progress in epoch #28, step 28, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #28, step 29, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #28, step 30, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #28, step 31, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #28, step 32, discriminator loss=0.699 , generator loss=0.714\n",
      "Training progress in epoch #28, step 33, discriminator loss=0.689 , generator loss=0.754\n",
      "Training progress in epoch #28, step 34, discriminator loss=0.694 , generator loss=0.737\n",
      "Training progress in epoch #28, step 35, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #28, step 36, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #28, step 37, discriminator loss=0.691 , generator loss=0.676\n",
      "Training progress in epoch #28, step 38, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #28, step 39, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #28, step 40, discriminator loss=0.683 , generator loss=0.711\n",
      "Training progress in epoch #28, step 41, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #28, step 42, discriminator loss=0.684 , generator loss=0.673\n",
      "Training progress in epoch #28, step 43, discriminator loss=0.682 , generator loss=0.715\n",
      "Training progress in epoch #28, step 44, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #28, step 45, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #28, step 46, discriminator loss=0.682 , generator loss=0.726\n",
      "Training progress in epoch #28, step 47, discriminator loss=0.684 , generator loss=0.723\n",
      "Training progress in epoch #28, step 48, discriminator loss=0.683 , generator loss=0.718\n",
      "Training progress in epoch #28, step 49, discriminator loss=0.683 , generator loss=0.714\n",
      "Training progress in epoch #28, step 50, discriminator loss=0.675 , generator loss=0.704\n",
      "Training progress in epoch #28, step 51, discriminator loss=0.687 , generator loss=0.673\n",
      "Training progress in epoch #28, step 52, discriminator loss=0.688 , generator loss=0.677\n",
      "Training progress in epoch #28, step 53, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #28, step 54, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #28, step 55, discriminator loss=0.691 , generator loss=0.723\n",
      "Training progress in epoch #28, step 56, discriminator loss=0.695 , generator loss=0.731\n",
      "Training progress in epoch #28, step 57, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #28, step 58, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #28, step 59, discriminator loss=0.698 , generator loss=0.731\n",
      "Training progress in epoch #28, step 60, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #28, step 61, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #28, step 62, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #28, step 63, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #28, step 64, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #28, step 65, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #28, step 66, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #28, step 67, discriminator loss=0.685 , generator loss=0.714\n",
      "Training progress in epoch #28, step 68, discriminator loss=0.687 , generator loss=0.747\n",
      "Training progress in epoch #28, step 69, discriminator loss=0.678 , generator loss=0.725\n",
      "Training progress in epoch #28, step 70, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #28, step 71, discriminator loss=0.685 , generator loss=0.728\n",
      "Training progress in epoch #28, step 72, discriminator loss=0.679 , generator loss=0.722\n",
      "Training progress in epoch #28, step 73, discriminator loss=0.684 , generator loss=0.714\n",
      "Training progress in epoch #28, step 74, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #28, step 75, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #28, step 76, discriminator loss=0.681 , generator loss=0.712\n",
      "Training progress in epoch #28, step 77, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #28, step 78, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #28, step 79, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #28, step 80, discriminator loss=0.683 , generator loss=0.671\n",
      "Training progress in epoch #28, step 81, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #28, step 82, discriminator loss=0.693 , generator loss=0.730\n",
      "Training progress in epoch #28, step 83, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #28, step 84, discriminator loss=0.692 , generator loss=0.733\n",
      "Training progress in epoch #28, step 85, discriminator loss=0.694 , generator loss=0.731\n",
      "Training progress in epoch #28, step 86, discriminator loss=0.686 , generator loss=0.726\n",
      "Training progress in epoch #28, step 87, discriminator loss=0.688 , generator loss=0.714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #28, step 88, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #28, step 89, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #28, step 90, discriminator loss=0.686 , generator loss=0.671\n",
      "Training progress in epoch #28, step 91, discriminator loss=0.687 , generator loss=0.677\n",
      "Training progress in epoch #28, step 92, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #28, step 93, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #28, step 94, discriminator loss=0.683 , generator loss=0.689\n",
      "Training progress in epoch #28, step 95, discriminator loss=0.679 , generator loss=0.687\n",
      "Training progress in epoch #28, step 96, discriminator loss=0.684 , generator loss=0.702\n",
      "Training progress in epoch #28, step 97, discriminator loss=0.691 , generator loss=0.756\n",
      "Training progress in epoch #28, step 98, discriminator loss=0.684 , generator loss=0.746\n",
      "Training progress in epoch #28, step 99, discriminator loss=0.681 , generator loss=0.729\n",
      "Training progress in epoch #28, step 100, discriminator loss=0.692 , generator loss=0.741\n",
      "Training progress in epoch #28, step 101, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #28, step 102, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #28, step 103, discriminator loss=0.696 , generator loss=0.661\n",
      "Training progress in epoch #28, step 104, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #28, step 105, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #28, step 106, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #28, step 107, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #28, step 108, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #28, step 109, discriminator loss=0.690 , generator loss=0.732\n",
      "Training progress in epoch #28, step 110, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #28, step 111, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #28, step 112, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #28, step 113, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #28, step 114, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #28, step 115, discriminator loss=0.685 , generator loss=0.718\n",
      "Training progress in epoch #28, step 116, discriminator loss=0.684 , generator loss=0.720\n",
      "Training progress in epoch #28, step 117, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #28, step 118, discriminator loss=0.683 , generator loss=0.709\n",
      "Training progress in epoch #28, step 119, discriminator loss=0.692 , generator loss=0.734\n",
      "Training progress in epoch #28, step 120, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #28, step 121, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #28, step 122, discriminator loss=0.681 , generator loss=0.688\n",
      "Training progress in epoch #28, step 123, discriminator loss=0.684 , generator loss=0.726\n",
      "Training progress in epoch #28, step 124, discriminator loss=0.691 , generator loss=0.736\n",
      "Training progress in epoch #28, step 125, discriminator loss=0.685 , generator loss=0.730\n",
      "Training progress in epoch #28, step 126, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #28, step 127, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #28, step 128, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #28, step 129, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #28, step 130, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #28, step 131, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #28, step 132, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #28, step 133, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #28, step 134, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #28, step 135, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #28, step 136, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #28, step 137, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #28, step 138, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #28, step 139, discriminator loss=0.684 , generator loss=0.688\n",
      "Training progress in epoch #28, step 140, discriminator loss=0.681 , generator loss=0.685\n",
      "Training progress in epoch #28, step 141, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #28, step 142, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #28, step 143, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #28, step 144, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #28, step 145, discriminator loss=0.685 , generator loss=0.732\n",
      "Training progress in epoch #28, step 146, discriminator loss=0.694 , generator loss=0.721\n",
      "Training progress in epoch #28, step 147, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #28, step 148, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #28, step 149, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #28, step 150, discriminator loss=0.678 , generator loss=0.689\n",
      "Training progress in epoch #28, step 151, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #28, step 152, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #28, step 153, discriminator loss=0.695 , generator loss=0.715\n",
      "Training progress in epoch #28, step 154, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #28, step 155, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #28, step 156, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #28, step 157, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #28, step 158, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #28, step 159, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #28, step 160, discriminator loss=0.686 , generator loss=0.674\n",
      "Training progress in epoch #28, step 161, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #28, step 162, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #28, step 163, discriminator loss=0.686 , generator loss=0.730\n",
      "Training progress in epoch #28, step 164, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #28, step 165, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #28, step 166, discriminator loss=0.688 , generator loss=0.741\n",
      "Training progress in epoch #28, step 167, discriminator loss=0.685 , generator loss=0.741\n",
      "Training progress in epoch #28, step 168, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #28, step 169, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #28, step 170, discriminator loss=0.685 , generator loss=0.748\n",
      "Training progress in epoch #28, step 171, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #28, step 172, discriminator loss=0.686 , generator loss=0.684\n",
      "Training progress in epoch #28, step 173, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #28, step 174, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #28, step 175, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #28, step 176, discriminator loss=0.683 , generator loss=0.675\n",
      "Training progress in epoch #28, step 177, discriminator loss=0.683 , generator loss=0.680\n",
      "Training progress in epoch #28, step 178, discriminator loss=0.692 , generator loss=0.733\n",
      "Training progress in epoch #28, step 179, discriminator loss=0.690 , generator loss=0.750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #28, step 180, discriminator loss=0.685 , generator loss=0.724\n",
      "Training progress in epoch #28, step 181, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #28, step 182, discriminator loss=0.697 , generator loss=0.722\n",
      "Training progress in epoch #28, step 183, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #28, step 184, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #28, step 185, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #28, step 186, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #28, step 187, discriminator loss=0.686 , generator loss=0.682\n",
      "Training progress in epoch #28, step 188, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #28, step 189, discriminator loss=0.688 , generator loss=0.675\n",
      "Training progress in epoch #28, step 190, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #28, step 191, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #28, step 192, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #28, step 193, discriminator loss=0.683 , generator loss=0.711\n",
      "Training progress in epoch #28, step 194, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #28, step 195, discriminator loss=0.681 , generator loss=0.754\n",
      "Training progress in epoch #28, step 196, discriminator loss=0.683 , generator loss=0.736\n",
      "Training progress in epoch #28, step 197, discriminator loss=0.681 , generator loss=0.702\n",
      "Training progress in epoch #28, step 198, discriminator loss=0.684 , generator loss=0.709\n",
      "Training progress in epoch #28, step 199, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #28, step 200, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #28, step 201, discriminator loss=0.683 , generator loss=0.671\n",
      "Training progress in epoch #28, step 202, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #28, step 203, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #28, step 204, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #28, step 205, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #28, step 206, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #28, step 207, discriminator loss=0.698 , generator loss=0.744\n",
      "Training progress in epoch #28, step 208, discriminator loss=0.685 , generator loss=0.765\n",
      "Training progress in epoch #28, step 209, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #28, step 210, discriminator loss=0.685 , generator loss=0.720\n",
      "Training progress in epoch #28, step 211, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #28, step 212, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #28, step 213, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #28, step 214, discriminator loss=0.690 , generator loss=0.675\n",
      "Training progress in epoch #28, step 215, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #28, step 216, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #28, step 217, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #28, step 218, discriminator loss=0.696 , generator loss=0.730\n",
      "Training progress in epoch #28, step 219, discriminator loss=0.695 , generator loss=0.723\n",
      "Training progress in epoch #28, step 220, discriminator loss=0.690 , generator loss=0.736\n",
      "Training progress in epoch #28, step 221, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #28, step 222, discriminator loss=0.685 , generator loss=0.764\n",
      "Training progress in epoch #28, step 223, discriminator loss=0.695 , generator loss=0.748\n",
      "Training progress in epoch #28, step 224, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #28, step 225, discriminator loss=0.691 , generator loss=0.664\n",
      "Training progress in epoch #28, step 226, discriminator loss=0.688 , generator loss=0.659\n",
      "Training progress in epoch #28, step 227, discriminator loss=0.689 , generator loss=0.673\n",
      "Training progress in epoch #28, step 228, discriminator loss=0.682 , generator loss=0.692\n",
      "Training progress in epoch #28, step 229, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #28, step 230, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #28, step 231, discriminator loss=0.691 , generator loss=0.752\n",
      "Training progress in epoch #28, step 232, discriminator loss=0.695 , generator loss=0.733\n",
      "Training progress in epoch #28, step 233, discriminator loss=0.690 , generator loss=0.728\n",
      "Disciminator Accuracy on real images: 25%, on fake images: 92%\n",
      "Training progress in epoch #29, step 0, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #29, step 1, discriminator loss=0.694 , generator loss=0.735\n",
      "Training progress in epoch #29, step 2, discriminator loss=0.702 , generator loss=0.720\n",
      "Training progress in epoch #29, step 3, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #29, step 4, discriminator loss=0.694 , generator loss=0.660\n",
      "Training progress in epoch #29, step 5, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #29, step 6, discriminator loss=0.684 , generator loss=0.690\n",
      "Training progress in epoch #29, step 7, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #29, step 8, discriminator loss=0.699 , generator loss=0.690\n",
      "Training progress in epoch #29, step 9, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #29, step 10, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #29, step 11, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #29, step 12, discriminator loss=0.683 , generator loss=0.729\n",
      "Training progress in epoch #29, step 13, discriminator loss=0.692 , generator loss=0.736\n",
      "Training progress in epoch #29, step 14, discriminator loss=0.680 , generator loss=0.724\n",
      "Training progress in epoch #29, step 15, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #29, step 16, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #29, step 17, discriminator loss=0.691 , generator loss=0.734\n",
      "Training progress in epoch #29, step 18, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #29, step 19, discriminator loss=0.691 , generator loss=0.648\n",
      "Training progress in epoch #29, step 20, discriminator loss=0.685 , generator loss=0.666\n",
      "Training progress in epoch #29, step 21, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #29, step 22, discriminator loss=0.680 , generator loss=0.719\n",
      "Training progress in epoch #29, step 23, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #29, step 24, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #29, step 25, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #29, step 26, discriminator loss=0.685 , generator loss=0.731\n",
      "Training progress in epoch #29, step 27, discriminator loss=0.689 , generator loss=0.741\n",
      "Training progress in epoch #29, step 28, discriminator loss=0.688 , generator loss=0.746\n",
      "Training progress in epoch #29, step 29, discriminator loss=0.686 , generator loss=0.727\n",
      "Training progress in epoch #29, step 30, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #29, step 31, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #29, step 32, discriminator loss=0.681 , generator loss=0.682\n",
      "Training progress in epoch #29, step 33, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #29, step 34, discriminator loss=0.685 , generator loss=0.687\n",
      "Training progress in epoch #29, step 35, discriminator loss=0.688 , generator loss=0.682\n",
      "Training progress in epoch #29, step 36, discriminator loss=0.689 , generator loss=0.704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #29, step 37, discriminator loss=0.689 , generator loss=0.735\n",
      "Training progress in epoch #29, step 38, discriminator loss=0.691 , generator loss=0.737\n",
      "Training progress in epoch #29, step 39, discriminator loss=0.691 , generator loss=0.727\n",
      "Training progress in epoch #29, step 40, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #29, step 41, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #29, step 42, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #29, step 43, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #29, step 44, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #29, step 45, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #29, step 46, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #29, step 47, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #29, step 48, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #29, step 49, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #29, step 50, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #29, step 51, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #29, step 52, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #29, step 53, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #29, step 54, discriminator loss=0.694 , generator loss=0.740\n",
      "Training progress in epoch #29, step 55, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #29, step 56, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #29, step 57, discriminator loss=0.685 , generator loss=0.688\n",
      "Training progress in epoch #29, step 58, discriminator loss=0.699 , generator loss=0.699\n",
      "Training progress in epoch #29, step 59, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #29, step 60, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #29, step 61, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #29, step 62, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #29, step 63, discriminator loss=0.679 , generator loss=0.735\n",
      "Training progress in epoch #29, step 64, discriminator loss=0.685 , generator loss=0.733\n",
      "Training progress in epoch #29, step 65, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #29, step 66, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #29, step 67, discriminator loss=0.688 , generator loss=0.739\n",
      "Training progress in epoch #29, step 68, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #29, step 69, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #29, step 70, discriminator loss=0.695 , generator loss=0.665\n",
      "Training progress in epoch #29, step 71, discriminator loss=0.692 , generator loss=0.668\n",
      "Training progress in epoch #29, step 72, discriminator loss=0.685 , generator loss=0.692\n",
      "Training progress in epoch #29, step 73, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #29, step 74, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #29, step 75, discriminator loss=0.685 , generator loss=0.728\n",
      "Training progress in epoch #29, step 76, discriminator loss=0.686 , generator loss=0.727\n",
      "Training progress in epoch #29, step 77, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #29, step 78, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #29, step 79, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #29, step 80, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #29, step 81, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #29, step 82, discriminator loss=0.683 , generator loss=0.688\n",
      "Training progress in epoch #29, step 83, discriminator loss=0.686 , generator loss=0.671\n",
      "Training progress in epoch #29, step 84, discriminator loss=0.684 , generator loss=0.677\n",
      "Training progress in epoch #29, step 85, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #29, step 86, discriminator loss=0.684 , generator loss=0.741\n",
      "Training progress in epoch #29, step 87, discriminator loss=0.683 , generator loss=0.699\n",
      "Training progress in epoch #29, step 88, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #29, step 89, discriminator loss=0.692 , generator loss=0.743\n",
      "Training progress in epoch #29, step 90, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #29, step 91, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #29, step 92, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #29, step 93, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #29, step 94, discriminator loss=0.687 , generator loss=0.725\n",
      "Training progress in epoch #29, step 95, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #29, step 96, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #29, step 97, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #29, step 98, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #29, step 99, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #29, step 100, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #29, step 101, discriminator loss=0.682 , generator loss=0.695\n",
      "Training progress in epoch #29, step 102, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #29, step 103, discriminator loss=0.690 , generator loss=0.737\n",
      "Training progress in epoch #29, step 104, discriminator loss=0.687 , generator loss=0.727\n",
      "Training progress in epoch #29, step 105, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #29, step 106, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #29, step 107, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #29, step 108, discriminator loss=0.690 , generator loss=0.677\n",
      "Training progress in epoch #29, step 109, discriminator loss=0.693 , generator loss=0.676\n",
      "Training progress in epoch #29, step 110, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #29, step 111, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #29, step 112, discriminator loss=0.683 , generator loss=0.691\n",
      "Training progress in epoch #29, step 113, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #29, step 114, discriminator loss=0.695 , generator loss=0.730\n",
      "Training progress in epoch #29, step 115, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #29, step 116, discriminator loss=0.693 , generator loss=0.732\n",
      "Training progress in epoch #29, step 117, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #29, step 118, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #29, step 119, discriminator loss=0.691 , generator loss=0.731\n",
      "Training progress in epoch #29, step 120, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #29, step 121, discriminator loss=0.681 , generator loss=0.684\n",
      "Training progress in epoch #29, step 122, discriminator loss=0.681 , generator loss=0.669\n",
      "Training progress in epoch #29, step 123, discriminator loss=0.690 , generator loss=0.677\n",
      "Training progress in epoch #29, step 124, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #29, step 125, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #29, step 126, discriminator loss=0.697 , generator loss=0.739\n",
      "Training progress in epoch #29, step 127, discriminator loss=0.695 , generator loss=0.740\n",
      "Training progress in epoch #29, step 128, discriminator loss=0.693 , generator loss=0.715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #29, step 129, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #29, step 130, discriminator loss=0.686 , generator loss=0.746\n",
      "Training progress in epoch #29, step 131, discriminator loss=0.691 , generator loss=0.737\n",
      "Training progress in epoch #29, step 132, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #29, step 133, discriminator loss=0.686 , generator loss=0.669\n",
      "Training progress in epoch #29, step 134, discriminator loss=0.693 , generator loss=0.677\n",
      "Training progress in epoch #29, step 135, discriminator loss=0.689 , generator loss=0.678\n",
      "Training progress in epoch #29, step 136, discriminator loss=0.682 , generator loss=0.683\n",
      "Training progress in epoch #29, step 137, discriminator loss=0.679 , generator loss=0.694\n",
      "Training progress in epoch #29, step 138, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #29, step 139, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #29, step 140, discriminator loss=0.684 , generator loss=0.759\n",
      "Training progress in epoch #29, step 141, discriminator loss=0.695 , generator loss=0.769\n",
      "Training progress in epoch #29, step 142, discriminator loss=0.686 , generator loss=0.754\n",
      "Training progress in epoch #29, step 143, discriminator loss=0.683 , generator loss=0.748\n",
      "Training progress in epoch #29, step 144, discriminator loss=0.685 , generator loss=0.712\n",
      "Training progress in epoch #29, step 145, discriminator loss=0.686 , generator loss=0.677\n",
      "Training progress in epoch #29, step 146, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #29, step 147, discriminator loss=0.697 , generator loss=0.656\n",
      "Training progress in epoch #29, step 148, discriminator loss=0.695 , generator loss=0.661\n",
      "Training progress in epoch #29, step 149, discriminator loss=0.696 , generator loss=0.676\n",
      "Training progress in epoch #29, step 150, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #29, step 151, discriminator loss=0.682 , generator loss=0.716\n",
      "Training progress in epoch #29, step 152, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #29, step 153, discriminator loss=0.684 , generator loss=0.748\n",
      "Training progress in epoch #29, step 154, discriminator loss=0.685 , generator loss=0.760\n",
      "Training progress in epoch #29, step 155, discriminator loss=0.685 , generator loss=0.741\n",
      "Training progress in epoch #29, step 156, discriminator loss=0.688 , generator loss=0.733\n",
      "Training progress in epoch #29, step 157, discriminator loss=0.690 , generator loss=0.732\n",
      "Training progress in epoch #29, step 158, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #29, step 159, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #29, step 160, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #29, step 161, discriminator loss=0.691 , generator loss=0.664\n",
      "Training progress in epoch #29, step 162, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #29, step 163, discriminator loss=0.696 , generator loss=0.667\n",
      "Training progress in epoch #29, step 164, discriminator loss=0.690 , generator loss=0.674\n",
      "Training progress in epoch #29, step 165, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #29, step 166, discriminator loss=0.695 , generator loss=0.749\n",
      "Training progress in epoch #29, step 167, discriminator loss=0.694 , generator loss=0.752\n",
      "Training progress in epoch #29, step 168, discriminator loss=0.688 , generator loss=0.751\n",
      "Training progress in epoch #29, step 169, discriminator loss=0.691 , generator loss=0.754\n",
      "Training progress in epoch #29, step 170, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #29, step 171, discriminator loss=0.684 , generator loss=0.740\n",
      "Training progress in epoch #29, step 172, discriminator loss=0.680 , generator loss=0.712\n",
      "Training progress in epoch #29, step 173, discriminator loss=0.683 , generator loss=0.714\n",
      "Training progress in epoch #29, step 174, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #29, step 175, discriminator loss=0.688 , generator loss=0.660\n",
      "Training progress in epoch #29, step 176, discriminator loss=0.682 , generator loss=0.673\n",
      "Training progress in epoch #29, step 177, discriminator loss=0.679 , generator loss=0.691\n",
      "Training progress in epoch #29, step 178, discriminator loss=0.682 , generator loss=0.707\n",
      "Training progress in epoch #29, step 179, discriminator loss=0.689 , generator loss=0.727\n",
      "Training progress in epoch #29, step 180, discriminator loss=0.695 , generator loss=0.738\n",
      "Training progress in epoch #29, step 181, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #29, step 182, discriminator loss=0.685 , generator loss=0.735\n",
      "Training progress in epoch #29, step 183, discriminator loss=0.696 , generator loss=0.753\n",
      "Training progress in epoch #29, step 184, discriminator loss=0.689 , generator loss=0.742\n",
      "Training progress in epoch #29, step 185, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #29, step 186, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #29, step 187, discriminator loss=0.699 , generator loss=0.694\n",
      "Training progress in epoch #29, step 188, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #29, step 189, discriminator loss=0.692 , generator loss=0.668\n",
      "Training progress in epoch #29, step 190, discriminator loss=0.687 , generator loss=0.661\n",
      "Training progress in epoch #29, step 191, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #29, step 192, discriminator loss=0.683 , generator loss=0.730\n",
      "Training progress in epoch #29, step 193, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #29, step 194, discriminator loss=0.686 , generator loss=0.733\n",
      "Training progress in epoch #29, step 195, discriminator loss=0.687 , generator loss=0.735\n",
      "Training progress in epoch #29, step 196, discriminator loss=0.691 , generator loss=0.739\n",
      "Training progress in epoch #29, step 197, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #29, step 198, discriminator loss=0.685 , generator loss=0.736\n",
      "Training progress in epoch #29, step 199, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #29, step 200, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #29, step 201, discriminator loss=0.686 , generator loss=0.682\n",
      "Training progress in epoch #29, step 202, discriminator loss=0.686 , generator loss=0.664\n",
      "Training progress in epoch #29, step 203, discriminator loss=0.690 , generator loss=0.660\n",
      "Training progress in epoch #29, step 204, discriminator loss=0.689 , generator loss=0.665\n",
      "Training progress in epoch #29, step 205, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #29, step 206, discriminator loss=0.694 , generator loss=0.753\n",
      "Training progress in epoch #29, step 207, discriminator loss=0.686 , generator loss=0.759\n",
      "Training progress in epoch #29, step 208, discriminator loss=0.691 , generator loss=0.756\n",
      "Training progress in epoch #29, step 209, discriminator loss=0.683 , generator loss=0.746\n",
      "Training progress in epoch #29, step 210, discriminator loss=0.680 , generator loss=0.701\n",
      "Training progress in epoch #29, step 211, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #29, step 212, discriminator loss=0.699 , generator loss=0.739\n",
      "Training progress in epoch #29, step 213, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #29, step 214, discriminator loss=0.694 , generator loss=0.671\n",
      "Training progress in epoch #29, step 215, discriminator loss=0.693 , generator loss=0.677\n",
      "Training progress in epoch #29, step 216, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #29, step 217, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #29, step 218, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #29, step 219, discriminator loss=0.694 , generator loss=0.726\n",
      "Training progress in epoch #29, step 220, discriminator loss=0.689 , generator loss=0.741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #29, step 221, discriminator loss=0.691 , generator loss=0.760\n",
      "Training progress in epoch #29, step 222, discriminator loss=0.687 , generator loss=0.754\n",
      "Training progress in epoch #29, step 223, discriminator loss=0.683 , generator loss=0.720\n",
      "Training progress in epoch #29, step 224, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #29, step 225, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #29, step 226, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #29, step 227, discriminator loss=0.684 , generator loss=0.684\n",
      "Training progress in epoch #29, step 228, discriminator loss=0.684 , generator loss=0.690\n",
      "Training progress in epoch #29, step 229, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #29, step 230, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #29, step 231, discriminator loss=0.684 , generator loss=0.713\n",
      "Training progress in epoch #29, step 232, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #29, step 233, discriminator loss=0.684 , generator loss=0.710\n",
      "Disciminator Accuracy on real images: 56%, on fake images: 56%\n",
      "Training progress in epoch #30, step 0, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #30, step 1, discriminator loss=0.694 , generator loss=0.730\n",
      "Training progress in epoch #30, step 2, discriminator loss=0.692 , generator loss=0.734\n",
      "Training progress in epoch #30, step 3, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #30, step 4, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #30, step 5, discriminator loss=0.690 , generator loss=0.674\n",
      "Training progress in epoch #30, step 6, discriminator loss=0.692 , generator loss=0.673\n",
      "Training progress in epoch #30, step 7, discriminator loss=0.685 , generator loss=0.691\n",
      "Training progress in epoch #30, step 8, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #30, step 9, discriminator loss=0.683 , generator loss=0.715\n",
      "Training progress in epoch #30, step 10, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #30, step 11, discriminator loss=0.685 , generator loss=0.734\n",
      "Training progress in epoch #30, step 12, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #30, step 13, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #30, step 14, discriminator loss=0.684 , generator loss=0.735\n",
      "Training progress in epoch #30, step 15, discriminator loss=0.680 , generator loss=0.721\n",
      "Training progress in epoch #30, step 16, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #30, step 17, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #30, step 18, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #30, step 19, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #30, step 20, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #30, step 21, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #30, step 22, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #30, step 23, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #30, step 24, discriminator loss=0.696 , generator loss=0.721\n",
      "Training progress in epoch #30, step 25, discriminator loss=0.697 , generator loss=0.732\n",
      "Training progress in epoch #30, step 26, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #30, step 27, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #30, step 28, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #30, step 29, discriminator loss=0.694 , generator loss=0.728\n",
      "Training progress in epoch #30, step 30, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #30, step 31, discriminator loss=0.683 , generator loss=0.694\n",
      "Training progress in epoch #30, step 32, discriminator loss=0.682 , generator loss=0.688\n",
      "Training progress in epoch #30, step 33, discriminator loss=0.683 , generator loss=0.711\n",
      "Training progress in epoch #30, step 34, discriminator loss=0.683 , generator loss=0.713\n",
      "Training progress in epoch #30, step 35, discriminator loss=0.679 , generator loss=0.706\n",
      "Training progress in epoch #30, step 36, discriminator loss=0.681 , generator loss=0.711\n",
      "Training progress in epoch #30, step 37, discriminator loss=0.676 , generator loss=0.740\n",
      "Training progress in epoch #30, step 38, discriminator loss=0.685 , generator loss=0.725\n",
      "Training progress in epoch #30, step 39, discriminator loss=0.684 , generator loss=0.735\n",
      "Training progress in epoch #30, step 40, discriminator loss=0.684 , generator loss=0.731\n",
      "Training progress in epoch #30, step 41, discriminator loss=0.691 , generator loss=0.737\n",
      "Training progress in epoch #30, step 42, discriminator loss=0.696 , generator loss=0.719\n",
      "Training progress in epoch #30, step 43, discriminator loss=0.698 , generator loss=0.678\n",
      "Training progress in epoch #30, step 44, discriminator loss=0.694 , generator loss=0.652\n",
      "Training progress in epoch #30, step 45, discriminator loss=0.692 , generator loss=0.643\n",
      "Training progress in epoch #30, step 46, discriminator loss=0.692 , generator loss=0.671\n",
      "Training progress in epoch #30, step 47, discriminator loss=0.699 , generator loss=0.676\n",
      "Training progress in epoch #30, step 48, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #30, step 49, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #30, step 50, discriminator loss=0.694 , generator loss=0.742\n",
      "Training progress in epoch #30, step 51, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #30, step 52, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #30, step 53, discriminator loss=0.689 , generator loss=0.749\n",
      "Training progress in epoch #30, step 54, discriminator loss=0.686 , generator loss=0.793\n",
      "Training progress in epoch #30, step 55, discriminator loss=0.680 , generator loss=0.767\n",
      "Training progress in epoch #30, step 56, discriminator loss=0.681 , generator loss=0.728\n",
      "Training progress in epoch #30, step 57, discriminator loss=0.675 , generator loss=0.704\n",
      "Training progress in epoch #30, step 58, discriminator loss=0.683 , generator loss=0.680\n",
      "Training progress in epoch #30, step 59, discriminator loss=0.683 , generator loss=0.661\n",
      "Training progress in epoch #30, step 60, discriminator loss=0.687 , generator loss=0.669\n",
      "Training progress in epoch #30, step 61, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #30, step 62, discriminator loss=0.684 , generator loss=0.681\n",
      "Training progress in epoch #30, step 63, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #30, step 64, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #30, step 65, discriminator loss=0.703 , generator loss=0.723\n",
      "Training progress in epoch #30, step 66, discriminator loss=0.694 , generator loss=0.749\n",
      "Training progress in epoch #30, step 67, discriminator loss=0.697 , generator loss=0.774\n",
      "Training progress in epoch #30, step 68, discriminator loss=0.690 , generator loss=0.755\n",
      "Training progress in epoch #30, step 69, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #30, step 70, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #30, step 71, discriminator loss=0.685 , generator loss=0.731\n",
      "Training progress in epoch #30, step 72, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #30, step 73, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #30, step 74, discriminator loss=0.688 , generator loss=0.668\n",
      "Training progress in epoch #30, step 75, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #30, step 76, discriminator loss=0.684 , generator loss=0.693\n",
      "Training progress in epoch #30, step 77, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #30, step 78, discriminator loss=0.679 , generator loss=0.695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #30, step 79, discriminator loss=0.681 , generator loss=0.733\n",
      "Training progress in epoch #30, step 80, discriminator loss=0.691 , generator loss=0.739\n",
      "Training progress in epoch #30, step 81, discriminator loss=0.684 , generator loss=0.763\n",
      "Training progress in epoch #30, step 82, discriminator loss=0.686 , generator loss=0.755\n",
      "Training progress in epoch #30, step 83, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #30, step 84, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #30, step 85, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #30, step 86, discriminator loss=0.693 , generator loss=0.728\n",
      "Training progress in epoch #30, step 87, discriminator loss=0.701 , generator loss=0.696\n",
      "Training progress in epoch #30, step 88, discriminator loss=0.700 , generator loss=0.670\n",
      "Training progress in epoch #30, step 89, discriminator loss=0.687 , generator loss=0.673\n",
      "Training progress in epoch #30, step 90, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #30, step 91, discriminator loss=0.697 , generator loss=0.673\n",
      "Training progress in epoch #30, step 92, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #30, step 93, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #30, step 94, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #30, step 95, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #30, step 96, discriminator loss=0.687 , generator loss=0.749\n",
      "Training progress in epoch #30, step 97, discriminator loss=0.691 , generator loss=0.756\n",
      "Training progress in epoch #30, step 98, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #30, step 99, discriminator loss=0.683 , generator loss=0.722\n",
      "Training progress in epoch #30, step 100, discriminator loss=0.680 , generator loss=0.726\n",
      "Training progress in epoch #30, step 101, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #30, step 102, discriminator loss=0.696 , generator loss=0.668\n",
      "Training progress in epoch #30, step 103, discriminator loss=0.694 , generator loss=0.658\n",
      "Training progress in epoch #30, step 104, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #30, step 105, discriminator loss=0.683 , generator loss=0.694\n",
      "Training progress in epoch #30, step 106, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #30, step 107, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #30, step 108, discriminator loss=0.699 , generator loss=0.707\n",
      "Training progress in epoch #30, step 109, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #30, step 110, discriminator loss=0.695 , generator loss=0.736\n",
      "Training progress in epoch #30, step 111, discriminator loss=0.694 , generator loss=0.736\n",
      "Training progress in epoch #30, step 112, discriminator loss=0.686 , generator loss=0.734\n",
      "Training progress in epoch #30, step 113, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #30, step 114, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #30, step 115, discriminator loss=0.685 , generator loss=0.714\n",
      "Training progress in epoch #30, step 116, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #30, step 117, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #30, step 118, discriminator loss=0.686 , generator loss=0.675\n",
      "Training progress in epoch #30, step 119, discriminator loss=0.690 , generator loss=0.674\n",
      "Training progress in epoch #30, step 120, discriminator loss=0.684 , generator loss=0.690\n",
      "Training progress in epoch #30, step 121, discriminator loss=0.683 , generator loss=0.728\n",
      "Training progress in epoch #30, step 122, discriminator loss=0.684 , generator loss=0.728\n",
      "Training progress in epoch #30, step 123, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #30, step 124, discriminator loss=0.697 , generator loss=0.736\n",
      "Training progress in epoch #30, step 125, discriminator loss=0.695 , generator loss=0.775\n",
      "Training progress in epoch #30, step 126, discriminator loss=0.694 , generator loss=0.754\n",
      "Training progress in epoch #30, step 127, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #30, step 128, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #30, step 129, discriminator loss=0.691 , generator loss=0.674\n",
      "Training progress in epoch #30, step 130, discriminator loss=0.689 , generator loss=0.667\n",
      "Training progress in epoch #30, step 131, discriminator loss=0.696 , generator loss=0.668\n",
      "Training progress in epoch #30, step 132, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #30, step 133, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #30, step 134, discriminator loss=0.683 , generator loss=0.699\n",
      "Training progress in epoch #30, step 135, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #30, step 136, discriminator loss=0.685 , generator loss=0.721\n",
      "Training progress in epoch #30, step 137, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #30, step 138, discriminator loss=0.680 , generator loss=0.738\n",
      "Training progress in epoch #30, step 139, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #30, step 140, discriminator loss=0.688 , generator loss=0.743\n",
      "Training progress in epoch #30, step 141, discriminator loss=0.679 , generator loss=0.721\n",
      "Training progress in epoch #30, step 142, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #30, step 143, discriminator loss=0.691 , generator loss=0.667\n",
      "Training progress in epoch #30, step 144, discriminator loss=0.693 , generator loss=0.674\n",
      "Training progress in epoch #30, step 145, discriminator loss=0.697 , generator loss=0.674\n",
      "Training progress in epoch #30, step 146, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #30, step 147, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #30, step 148, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #30, step 149, discriminator loss=0.698 , generator loss=0.711\n",
      "Training progress in epoch #30, step 150, discriminator loss=0.695 , generator loss=0.748\n",
      "Training progress in epoch #30, step 151, discriminator loss=0.694 , generator loss=0.743\n",
      "Training progress in epoch #30, step 152, discriminator loss=0.683 , generator loss=0.716\n",
      "Training progress in epoch #30, step 153, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #30, step 154, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #30, step 155, discriminator loss=0.684 , generator loss=0.736\n",
      "Training progress in epoch #30, step 156, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #30, step 157, discriminator loss=0.678 , generator loss=0.705\n",
      "Training progress in epoch #30, step 158, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #30, step 159, discriminator loss=0.684 , generator loss=0.685\n",
      "Training progress in epoch #30, step 160, discriminator loss=0.684 , generator loss=0.687\n",
      "Training progress in epoch #30, step 161, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #30, step 162, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #30, step 163, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #30, step 164, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #30, step 165, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #30, step 166, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #30, step 167, discriminator loss=0.693 , generator loss=0.761\n",
      "Training progress in epoch #30, step 168, discriminator loss=0.695 , generator loss=0.724\n",
      "Training progress in epoch #30, step 169, discriminator loss=0.685 , generator loss=0.684\n",
      "Training progress in epoch #30, step 170, discriminator loss=0.683 , generator loss=0.691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #30, step 171, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #30, step 172, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #30, step 173, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #30, step 174, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #30, step 175, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #30, step 176, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #30, step 177, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #30, step 178, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #30, step 179, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #30, step 180, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #30, step 181, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #30, step 182, discriminator loss=0.688 , generator loss=0.735\n",
      "Training progress in epoch #30, step 183, discriminator loss=0.686 , generator loss=0.724\n",
      "Training progress in epoch #30, step 184, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #30, step 185, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #30, step 186, discriminator loss=0.690 , generator loss=0.671\n",
      "Training progress in epoch #30, step 187, discriminator loss=0.689 , generator loss=0.678\n",
      "Training progress in epoch #30, step 188, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #30, step 189, discriminator loss=0.687 , generator loss=0.730\n",
      "Training progress in epoch #30, step 190, discriminator loss=0.693 , generator loss=0.732\n",
      "Training progress in epoch #30, step 191, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #30, step 192, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #30, step 193, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #30, step 194, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #30, step 195, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #30, step 196, discriminator loss=0.686 , generator loss=0.728\n",
      "Training progress in epoch #30, step 197, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #30, step 198, discriminator loss=0.686 , generator loss=0.685\n",
      "Training progress in epoch #30, step 199, discriminator loss=0.684 , generator loss=0.684\n",
      "Training progress in epoch #30, step 200, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #30, step 201, discriminator loss=0.694 , generator loss=0.739\n",
      "Training progress in epoch #30, step 202, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #30, step 203, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #30, step 204, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #30, step 205, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #30, step 206, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #30, step 207, discriminator loss=0.682 , generator loss=0.681\n",
      "Training progress in epoch #30, step 208, discriminator loss=0.684 , generator loss=0.705\n",
      "Training progress in epoch #30, step 209, discriminator loss=0.696 , generator loss=0.707\n",
      "Training progress in epoch #30, step 210, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #30, step 211, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #30, step 212, discriminator loss=0.687 , generator loss=0.731\n",
      "Training progress in epoch #30, step 213, discriminator loss=0.686 , generator loss=0.732\n",
      "Training progress in epoch #30, step 214, discriminator loss=0.683 , generator loss=0.743\n",
      "Training progress in epoch #30, step 215, discriminator loss=0.686 , generator loss=0.724\n",
      "Training progress in epoch #30, step 216, discriminator loss=0.695 , generator loss=0.672\n",
      "Training progress in epoch #30, step 217, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #30, step 218, discriminator loss=0.681 , generator loss=0.704\n",
      "Training progress in epoch #30, step 219, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #30, step 220, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #30, step 221, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #30, step 222, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #30, step 223, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #30, step 224, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #30, step 225, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #30, step 226, discriminator loss=0.691 , generator loss=0.738\n",
      "Training progress in epoch #30, step 227, discriminator loss=0.695 , generator loss=0.724\n",
      "Training progress in epoch #30, step 228, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #30, step 229, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #30, step 230, discriminator loss=0.686 , generator loss=0.668\n",
      "Training progress in epoch #30, step 231, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #30, step 232, discriminator loss=0.687 , generator loss=0.726\n",
      "Training progress in epoch #30, step 233, discriminator loss=0.688 , generator loss=0.719\n",
      "Disciminator Accuracy on real images: 70%, on fake images: 75%\n",
      "Training progress in epoch #31, step 0, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #31, step 1, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #31, step 2, discriminator loss=0.684 , generator loss=0.700\n",
      "Training progress in epoch #31, step 3, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #31, step 4, discriminator loss=0.684 , generator loss=0.720\n",
      "Training progress in epoch #31, step 5, discriminator loss=0.692 , generator loss=0.729\n",
      "Training progress in epoch #31, step 6, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #31, step 7, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #31, step 8, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #31, step 9, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #31, step 10, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #31, step 11, discriminator loss=0.683 , generator loss=0.713\n",
      "Training progress in epoch #31, step 12, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #31, step 13, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #31, step 14, discriminator loss=0.693 , generator loss=0.748\n",
      "Training progress in epoch #31, step 15, discriminator loss=0.688 , generator loss=0.752\n",
      "Training progress in epoch #31, step 16, discriminator loss=0.683 , generator loss=0.703\n",
      "Training progress in epoch #31, step 17, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #31, step 18, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #31, step 19, discriminator loss=0.682 , generator loss=0.697\n",
      "Training progress in epoch #31, step 20, discriminator loss=0.691 , generator loss=0.677\n",
      "Training progress in epoch #31, step 21, discriminator loss=0.681 , generator loss=0.688\n",
      "Training progress in epoch #31, step 22, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #31, step 23, discriminator loss=0.681 , generator loss=0.713\n",
      "Training progress in epoch #31, step 24, discriminator loss=0.678 , generator loss=0.700\n",
      "Training progress in epoch #31, step 25, discriminator loss=0.685 , generator loss=0.718\n",
      "Training progress in epoch #31, step 26, discriminator loss=0.686 , generator loss=0.727\n",
      "Training progress in epoch #31, step 27, discriminator loss=0.686 , generator loss=0.730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #31, step 28, discriminator loss=0.694 , generator loss=0.728\n",
      "Training progress in epoch #31, step 29, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #31, step 30, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #31, step 31, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #31, step 32, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #31, step 33, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #31, step 34, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #31, step 35, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #31, step 36, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #31, step 37, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #31, step 38, discriminator loss=0.689 , generator loss=0.744\n",
      "Training progress in epoch #31, step 39, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #31, step 40, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #31, step 41, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #31, step 42, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #31, step 43, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #31, step 44, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #31, step 45, discriminator loss=0.682 , generator loss=0.727\n",
      "Training progress in epoch #31, step 46, discriminator loss=0.684 , generator loss=0.720\n",
      "Training progress in epoch #31, step 47, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #31, step 48, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #31, step 49, discriminator loss=0.681 , generator loss=0.686\n",
      "Training progress in epoch #31, step 50, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #31, step 51, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #31, step 52, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #31, step 53, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #31, step 54, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #31, step 55, discriminator loss=0.683 , generator loss=0.726\n",
      "Training progress in epoch #31, step 56, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #31, step 57, discriminator loss=0.699 , generator loss=0.704\n",
      "Training progress in epoch #31, step 58, discriminator loss=0.691 , generator loss=0.675\n",
      "Training progress in epoch #31, step 59, discriminator loss=0.696 , generator loss=0.680\n",
      "Training progress in epoch #31, step 60, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #31, step 61, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #31, step 62, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #31, step 63, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #31, step 64, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #31, step 65, discriminator loss=0.683 , generator loss=0.728\n",
      "Training progress in epoch #31, step 66, discriminator loss=0.679 , generator loss=0.740\n",
      "Training progress in epoch #31, step 67, discriminator loss=0.685 , generator loss=0.732\n",
      "Training progress in epoch #31, step 68, discriminator loss=0.684 , generator loss=0.696\n",
      "Training progress in epoch #31, step 69, discriminator loss=0.688 , generator loss=0.673\n",
      "Training progress in epoch #31, step 70, discriminator loss=0.685 , generator loss=0.701\n",
      "Training progress in epoch #31, step 71, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #31, step 72, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #31, step 73, discriminator loss=0.685 , generator loss=0.715\n",
      "Training progress in epoch #31, step 74, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #31, step 75, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #31, step 76, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #31, step 77, discriminator loss=0.696 , generator loss=0.724\n",
      "Training progress in epoch #31, step 78, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #31, step 79, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #31, step 80, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #31, step 81, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #31, step 82, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #31, step 83, discriminator loss=0.696 , generator loss=0.686\n",
      "Training progress in epoch #31, step 84, discriminator loss=0.684 , generator loss=0.732\n",
      "Training progress in epoch #31, step 85, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #31, step 86, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #31, step 87, discriminator loss=0.691 , generator loss=0.736\n",
      "Training progress in epoch #31, step 88, discriminator loss=0.679 , generator loss=0.729\n",
      "Training progress in epoch #31, step 89, discriminator loss=0.681 , generator loss=0.714\n",
      "Training progress in epoch #31, step 90, discriminator loss=0.675 , generator loss=0.704\n",
      "Training progress in epoch #31, step 91, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #31, step 92, discriminator loss=0.678 , generator loss=0.719\n",
      "Training progress in epoch #31, step 93, discriminator loss=0.684 , generator loss=0.688\n",
      "Training progress in epoch #31, step 94, discriminator loss=0.687 , generator loss=0.666\n",
      "Training progress in epoch #31, step 95, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #31, step 96, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #31, step 97, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #31, step 98, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #31, step 99, discriminator loss=0.702 , generator loss=0.736\n",
      "Training progress in epoch #31, step 100, discriminator loss=0.698 , generator loss=0.728\n",
      "Training progress in epoch #31, step 101, discriminator loss=0.696 , generator loss=0.719\n",
      "Training progress in epoch #31, step 102, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #31, step 103, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #31, step 104, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #31, step 105, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #31, step 106, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #31, step 107, discriminator loss=0.681 , generator loss=0.700\n",
      "Training progress in epoch #31, step 108, discriminator loss=0.677 , generator loss=0.748\n",
      "Training progress in epoch #31, step 109, discriminator loss=0.683 , generator loss=0.740\n",
      "Training progress in epoch #31, step 110, discriminator loss=0.685 , generator loss=0.765\n",
      "Training progress in epoch #31, step 111, discriminator loss=0.676 , generator loss=0.750\n",
      "Training progress in epoch #31, step 112, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #31, step 113, discriminator loss=0.683 , generator loss=0.691\n",
      "Training progress in epoch #31, step 114, discriminator loss=0.686 , generator loss=0.678\n",
      "Training progress in epoch #31, step 115, discriminator loss=0.681 , generator loss=0.681\n",
      "Training progress in epoch #31, step 116, discriminator loss=0.684 , generator loss=0.685\n",
      "Training progress in epoch #31, step 117, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #31, step 118, discriminator loss=0.697 , generator loss=0.722\n",
      "Training progress in epoch #31, step 119, discriminator loss=0.694 , generator loss=0.723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #31, step 120, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #31, step 121, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #31, step 122, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #31, step 123, discriminator loss=0.697 , generator loss=0.703\n",
      "Training progress in epoch #31, step 124, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #31, step 125, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #31, step 126, discriminator loss=0.696 , generator loss=0.668\n",
      "Training progress in epoch #31, step 127, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #31, step 128, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #31, step 129, discriminator loss=0.690 , generator loss=0.749\n",
      "Training progress in epoch #31, step 130, discriminator loss=0.694 , generator loss=0.742\n",
      "Training progress in epoch #31, step 131, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #31, step 132, discriminator loss=0.684 , generator loss=0.723\n",
      "Training progress in epoch #31, step 133, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #31, step 134, discriminator loss=0.687 , generator loss=0.738\n",
      "Training progress in epoch #31, step 135, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #31, step 136, discriminator loss=0.689 , generator loss=0.656\n",
      "Training progress in epoch #31, step 137, discriminator loss=0.682 , generator loss=0.657\n",
      "Training progress in epoch #31, step 138, discriminator loss=0.683 , generator loss=0.682\n",
      "Training progress in epoch #31, step 139, discriminator loss=0.680 , generator loss=0.708\n",
      "Training progress in epoch #31, step 140, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #31, step 141, discriminator loss=0.699 , generator loss=0.720\n",
      "Training progress in epoch #31, step 142, discriminator loss=0.695 , generator loss=0.731\n",
      "Training progress in epoch #31, step 143, discriminator loss=0.693 , generator loss=0.727\n",
      "Training progress in epoch #31, step 144, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #31, step 145, discriminator loss=0.697 , generator loss=0.730\n",
      "Training progress in epoch #31, step 146, discriminator loss=0.687 , generator loss=0.720\n",
      "Training progress in epoch #31, step 147, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #31, step 148, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #31, step 149, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #31, step 150, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #31, step 151, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #31, step 152, discriminator loss=0.682 , generator loss=0.722\n",
      "Training progress in epoch #31, step 153, discriminator loss=0.685 , generator loss=0.739\n",
      "Training progress in epoch #31, step 154, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #31, step 155, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #31, step 156, discriminator loss=0.687 , generator loss=0.731\n",
      "Training progress in epoch #31, step 157, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #31, step 158, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #31, step 159, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #31, step 160, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #31, step 161, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #31, step 162, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #31, step 163, discriminator loss=0.687 , generator loss=0.679\n",
      "Training progress in epoch #31, step 164, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #31, step 165, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #31, step 166, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #31, step 167, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #31, step 168, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #31, step 169, discriminator loss=0.685 , generator loss=0.735\n",
      "Training progress in epoch #31, step 170, discriminator loss=0.680 , generator loss=0.719\n",
      "Training progress in epoch #31, step 171, discriminator loss=0.696 , generator loss=0.739\n",
      "Training progress in epoch #31, step 172, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #31, step 173, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #31, step 174, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #31, step 175, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #31, step 176, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #31, step 177, discriminator loss=0.688 , generator loss=0.682\n",
      "Training progress in epoch #31, step 178, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #31, step 179, discriminator loss=0.682 , generator loss=0.709\n",
      "Training progress in epoch #31, step 180, discriminator loss=0.693 , generator loss=0.737\n",
      "Training progress in epoch #31, step 181, discriminator loss=0.687 , generator loss=0.727\n",
      "Training progress in epoch #31, step 182, discriminator loss=0.683 , generator loss=0.693\n",
      "Training progress in epoch #31, step 183, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #31, step 184, discriminator loss=0.689 , generator loss=0.758\n",
      "Training progress in epoch #31, step 185, discriminator loss=0.688 , generator loss=0.751\n",
      "Training progress in epoch #31, step 186, discriminator loss=0.680 , generator loss=0.721\n",
      "Training progress in epoch #31, step 187, discriminator loss=0.694 , generator loss=0.672\n",
      "Training progress in epoch #31, step 188, discriminator loss=0.691 , generator loss=0.666\n",
      "Training progress in epoch #31, step 189, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #31, step 190, discriminator loss=0.695 , generator loss=0.677\n",
      "Training progress in epoch #31, step 191, discriminator loss=0.696 , generator loss=0.676\n",
      "Training progress in epoch #31, step 192, discriminator loss=0.689 , generator loss=0.667\n",
      "Training progress in epoch #31, step 193, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #31, step 194, discriminator loss=0.687 , generator loss=0.749\n",
      "Training progress in epoch #31, step 195, discriminator loss=0.683 , generator loss=0.757\n",
      "Training progress in epoch #31, step 196, discriminator loss=0.689 , generator loss=0.757\n",
      "Training progress in epoch #31, step 197, discriminator loss=0.690 , generator loss=0.758\n",
      "Training progress in epoch #31, step 198, discriminator loss=0.690 , generator loss=0.751\n",
      "Training progress in epoch #31, step 199, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #31, step 200, discriminator loss=0.682 , generator loss=0.690\n",
      "Training progress in epoch #31, step 201, discriminator loss=0.699 , generator loss=0.666\n",
      "Training progress in epoch #31, step 202, discriminator loss=0.692 , generator loss=0.663\n",
      "Training progress in epoch #31, step 203, discriminator loss=0.687 , generator loss=0.672\n",
      "Training progress in epoch #31, step 204, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #31, step 205, discriminator loss=0.683 , generator loss=0.715\n",
      "Training progress in epoch #31, step 206, discriminator loss=0.681 , generator loss=0.723\n",
      "Training progress in epoch #31, step 207, discriminator loss=0.686 , generator loss=0.730\n",
      "Training progress in epoch #31, step 208, discriminator loss=0.686 , generator loss=0.729\n",
      "Training progress in epoch #31, step 209, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #31, step 210, discriminator loss=0.690 , generator loss=0.746\n",
      "Training progress in epoch #31, step 211, discriminator loss=0.695 , generator loss=0.745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #31, step 212, discriminator loss=0.683 , generator loss=0.725\n",
      "Training progress in epoch #31, step 213, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #31, step 214, discriminator loss=0.681 , generator loss=0.692\n",
      "Training progress in epoch #31, step 215, discriminator loss=0.682 , generator loss=0.665\n",
      "Training progress in epoch #31, step 216, discriminator loss=0.683 , generator loss=0.683\n",
      "Training progress in epoch #31, step 217, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #31, step 218, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #31, step 219, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #31, step 220, discriminator loss=0.696 , generator loss=0.721\n",
      "Training progress in epoch #31, step 221, discriminator loss=0.693 , generator loss=0.740\n",
      "Training progress in epoch #31, step 222, discriminator loss=0.698 , generator loss=0.738\n",
      "Training progress in epoch #31, step 223, discriminator loss=0.700 , generator loss=0.712\n",
      "Training progress in epoch #31, step 224, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #31, step 225, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #31, step 226, discriminator loss=0.686 , generator loss=0.662\n",
      "Training progress in epoch #31, step 227, discriminator loss=0.693 , generator loss=0.674\n",
      "Training progress in epoch #31, step 228, discriminator loss=0.683 , generator loss=0.727\n",
      "Training progress in epoch #31, step 229, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #31, step 230, discriminator loss=0.694 , generator loss=0.728\n",
      "Training progress in epoch #31, step 231, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #31, step 232, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #31, step 233, discriminator loss=0.691 , generator loss=0.708\n",
      "Disciminator Accuracy on real images: 56%, on fake images: 82%\n",
      "Training progress in epoch #32, step 0, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #32, step 1, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #32, step 2, discriminator loss=0.681 , generator loss=0.691\n",
      "Training progress in epoch #32, step 3, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #32, step 4, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #32, step 5, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #32, step 6, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #32, step 7, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #32, step 8, discriminator loss=0.699 , generator loss=0.716\n",
      "Training progress in epoch #32, step 9, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #32, step 10, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #32, step 11, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #32, step 12, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #32, step 13, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #32, step 14, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #32, step 15, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #32, step 16, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #32, step 17, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #32, step 18, discriminator loss=0.682 , generator loss=0.714\n",
      "Training progress in epoch #32, step 19, discriminator loss=0.694 , generator loss=0.738\n",
      "Training progress in epoch #32, step 20, discriminator loss=0.688 , generator loss=0.739\n",
      "Training progress in epoch #32, step 21, discriminator loss=0.683 , generator loss=0.723\n",
      "Training progress in epoch #32, step 22, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #32, step 23, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #32, step 24, discriminator loss=0.691 , generator loss=0.677\n",
      "Training progress in epoch #32, step 25, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #32, step 26, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #32, step 27, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #32, step 28, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #32, step 29, discriminator loss=0.677 , generator loss=0.693\n",
      "Training progress in epoch #32, step 30, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #32, step 31, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #32, step 32, discriminator loss=0.694 , generator loss=0.738\n",
      "Training progress in epoch #32, step 33, discriminator loss=0.686 , generator loss=0.752\n",
      "Training progress in epoch #32, step 34, discriminator loss=0.687 , generator loss=0.727\n",
      "Training progress in epoch #32, step 35, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #32, step 36, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #32, step 37, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #32, step 38, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #32, step 39, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #32, step 40, discriminator loss=0.699 , generator loss=0.664\n",
      "Training progress in epoch #32, step 41, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #32, step 42, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #32, step 43, discriminator loss=0.698 , generator loss=0.745\n",
      "Training progress in epoch #32, step 44, discriminator loss=0.683 , generator loss=0.730\n",
      "Training progress in epoch #32, step 45, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #32, step 46, discriminator loss=0.686 , generator loss=0.740\n",
      "Training progress in epoch #32, step 47, discriminator loss=0.678 , generator loss=0.706\n",
      "Training progress in epoch #32, step 48, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #32, step 49, discriminator loss=0.686 , generator loss=0.680\n",
      "Training progress in epoch #32, step 50, discriminator loss=0.681 , generator loss=0.694\n",
      "Training progress in epoch #32, step 51, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #32, step 52, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #32, step 53, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #32, step 54, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #32, step 55, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #32, step 56, discriminator loss=0.685 , generator loss=0.729\n",
      "Training progress in epoch #32, step 57, discriminator loss=0.696 , generator loss=0.727\n",
      "Training progress in epoch #32, step 58, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #32, step 59, discriminator loss=0.688 , generator loss=0.737\n",
      "Training progress in epoch #32, step 60, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #32, step 61, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #32, step 62, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #32, step 63, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #32, step 64, discriminator loss=0.689 , generator loss=0.671\n",
      "Training progress in epoch #32, step 65, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #32, step 66, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #32, step 67, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #32, step 68, discriminator loss=0.683 , generator loss=0.696\n",
      "Training progress in epoch #32, step 69, discriminator loss=0.694 , generator loss=0.727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #32, step 70, discriminator loss=0.693 , generator loss=0.751\n",
      "Training progress in epoch #32, step 71, discriminator loss=0.692 , generator loss=0.745\n",
      "Training progress in epoch #32, step 72, discriminator loss=0.691 , generator loss=0.761\n",
      "Training progress in epoch #32, step 73, discriminator loss=0.687 , generator loss=0.740\n",
      "Training progress in epoch #32, step 74, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #32, step 75, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #32, step 76, discriminator loss=0.686 , generator loss=0.674\n",
      "Training progress in epoch #32, step 77, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #32, step 78, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #32, step 79, discriminator loss=0.678 , generator loss=0.683\n",
      "Training progress in epoch #32, step 80, discriminator loss=0.677 , generator loss=0.696\n",
      "Training progress in epoch #32, step 81, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #32, step 82, discriminator loss=0.682 , generator loss=0.698\n",
      "Training progress in epoch #32, step 83, discriminator loss=0.685 , generator loss=0.721\n",
      "Training progress in epoch #32, step 84, discriminator loss=0.686 , generator loss=0.749\n",
      "Training progress in epoch #32, step 85, discriminator loss=0.689 , generator loss=0.780\n",
      "Training progress in epoch #32, step 86, discriminator loss=0.686 , generator loss=0.752\n",
      "Training progress in epoch #32, step 87, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #32, step 88, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #32, step 89, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #32, step 90, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #32, step 91, discriminator loss=0.698 , generator loss=0.682\n",
      "Training progress in epoch #32, step 92, discriminator loss=0.687 , generator loss=0.656\n",
      "Training progress in epoch #32, step 93, discriminator loss=0.692 , generator loss=0.656\n",
      "Training progress in epoch #32, step 94, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #32, step 95, discriminator loss=0.694 , generator loss=0.757\n",
      "Training progress in epoch #32, step 96, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #32, step 97, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #32, step 98, discriminator loss=0.688 , generator loss=0.738\n",
      "Training progress in epoch #32, step 99, discriminator loss=0.690 , generator loss=0.736\n",
      "Training progress in epoch #32, step 100, discriminator loss=0.673 , generator loss=0.710\n",
      "Training progress in epoch #32, step 101, discriminator loss=0.681 , generator loss=0.716\n",
      "Training progress in epoch #32, step 102, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #32, step 103, discriminator loss=0.683 , generator loss=0.702\n",
      "Training progress in epoch #32, step 104, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #32, step 105, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #32, step 106, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #32, step 107, discriminator loss=0.682 , generator loss=0.665\n",
      "Training progress in epoch #32, step 108, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #32, step 109, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #32, step 110, discriminator loss=0.694 , generator loss=0.728\n",
      "Training progress in epoch #32, step 111, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #32, step 112, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #32, step 113, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #32, step 114, discriminator loss=0.698 , generator loss=0.752\n",
      "Training progress in epoch #32, step 115, discriminator loss=0.687 , generator loss=0.750\n",
      "Training progress in epoch #32, step 116, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #32, step 117, discriminator loss=0.695 , generator loss=0.676\n",
      "Training progress in epoch #32, step 118, discriminator loss=0.686 , generator loss=0.663\n",
      "Training progress in epoch #32, step 119, discriminator loss=0.690 , generator loss=0.659\n",
      "Training progress in epoch #32, step 120, discriminator loss=0.686 , generator loss=0.685\n",
      "Training progress in epoch #32, step 121, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #32, step 122, discriminator loss=0.683 , generator loss=0.721\n",
      "Training progress in epoch #32, step 123, discriminator loss=0.686 , generator loss=0.731\n",
      "Training progress in epoch #32, step 124, discriminator loss=0.681 , generator loss=0.734\n",
      "Training progress in epoch #32, step 125, discriminator loss=0.686 , generator loss=0.733\n",
      "Training progress in epoch #32, step 126, discriminator loss=0.690 , generator loss=0.747\n",
      "Training progress in epoch #32, step 127, discriminator loss=0.686 , generator loss=0.745\n",
      "Training progress in epoch #32, step 128, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #32, step 129, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #32, step 130, discriminator loss=0.684 , generator loss=0.687\n",
      "Training progress in epoch #32, step 131, discriminator loss=0.691 , generator loss=0.731\n",
      "Training progress in epoch #32, step 132, discriminator loss=0.684 , generator loss=0.690\n",
      "Training progress in epoch #32, step 133, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #32, step 134, discriminator loss=0.698 , generator loss=0.688\n",
      "Training progress in epoch #32, step 135, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #32, step 136, discriminator loss=0.692 , generator loss=0.742\n",
      "Training progress in epoch #32, step 137, discriminator loss=0.694 , generator loss=0.731\n",
      "Training progress in epoch #32, step 138, discriminator loss=0.697 , generator loss=0.757\n",
      "Training progress in epoch #32, step 139, discriminator loss=0.698 , generator loss=0.724\n",
      "Training progress in epoch #32, step 140, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #32, step 141, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #32, step 142, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #32, step 143, discriminator loss=0.697 , generator loss=0.672\n",
      "Training progress in epoch #32, step 144, discriminator loss=0.688 , generator loss=0.667\n",
      "Training progress in epoch #32, step 145, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #32, step 146, discriminator loss=0.682 , generator loss=0.719\n",
      "Training progress in epoch #32, step 147, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #32, step 148, discriminator loss=0.681 , generator loss=0.739\n",
      "Training progress in epoch #32, step 149, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #32, step 150, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #32, step 151, discriminator loss=0.688 , generator loss=0.741\n",
      "Training progress in epoch #32, step 152, discriminator loss=0.681 , generator loss=0.735\n",
      "Training progress in epoch #32, step 153, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #32, step 154, discriminator loss=0.684 , generator loss=0.691\n",
      "Training progress in epoch #32, step 155, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #32, step 156, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #32, step 157, discriminator loss=0.696 , generator loss=0.682\n",
      "Training progress in epoch #32, step 158, discriminator loss=0.695 , generator loss=0.673\n",
      "Training progress in epoch #32, step 159, discriminator loss=0.688 , generator loss=0.678\n",
      "Training progress in epoch #32, step 160, discriminator loss=0.688 , generator loss=0.732\n",
      "Training progress in epoch #32, step 161, discriminator loss=0.690 , generator loss=0.767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #32, step 162, discriminator loss=0.696 , generator loss=0.779\n",
      "Training progress in epoch #32, step 163, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #32, step 164, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #32, step 165, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #32, step 166, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #32, step 167, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #32, step 168, discriminator loss=0.682 , generator loss=0.679\n",
      "Training progress in epoch #32, step 169, discriminator loss=0.680 , generator loss=0.693\n",
      "Training progress in epoch #32, step 170, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #32, step 171, discriminator loss=0.682 , generator loss=0.718\n",
      "Training progress in epoch #32, step 172, discriminator loss=0.695 , generator loss=0.719\n",
      "Training progress in epoch #32, step 173, discriminator loss=0.682 , generator loss=0.718\n",
      "Training progress in epoch #32, step 174, discriminator loss=0.695 , generator loss=0.728\n",
      "Training progress in epoch #32, step 175, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #32, step 176, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #32, step 177, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #32, step 178, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #32, step 179, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #32, step 180, discriminator loss=0.699 , generator loss=0.725\n",
      "Training progress in epoch #32, step 181, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #32, step 182, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #32, step 183, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #32, step 184, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #32, step 185, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #32, step 186, discriminator loss=0.684 , generator loss=0.701\n",
      "Training progress in epoch #32, step 187, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #32, step 188, discriminator loss=0.693 , generator loss=0.738\n",
      "Training progress in epoch #32, step 189, discriminator loss=0.687 , generator loss=0.742\n",
      "Training progress in epoch #32, step 190, discriminator loss=0.687 , generator loss=0.736\n",
      "Training progress in epoch #32, step 191, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #32, step 192, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #32, step 193, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #32, step 194, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #32, step 195, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #32, step 196, discriminator loss=0.697 , generator loss=0.693\n",
      "Training progress in epoch #32, step 197, discriminator loss=0.683 , generator loss=0.648\n",
      "Training progress in epoch #32, step 198, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #32, step 199, discriminator loss=0.699 , generator loss=0.735\n",
      "Training progress in epoch #32, step 200, discriminator loss=0.692 , generator loss=0.743\n",
      "Training progress in epoch #32, step 201, discriminator loss=0.696 , generator loss=0.734\n",
      "Training progress in epoch #32, step 202, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #32, step 203, discriminator loss=0.681 , generator loss=0.696\n",
      "Training progress in epoch #32, step 204, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #32, step 205, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #32, step 206, discriminator loss=0.683 , generator loss=0.720\n",
      "Training progress in epoch #32, step 207, discriminator loss=0.679 , generator loss=0.714\n",
      "Training progress in epoch #32, step 208, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #32, step 209, discriminator loss=0.683 , generator loss=0.694\n",
      "Training progress in epoch #32, step 210, discriminator loss=0.683 , generator loss=0.703\n",
      "Training progress in epoch #32, step 211, discriminator loss=0.696 , generator loss=0.728\n",
      "Training progress in epoch #32, step 212, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #32, step 213, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #32, step 214, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #32, step 215, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #32, step 216, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #32, step 217, discriminator loss=0.696 , generator loss=0.722\n",
      "Training progress in epoch #32, step 218, discriminator loss=0.695 , generator loss=0.723\n",
      "Training progress in epoch #32, step 219, discriminator loss=0.696 , generator loss=0.726\n",
      "Training progress in epoch #32, step 220, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #32, step 221, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #32, step 222, discriminator loss=0.695 , generator loss=0.738\n",
      "Training progress in epoch #32, step 223, discriminator loss=0.683 , generator loss=0.745\n",
      "Training progress in epoch #32, step 224, discriminator loss=0.682 , generator loss=0.710\n",
      "Training progress in epoch #32, step 225, discriminator loss=0.678 , generator loss=0.718\n",
      "Training progress in epoch #32, step 226, discriminator loss=0.682 , generator loss=0.718\n",
      "Training progress in epoch #32, step 227, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #32, step 228, discriminator loss=0.678 , generator loss=0.710\n",
      "Training progress in epoch #32, step 229, discriminator loss=0.676 , generator loss=0.704\n",
      "Training progress in epoch #32, step 230, discriminator loss=0.684 , generator loss=0.707\n",
      "Training progress in epoch #32, step 231, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #32, step 232, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #32, step 233, discriminator loss=0.684 , generator loss=0.738\n",
      "Disciminator Accuracy on real images: 33%, on fake images: 81%\n",
      "Training progress in epoch #33, step 0, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #33, step 1, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #33, step 2, discriminator loss=0.702 , generator loss=0.692\n",
      "Training progress in epoch #33, step 3, discriminator loss=0.700 , generator loss=0.714\n",
      "Training progress in epoch #33, step 4, discriminator loss=0.698 , generator loss=0.699\n",
      "Training progress in epoch #33, step 5, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #33, step 6, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #33, step 7, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #33, step 8, discriminator loss=0.691 , generator loss=0.733\n",
      "Training progress in epoch #33, step 9, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #33, step 10, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #33, step 11, discriminator loss=0.686 , generator loss=0.683\n",
      "Training progress in epoch #33, step 12, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #33, step 13, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #33, step 14, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #33, step 15, discriminator loss=0.683 , generator loss=0.757\n",
      "Training progress in epoch #33, step 16, discriminator loss=0.682 , generator loss=0.737\n",
      "Training progress in epoch #33, step 17, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #33, step 18, discriminator loss=0.683 , generator loss=0.712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #33, step 19, discriminator loss=0.691 , generator loss=0.676\n",
      "Training progress in epoch #33, step 20, discriminator loss=0.680 , generator loss=0.695\n",
      "Training progress in epoch #33, step 21, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #33, step 22, discriminator loss=0.698 , generator loss=0.683\n",
      "Training progress in epoch #33, step 23, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #33, step 24, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #33, step 25, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #33, step 26, discriminator loss=0.695 , generator loss=0.727\n",
      "Training progress in epoch #33, step 27, discriminator loss=0.686 , generator loss=0.733\n",
      "Training progress in epoch #33, step 28, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #33, step 29, discriminator loss=0.685 , generator loss=0.750\n",
      "Training progress in epoch #33, step 30, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #33, step 31, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #33, step 32, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #33, step 33, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #33, step 34, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #33, step 35, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #33, step 36, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #33, step 37, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #33, step 38, discriminator loss=0.693 , generator loss=0.748\n",
      "Training progress in epoch #33, step 39, discriminator loss=0.695 , generator loss=0.766\n",
      "Training progress in epoch #33, step 40, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #33, step 41, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #33, step 42, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #33, step 43, discriminator loss=0.694 , generator loss=0.735\n",
      "Training progress in epoch #33, step 44, discriminator loss=0.697 , generator loss=0.717\n",
      "Training progress in epoch #33, step 45, discriminator loss=0.697 , generator loss=0.689\n",
      "Training progress in epoch #33, step 46, discriminator loss=0.690 , generator loss=0.653\n",
      "Training progress in epoch #33, step 47, discriminator loss=0.691 , generator loss=0.676\n",
      "Training progress in epoch #33, step 48, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #33, step 49, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #33, step 50, discriminator loss=0.683 , generator loss=0.728\n",
      "Training progress in epoch #33, step 51, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #33, step 52, discriminator loss=0.684 , generator loss=0.747\n",
      "Training progress in epoch #33, step 53, discriminator loss=0.682 , generator loss=0.737\n",
      "Training progress in epoch #33, step 54, discriminator loss=0.688 , generator loss=0.735\n",
      "Training progress in epoch #33, step 55, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #33, step 56, discriminator loss=0.690 , generator loss=0.679\n",
      "Training progress in epoch #33, step 57, discriminator loss=0.684 , generator loss=0.696\n",
      "Training progress in epoch #33, step 58, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #33, step 59, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #33, step 60, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #33, step 61, discriminator loss=0.697 , generator loss=0.684\n",
      "Training progress in epoch #33, step 62, discriminator loss=0.697 , generator loss=0.680\n",
      "Training progress in epoch #33, step 63, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #33, step 64, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #33, step 65, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #33, step 66, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #33, step 67, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #33, step 68, discriminator loss=0.683 , generator loss=0.722\n",
      "Training progress in epoch #33, step 69, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #33, step 70, discriminator loss=0.682 , generator loss=0.717\n",
      "Training progress in epoch #33, step 71, discriminator loss=0.684 , generator loss=0.725\n",
      "Training progress in epoch #33, step 72, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #33, step 73, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #33, step 74, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #33, step 75, discriminator loss=0.685 , generator loss=0.687\n",
      "Training progress in epoch #33, step 76, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #33, step 77, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #33, step 78, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #33, step 79, discriminator loss=0.690 , generator loss=0.734\n",
      "Training progress in epoch #33, step 80, discriminator loss=0.687 , generator loss=0.735\n",
      "Training progress in epoch #33, step 81, discriminator loss=0.697 , generator loss=0.710\n",
      "Training progress in epoch #33, step 82, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #33, step 83, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #33, step 84, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #33, step 85, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #33, step 86, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #33, step 87, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #33, step 88, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #33, step 89, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #33, step 90, discriminator loss=0.679 , generator loss=0.702\n",
      "Training progress in epoch #33, step 91, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #33, step 92, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #33, step 93, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #33, step 94, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #33, step 95, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #33, step 96, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #33, step 97, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #33, step 98, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #33, step 99, discriminator loss=0.679 , generator loss=0.720\n",
      "Training progress in epoch #33, step 100, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #33, step 101, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #33, step 102, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #33, step 103, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #33, step 104, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #33, step 105, discriminator loss=0.684 , generator loss=0.705\n",
      "Training progress in epoch #33, step 106, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #33, step 107, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #33, step 108, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #33, step 109, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #33, step 110, discriminator loss=0.691 , generator loss=0.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #33, step 111, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #33, step 112, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #33, step 113, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #33, step 114, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #33, step 115, discriminator loss=0.698 , generator loss=0.713\n",
      "Training progress in epoch #33, step 116, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #33, step 117, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #33, step 118, discriminator loss=0.689 , generator loss=0.732\n",
      "Training progress in epoch #33, step 119, discriminator loss=0.677 , generator loss=0.739\n",
      "Training progress in epoch #33, step 120, discriminator loss=0.688 , generator loss=0.742\n",
      "Training progress in epoch #33, step 121, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #33, step 122, discriminator loss=0.684 , generator loss=0.696\n",
      "Training progress in epoch #33, step 123, discriminator loss=0.690 , generator loss=0.678\n",
      "Training progress in epoch #33, step 124, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #33, step 125, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #33, step 126, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #33, step 127, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #33, step 128, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #33, step 129, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #33, step 130, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #33, step 131, discriminator loss=0.687 , generator loss=0.748\n",
      "Training progress in epoch #33, step 132, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #33, step 133, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #33, step 134, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #33, step 135, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #33, step 136, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #33, step 137, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #33, step 138, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #33, step 139, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #33, step 140, discriminator loss=0.684 , generator loss=0.731\n",
      "Training progress in epoch #33, step 141, discriminator loss=0.681 , generator loss=0.716\n",
      "Training progress in epoch #33, step 142, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #33, step 143, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #33, step 144, discriminator loss=0.692 , generator loss=0.742\n",
      "Training progress in epoch #33, step 145, discriminator loss=0.679 , generator loss=0.724\n",
      "Training progress in epoch #33, step 146, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #33, step 147, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #33, step 148, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #33, step 149, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #33, step 150, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #33, step 151, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #33, step 152, discriminator loss=0.699 , generator loss=0.702\n",
      "Training progress in epoch #33, step 153, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #33, step 154, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #33, step 155, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #33, step 156, discriminator loss=0.697 , generator loss=0.733\n",
      "Training progress in epoch #33, step 157, discriminator loss=0.686 , generator loss=0.736\n",
      "Training progress in epoch #33, step 158, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #33, step 159, discriminator loss=0.684 , generator loss=0.686\n",
      "Training progress in epoch #33, step 160, discriminator loss=0.686 , generator loss=0.680\n",
      "Training progress in epoch #33, step 161, discriminator loss=0.682 , generator loss=0.684\n",
      "Training progress in epoch #33, step 162, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #33, step 163, discriminator loss=0.675 , generator loss=0.689\n",
      "Training progress in epoch #33, step 164, discriminator loss=0.695 , generator loss=0.729\n",
      "Training progress in epoch #33, step 165, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #33, step 166, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #33, step 167, discriminator loss=0.692 , generator loss=0.769\n",
      "Training progress in epoch #33, step 168, discriminator loss=0.682 , generator loss=0.769\n",
      "Training progress in epoch #33, step 169, discriminator loss=0.696 , generator loss=0.727\n",
      "Training progress in epoch #33, step 170, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #33, step 171, discriminator loss=0.699 , generator loss=0.673\n",
      "Training progress in epoch #33, step 172, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #33, step 173, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #33, step 174, discriminator loss=0.698 , generator loss=0.695\n",
      "Training progress in epoch #33, step 175, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #33, step 176, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #33, step 177, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #33, step 178, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #33, step 179, discriminator loss=0.693 , generator loss=0.738\n",
      "Training progress in epoch #33, step 180, discriminator loss=0.688 , generator loss=0.735\n",
      "Training progress in epoch #33, step 181, discriminator loss=0.680 , generator loss=0.731\n",
      "Training progress in epoch #33, step 182, discriminator loss=0.685 , generator loss=0.738\n",
      "Training progress in epoch #33, step 183, discriminator loss=0.682 , generator loss=0.716\n",
      "Training progress in epoch #33, step 184, discriminator loss=0.682 , generator loss=0.690\n",
      "Training progress in epoch #33, step 185, discriminator loss=0.674 , generator loss=0.688\n",
      "Training progress in epoch #33, step 186, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #33, step 187, discriminator loss=0.681 , generator loss=0.716\n",
      "Training progress in epoch #33, step 188, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #33, step 189, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #33, step 190, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #33, step 191, discriminator loss=0.696 , generator loss=0.740\n",
      "Training progress in epoch #33, step 192, discriminator loss=0.696 , generator loss=0.722\n",
      "Training progress in epoch #33, step 193, discriminator loss=0.686 , generator loss=0.684\n",
      "Training progress in epoch #33, step 194, discriminator loss=0.697 , generator loss=0.708\n",
      "Training progress in epoch #33, step 195, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #33, step 196, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #33, step 197, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #33, step 198, discriminator loss=0.694 , generator loss=0.667\n",
      "Training progress in epoch #33, step 199, discriminator loss=0.689 , generator loss=0.656\n",
      "Training progress in epoch #33, step 200, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #33, step 201, discriminator loss=0.687 , generator loss=0.738\n",
      "Training progress in epoch #33, step 202, discriminator loss=0.684 , generator loss=0.717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #33, step 203, discriminator loss=0.683 , generator loss=0.718\n",
      "Training progress in epoch #33, step 204, discriminator loss=0.681 , generator loss=0.728\n",
      "Training progress in epoch #33, step 205, discriminator loss=0.682 , generator loss=0.741\n",
      "Training progress in epoch #33, step 206, discriminator loss=0.680 , generator loss=0.728\n",
      "Training progress in epoch #33, step 207, discriminator loss=0.678 , generator loss=0.715\n",
      "Training progress in epoch #33, step 208, discriminator loss=0.684 , generator loss=0.705\n",
      "Training progress in epoch #33, step 209, discriminator loss=0.684 , generator loss=0.696\n",
      "Training progress in epoch #33, step 210, discriminator loss=0.685 , generator loss=0.688\n",
      "Training progress in epoch #33, step 211, discriminator loss=0.683 , generator loss=0.688\n",
      "Training progress in epoch #33, step 212, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #33, step 213, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #33, step 214, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #33, step 215, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #33, step 216, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #33, step 217, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #33, step 218, discriminator loss=0.688 , generator loss=0.724\n",
      "Training progress in epoch #33, step 219, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #33, step 220, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #33, step 221, discriminator loss=0.687 , generator loss=0.677\n",
      "Training progress in epoch #33, step 222, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #33, step 223, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #33, step 224, discriminator loss=0.685 , generator loss=0.738\n",
      "Training progress in epoch #33, step 225, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #33, step 226, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #33, step 227, discriminator loss=0.682 , generator loss=0.741\n",
      "Training progress in epoch #33, step 228, discriminator loss=0.688 , generator loss=0.725\n",
      "Training progress in epoch #33, step 229, discriminator loss=0.685 , generator loss=0.710\n",
      "Training progress in epoch #33, step 230, discriminator loss=0.692 , generator loss=0.732\n",
      "Training progress in epoch #33, step 231, discriminator loss=0.684 , generator loss=0.716\n",
      "Training progress in epoch #33, step 232, discriminator loss=0.685 , generator loss=0.723\n",
      "Training progress in epoch #33, step 233, discriminator loss=0.693 , generator loss=0.678\n",
      "Disciminator Accuracy on real images: 95%, on fake images: 15%\n",
      "Training progress in epoch #34, step 0, discriminator loss=0.684 , generator loss=0.660\n",
      "Training progress in epoch #34, step 1, discriminator loss=0.697 , generator loss=0.707\n",
      "Training progress in epoch #34, step 2, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #34, step 3, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #34, step 4, discriminator loss=0.699 , generator loss=0.728\n",
      "Training progress in epoch #34, step 5, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #34, step 6, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #34, step 7, discriminator loss=0.698 , generator loss=0.693\n",
      "Training progress in epoch #34, step 8, discriminator loss=0.698 , generator loss=0.697\n",
      "Training progress in epoch #34, step 9, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #34, step 10, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #34, step 11, discriminator loss=0.680 , generator loss=0.716\n",
      "Training progress in epoch #34, step 12, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #34, step 13, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #34, step 14, discriminator loss=0.683 , generator loss=0.701\n",
      "Training progress in epoch #34, step 15, discriminator loss=0.683 , generator loss=0.714\n",
      "Training progress in epoch #34, step 16, discriminator loss=0.684 , generator loss=0.722\n",
      "Training progress in epoch #34, step 17, discriminator loss=0.684 , generator loss=0.726\n",
      "Training progress in epoch #34, step 18, discriminator loss=0.686 , generator loss=0.740\n",
      "Training progress in epoch #34, step 19, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #34, step 20, discriminator loss=0.680 , generator loss=0.689\n",
      "Training progress in epoch #34, step 21, discriminator loss=0.687 , generator loss=0.676\n",
      "Training progress in epoch #34, step 22, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #34, step 23, discriminator loss=0.683 , generator loss=0.718\n",
      "Training progress in epoch #34, step 24, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #34, step 25, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #34, step 26, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #34, step 27, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #34, step 28, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #34, step 29, discriminator loss=0.693 , generator loss=0.734\n",
      "Training progress in epoch #34, step 30, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #34, step 31, discriminator loss=0.697 , generator loss=0.677\n",
      "Training progress in epoch #34, step 32, discriminator loss=0.697 , generator loss=0.676\n",
      "Training progress in epoch #34, step 33, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #34, step 34, discriminator loss=0.692 , generator loss=0.731\n",
      "Training progress in epoch #34, step 35, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #34, step 36, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #34, step 37, discriminator loss=0.683 , generator loss=0.685\n",
      "Training progress in epoch #34, step 38, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #34, step 39, discriminator loss=0.688 , generator loss=0.724\n",
      "Training progress in epoch #34, step 40, discriminator loss=0.683 , generator loss=0.721\n",
      "Training progress in epoch #34, step 41, discriminator loss=0.683 , generator loss=0.710\n",
      "Training progress in epoch #34, step 42, discriminator loss=0.684 , generator loss=0.680\n",
      "Training progress in epoch #34, step 43, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #34, step 44, discriminator loss=0.686 , generator loss=0.734\n",
      "Training progress in epoch #34, step 45, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #34, step 46, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #34, step 47, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #34, step 48, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #34, step 49, discriminator loss=0.686 , generator loss=0.681\n",
      "Training progress in epoch #34, step 50, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #34, step 51, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #34, step 52, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #34, step 53, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #34, step 54, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #34, step 55, discriminator loss=0.691 , generator loss=0.743\n",
      "Training progress in epoch #34, step 56, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #34, step 57, discriminator loss=0.697 , generator loss=0.688\n",
      "Training progress in epoch #34, step 58, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #34, step 59, discriminator loss=0.698 , generator loss=0.695\n",
      "Training progress in epoch #34, step 60, discriminator loss=0.688 , generator loss=0.695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #34, step 61, discriminator loss=0.685 , generator loss=0.701\n",
      "Training progress in epoch #34, step 62, discriminator loss=0.689 , generator loss=0.732\n",
      "Training progress in epoch #34, step 63, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #34, step 64, discriminator loss=0.692 , generator loss=0.733\n",
      "Training progress in epoch #34, step 65, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #34, step 66, discriminator loss=0.685 , generator loss=0.686\n",
      "Training progress in epoch #34, step 67, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #34, step 68, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #34, step 69, discriminator loss=0.684 , generator loss=0.698\n",
      "Training progress in epoch #34, step 70, discriminator loss=0.680 , generator loss=0.701\n",
      "Training progress in epoch #34, step 71, discriminator loss=0.684 , generator loss=0.709\n",
      "Training progress in epoch #34, step 72, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #34, step 73, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #34, step 74, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #34, step 75, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #34, step 76, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #34, step 77, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #34, step 78, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #34, step 79, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #34, step 80, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #34, step 81, discriminator loss=0.697 , generator loss=0.690\n",
      "Training progress in epoch #34, step 82, discriminator loss=0.684 , generator loss=0.695\n",
      "Training progress in epoch #34, step 83, discriminator loss=0.684 , generator loss=0.720\n",
      "Training progress in epoch #34, step 84, discriminator loss=0.686 , generator loss=0.734\n",
      "Training progress in epoch #34, step 85, discriminator loss=0.684 , generator loss=0.721\n",
      "Training progress in epoch #34, step 86, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #34, step 87, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #34, step 88, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #34, step 89, discriminator loss=0.694 , generator loss=0.730\n",
      "Training progress in epoch #34, step 90, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #34, step 91, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #34, step 92, discriminator loss=0.685 , generator loss=0.681\n",
      "Training progress in epoch #34, step 93, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #34, step 94, discriminator loss=0.685 , generator loss=0.714\n",
      "Training progress in epoch #34, step 95, discriminator loss=0.682 , generator loss=0.726\n",
      "Training progress in epoch #34, step 96, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #34, step 97, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #34, step 98, discriminator loss=0.689 , generator loss=0.740\n",
      "Training progress in epoch #34, step 99, discriminator loss=0.686 , generator loss=0.726\n",
      "Training progress in epoch #34, step 100, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #34, step 101, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #34, step 102, discriminator loss=0.693 , generator loss=0.668\n",
      "Training progress in epoch #34, step 103, discriminator loss=0.695 , generator loss=0.673\n",
      "Training progress in epoch #34, step 104, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #34, step 105, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #34, step 106, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #34, step 107, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #34, step 108, discriminator loss=0.690 , generator loss=0.756\n",
      "Training progress in epoch #34, step 109, discriminator loss=0.687 , generator loss=0.741\n",
      "Training progress in epoch #34, step 110, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #34, step 111, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #34, step 112, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #34, step 113, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #34, step 114, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #34, step 115, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #34, step 116, discriminator loss=0.693 , generator loss=0.677\n",
      "Training progress in epoch #34, step 117, discriminator loss=0.687 , generator loss=0.675\n",
      "Training progress in epoch #34, step 118, discriminator loss=0.683 , generator loss=0.728\n",
      "Training progress in epoch #34, step 119, discriminator loss=0.692 , generator loss=0.747\n",
      "Training progress in epoch #34, step 120, discriminator loss=0.692 , generator loss=0.736\n",
      "Training progress in epoch #34, step 121, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #34, step 122, discriminator loss=0.684 , generator loss=0.726\n",
      "Training progress in epoch #34, step 123, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #34, step 124, discriminator loss=0.689 , generator loss=0.678\n",
      "Training progress in epoch #34, step 125, discriminator loss=0.689 , generator loss=0.677\n",
      "Training progress in epoch #34, step 126, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #34, step 127, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #34, step 128, discriminator loss=0.684 , generator loss=0.722\n",
      "Training progress in epoch #34, step 129, discriminator loss=0.679 , generator loss=0.738\n",
      "Training progress in epoch #34, step 130, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #34, step 131, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #34, step 132, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #34, step 133, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #34, step 134, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #34, step 135, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #34, step 136, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #34, step 137, discriminator loss=0.677 , generator loss=0.695\n",
      "Training progress in epoch #34, step 138, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #34, step 139, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #34, step 140, discriminator loss=0.684 , generator loss=0.748\n",
      "Training progress in epoch #34, step 141, discriminator loss=0.697 , generator loss=0.716\n",
      "Training progress in epoch #34, step 142, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #34, step 143, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #34, step 144, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #34, step 145, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #34, step 146, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #34, step 147, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #34, step 148, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #34, step 149, discriminator loss=0.685 , generator loss=0.672\n",
      "Training progress in epoch #34, step 150, discriminator loss=0.687 , generator loss=0.665\n",
      "Training progress in epoch #34, step 151, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #34, step 152, discriminator loss=0.689 , generator loss=0.718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #34, step 153, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #34, step 154, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #34, step 155, discriminator loss=0.688 , generator loss=0.724\n",
      "Training progress in epoch #34, step 156, discriminator loss=0.686 , generator loss=0.733\n",
      "Training progress in epoch #34, step 157, discriminator loss=0.689 , generator loss=0.728\n",
      "Training progress in epoch #34, step 158, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #34, step 159, discriminator loss=0.687 , generator loss=0.674\n",
      "Training progress in epoch #34, step 160, discriminator loss=0.686 , generator loss=0.651\n",
      "Training progress in epoch #34, step 161, discriminator loss=0.691 , generator loss=0.676\n",
      "Training progress in epoch #34, step 162, discriminator loss=0.695 , generator loss=0.726\n",
      "Training progress in epoch #34, step 163, discriminator loss=0.694 , generator loss=0.737\n",
      "Training progress in epoch #34, step 164, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #34, step 165, discriminator loss=0.679 , generator loss=0.705\n",
      "Training progress in epoch #34, step 166, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #34, step 167, discriminator loss=0.691 , generator loss=0.738\n",
      "Training progress in epoch #34, step 168, discriminator loss=0.688 , generator loss=0.759\n",
      "Training progress in epoch #34, step 169, discriminator loss=0.697 , generator loss=0.737\n",
      "Training progress in epoch #34, step 170, discriminator loss=0.693 , generator loss=0.676\n",
      "Training progress in epoch #34, step 171, discriminator loss=0.695 , generator loss=0.665\n",
      "Training progress in epoch #34, step 172, discriminator loss=0.694 , generator loss=0.657\n",
      "Training progress in epoch #34, step 173, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #34, step 174, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #34, step 175, discriminator loss=0.681 , generator loss=0.720\n",
      "Training progress in epoch #34, step 176, discriminator loss=0.687 , generator loss=0.720\n",
      "Training progress in epoch #34, step 177, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #34, step 178, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #34, step 179, discriminator loss=0.684 , generator loss=0.737\n",
      "Training progress in epoch #34, step 180, discriminator loss=0.682 , generator loss=0.734\n",
      "Training progress in epoch #34, step 181, discriminator loss=0.685 , generator loss=0.739\n",
      "Training progress in epoch #34, step 182, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #34, step 183, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #34, step 184, discriminator loss=0.686 , generator loss=0.677\n",
      "Training progress in epoch #34, step 185, discriminator loss=0.684 , generator loss=0.691\n",
      "Training progress in epoch #34, step 186, discriminator loss=0.690 , generator loss=0.669\n",
      "Training progress in epoch #34, step 187, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #34, step 188, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #34, step 189, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #34, step 190, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #34, step 191, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #34, step 192, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #34, step 193, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #34, step 194, discriminator loss=0.684 , generator loss=0.762\n",
      "Training progress in epoch #34, step 195, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #34, step 196, discriminator loss=0.690 , generator loss=0.667\n",
      "Training progress in epoch #34, step 197, discriminator loss=0.689 , generator loss=0.657\n",
      "Training progress in epoch #34, step 198, discriminator loss=0.687 , generator loss=0.676\n",
      "Training progress in epoch #34, step 199, discriminator loss=0.682 , generator loss=0.682\n",
      "Training progress in epoch #34, step 200, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #34, step 201, discriminator loss=0.683 , generator loss=0.725\n",
      "Training progress in epoch #34, step 202, discriminator loss=0.686 , generator loss=0.733\n",
      "Training progress in epoch #34, step 203, discriminator loss=0.688 , generator loss=0.763\n",
      "Training progress in epoch #34, step 204, discriminator loss=0.687 , generator loss=0.770\n",
      "Training progress in epoch #34, step 205, discriminator loss=0.688 , generator loss=0.777\n",
      "Training progress in epoch #34, step 206, discriminator loss=0.687 , generator loss=0.726\n",
      "Training progress in epoch #34, step 207, discriminator loss=0.689 , generator loss=0.661\n",
      "Training progress in epoch #34, step 208, discriminator loss=0.689 , generator loss=0.667\n",
      "Training progress in epoch #34, step 209, discriminator loss=0.691 , generator loss=0.677\n",
      "Training progress in epoch #34, step 210, discriminator loss=0.691 , generator loss=0.668\n",
      "Training progress in epoch #34, step 211, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #34, step 212, discriminator loss=0.697 , generator loss=0.701\n",
      "Training progress in epoch #34, step 213, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #34, step 214, discriminator loss=0.696 , generator loss=0.740\n",
      "Training progress in epoch #34, step 215, discriminator loss=0.699 , generator loss=0.761\n",
      "Training progress in epoch #34, step 216, discriminator loss=0.687 , generator loss=0.761\n",
      "Training progress in epoch #34, step 217, discriminator loss=0.689 , generator loss=0.743\n",
      "Training progress in epoch #34, step 218, discriminator loss=0.687 , generator loss=0.743\n",
      "Training progress in epoch #34, step 219, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #34, step 220, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #34, step 221, discriminator loss=0.683 , generator loss=0.657\n",
      "Training progress in epoch #34, step 222, discriminator loss=0.695 , generator loss=0.658\n",
      "Training progress in epoch #34, step 223, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #34, step 224, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #34, step 225, discriminator loss=0.686 , generator loss=0.727\n",
      "Training progress in epoch #34, step 226, discriminator loss=0.678 , generator loss=0.734\n",
      "Training progress in epoch #34, step 227, discriminator loss=0.682 , generator loss=0.736\n",
      "Training progress in epoch #34, step 228, discriminator loss=0.695 , generator loss=0.749\n",
      "Training progress in epoch #34, step 229, discriminator loss=0.683 , generator loss=0.740\n",
      "Training progress in epoch #34, step 230, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #34, step 231, discriminator loss=0.687 , generator loss=0.661\n",
      "Training progress in epoch #34, step 232, discriminator loss=0.686 , generator loss=0.662\n",
      "Training progress in epoch #34, step 233, discriminator loss=0.689 , generator loss=0.705\n",
      "Disciminator Accuracy on real images: 66%, on fake images: 69%\n",
      "Training progress in epoch #35, step 0, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #35, step 1, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #35, step 2, discriminator loss=0.683 , generator loss=0.700\n",
      "Training progress in epoch #35, step 3, discriminator loss=0.683 , generator loss=0.719\n",
      "Training progress in epoch #35, step 4, discriminator loss=0.687 , generator loss=0.737\n",
      "Training progress in epoch #35, step 5, discriminator loss=0.687 , generator loss=0.734\n",
      "Training progress in epoch #35, step 6, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #35, step 7, discriminator loss=0.691 , generator loss=0.668\n",
      "Training progress in epoch #35, step 8, discriminator loss=0.691 , generator loss=0.673\n",
      "Training progress in epoch #35, step 9, discriminator loss=0.688 , generator loss=0.697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #35, step 10, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #35, step 11, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #35, step 12, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #35, step 13, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #35, step 14, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #35, step 15, discriminator loss=0.696 , generator loss=0.722\n",
      "Training progress in epoch #35, step 16, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #35, step 17, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #35, step 18, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #35, step 19, discriminator loss=0.687 , generator loss=0.676\n",
      "Training progress in epoch #35, step 20, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #35, step 21, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #35, step 22, discriminator loss=0.679 , generator loss=0.727\n",
      "Training progress in epoch #35, step 23, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #35, step 24, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #35, step 25, discriminator loss=0.690 , generator loss=0.734\n",
      "Training progress in epoch #35, step 26, discriminator loss=0.697 , generator loss=0.750\n",
      "Training progress in epoch #35, step 27, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #35, step 28, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #35, step 29, discriminator loss=0.694 , generator loss=0.670\n",
      "Training progress in epoch #35, step 30, discriminator loss=0.694 , generator loss=0.671\n",
      "Training progress in epoch #35, step 31, discriminator loss=0.698 , generator loss=0.694\n",
      "Training progress in epoch #35, step 32, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #35, step 33, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #35, step 34, discriminator loss=0.683 , generator loss=0.723\n",
      "Training progress in epoch #35, step 35, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #35, step 36, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #35, step 37, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #35, step 38, discriminator loss=0.688 , generator loss=0.741\n",
      "Training progress in epoch #35, step 39, discriminator loss=0.679 , generator loss=0.709\n",
      "Training progress in epoch #35, step 40, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #35, step 41, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #35, step 42, discriminator loss=0.681 , generator loss=0.700\n",
      "Training progress in epoch #35, step 43, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #35, step 44, discriminator loss=0.685 , generator loss=0.688\n",
      "Training progress in epoch #35, step 45, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #35, step 46, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #35, step 47, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #35, step 48, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #35, step 49, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #35, step 50, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #35, step 51, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #35, step 52, discriminator loss=0.698 , generator loss=0.682\n",
      "Training progress in epoch #35, step 53, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #35, step 54, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #35, step 55, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #35, step 56, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #35, step 57, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #35, step 58, discriminator loss=0.677 , generator loss=0.708\n",
      "Training progress in epoch #35, step 59, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #35, step 60, discriminator loss=0.682 , generator loss=0.734\n",
      "Training progress in epoch #35, step 61, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #35, step 62, discriminator loss=0.683 , generator loss=0.710\n",
      "Training progress in epoch #35, step 63, discriminator loss=0.683 , generator loss=0.708\n",
      "Training progress in epoch #35, step 64, discriminator loss=0.683 , generator loss=0.702\n",
      "Training progress in epoch #35, step 65, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #35, step 66, discriminator loss=0.684 , generator loss=0.697\n",
      "Training progress in epoch #35, step 67, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #35, step 68, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #35, step 69, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #35, step 70, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #35, step 71, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #35, step 72, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #35, step 73, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #35, step 74, discriminator loss=0.697 , generator loss=0.691\n",
      "Training progress in epoch #35, step 75, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #35, step 76, discriminator loss=0.692 , generator loss=0.752\n",
      "Training progress in epoch #35, step 77, discriminator loss=0.688 , generator loss=0.764\n",
      "Training progress in epoch #35, step 78, discriminator loss=0.688 , generator loss=0.733\n",
      "Training progress in epoch #35, step 79, discriminator loss=0.682 , generator loss=0.682\n",
      "Training progress in epoch #35, step 80, discriminator loss=0.683 , generator loss=0.677\n",
      "Training progress in epoch #35, step 81, discriminator loss=0.682 , generator loss=0.701\n",
      "Training progress in epoch #35, step 82, discriminator loss=0.683 , generator loss=0.704\n",
      "Training progress in epoch #35, step 83, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #35, step 84, discriminator loss=0.685 , generator loss=0.725\n",
      "Training progress in epoch #35, step 85, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #35, step 86, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #35, step 87, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #35, step 88, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #35, step 89, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #35, step 90, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #35, step 91, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #35, step 92, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #35, step 93, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #35, step 94, discriminator loss=0.698 , generator loss=0.681\n",
      "Training progress in epoch #35, step 95, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #35, step 96, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #35, step 97, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #35, step 98, discriminator loss=0.698 , generator loss=0.719\n",
      "Training progress in epoch #35, step 99, discriminator loss=0.703 , generator loss=0.711\n",
      "Training progress in epoch #35, step 100, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #35, step 101, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #35, step 102, discriminator loss=0.691 , generator loss=0.743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #35, step 103, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #35, step 104, discriminator loss=0.682 , generator loss=0.692\n",
      "Training progress in epoch #35, step 105, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #35, step 106, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #35, step 107, discriminator loss=0.683 , generator loss=0.700\n",
      "Training progress in epoch #35, step 108, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #35, step 109, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #35, step 110, discriminator loss=0.704 , generator loss=0.707\n",
      "Training progress in epoch #35, step 111, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #35, step 112, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #35, step 113, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #35, step 114, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #35, step 115, discriminator loss=0.685 , generator loss=0.733\n",
      "Training progress in epoch #35, step 116, discriminator loss=0.683 , generator loss=0.733\n",
      "Training progress in epoch #35, step 117, discriminator loss=0.684 , generator loss=0.713\n",
      "Training progress in epoch #35, step 118, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #35, step 119, discriminator loss=0.697 , generator loss=0.716\n",
      "Training progress in epoch #35, step 120, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #35, step 121, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #35, step 122, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #35, step 123, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #35, step 124, discriminator loss=0.686 , generator loss=0.680\n",
      "Training progress in epoch #35, step 125, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #35, step 126, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #35, step 127, discriminator loss=0.683 , generator loss=0.716\n",
      "Training progress in epoch #35, step 128, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #35, step 129, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #35, step 130, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #35, step 131, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #35, step 132, discriminator loss=0.687 , generator loss=0.730\n",
      "Training progress in epoch #35, step 133, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #35, step 134, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #35, step 135, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #35, step 136, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #35, step 137, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #35, step 138, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #35, step 139, discriminator loss=0.681 , generator loss=0.703\n",
      "Training progress in epoch #35, step 140, discriminator loss=0.691 , generator loss=0.731\n",
      "Training progress in epoch #35, step 141, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #35, step 142, discriminator loss=0.685 , generator loss=0.709\n",
      "Training progress in epoch #35, step 143, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #35, step 144, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #35, step 145, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #35, step 146, discriminator loss=0.689 , generator loss=0.668\n",
      "Training progress in epoch #35, step 147, discriminator loss=0.685 , generator loss=0.691\n",
      "Training progress in epoch #35, step 148, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #35, step 149, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #35, step 150, discriminator loss=0.690 , generator loss=0.749\n",
      "Training progress in epoch #35, step 151, discriminator loss=0.681 , generator loss=0.740\n",
      "Training progress in epoch #35, step 152, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #35, step 153, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #35, step 154, discriminator loss=0.696 , generator loss=0.706\n",
      "Training progress in epoch #35, step 155, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #35, step 156, discriminator loss=0.701 , generator loss=0.701\n",
      "Training progress in epoch #35, step 157, discriminator loss=0.689 , generator loss=0.728\n",
      "Training progress in epoch #35, step 158, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #35, step 159, discriminator loss=0.682 , generator loss=0.713\n",
      "Training progress in epoch #35, step 160, discriminator loss=0.684 , generator loss=0.699\n",
      "Training progress in epoch #35, step 161, discriminator loss=0.689 , generator loss=0.728\n",
      "Training progress in epoch #35, step 162, discriminator loss=0.688 , generator loss=0.763\n",
      "Training progress in epoch #35, step 163, discriminator loss=0.692 , generator loss=0.749\n",
      "Training progress in epoch #35, step 164, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #35, step 165, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #35, step 166, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #35, step 167, discriminator loss=0.684 , generator loss=0.679\n",
      "Training progress in epoch #35, step 168, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #35, step 169, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #35, step 170, discriminator loss=0.686 , generator loss=0.687\n",
      "Training progress in epoch #35, step 171, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #35, step 172, discriminator loss=0.690 , generator loss=0.739\n",
      "Training progress in epoch #35, step 173, discriminator loss=0.690 , generator loss=0.743\n",
      "Training progress in epoch #35, step 174, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #35, step 175, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #35, step 176, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #35, step 177, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #35, step 178, discriminator loss=0.699 , generator loss=0.687\n",
      "Training progress in epoch #35, step 179, discriminator loss=0.685 , generator loss=0.657\n",
      "Training progress in epoch #35, step 180, discriminator loss=0.687 , generator loss=0.679\n",
      "Training progress in epoch #35, step 181, discriminator loss=0.694 , generator loss=0.724\n",
      "Training progress in epoch #35, step 182, discriminator loss=0.691 , generator loss=0.731\n",
      "Training progress in epoch #35, step 183, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #35, step 184, discriminator loss=0.698 , generator loss=0.692\n",
      "Training progress in epoch #35, step 185, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #35, step 186, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #35, step 187, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #35, step 188, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #35, step 189, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #35, step 190, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #35, step 191, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #35, step 192, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #35, step 193, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #35, step 194, discriminator loss=0.691 , generator loss=0.714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #35, step 195, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #35, step 196, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #35, step 197, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #35, step 198, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #35, step 199, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #35, step 200, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #35, step 201, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #35, step 202, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #35, step 203, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #35, step 204, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #35, step 205, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #35, step 206, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #35, step 207, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #35, step 208, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #35, step 209, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #35, step 210, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #35, step 211, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #35, step 212, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #35, step 213, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #35, step 214, discriminator loss=0.698 , generator loss=0.708\n",
      "Training progress in epoch #35, step 215, discriminator loss=0.697 , generator loss=0.730\n",
      "Training progress in epoch #35, step 216, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #35, step 217, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #35, step 218, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #35, step 219, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #35, step 220, discriminator loss=0.684 , generator loss=0.736\n",
      "Training progress in epoch #35, step 221, discriminator loss=0.688 , generator loss=0.733\n",
      "Training progress in epoch #35, step 222, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #35, step 223, discriminator loss=0.690 , generator loss=0.737\n",
      "Training progress in epoch #35, step 224, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #35, step 225, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #35, step 226, discriminator loss=0.686 , generator loss=0.669\n",
      "Training progress in epoch #35, step 227, discriminator loss=0.693 , generator loss=0.665\n",
      "Training progress in epoch #35, step 228, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #35, step 229, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #35, step 230, discriminator loss=0.694 , generator loss=0.739\n",
      "Training progress in epoch #35, step 231, discriminator loss=0.694 , generator loss=0.740\n",
      "Training progress in epoch #35, step 232, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #35, step 233, discriminator loss=0.681 , generator loss=0.727\n",
      "Disciminator Accuracy on real images: 29%, on fake images: 94%\n",
      "Training progress in epoch #36, step 0, discriminator loss=0.689 , generator loss=0.740\n",
      "Training progress in epoch #36, step 1, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #36, step 2, discriminator loss=0.685 , generator loss=0.687\n",
      "Training progress in epoch #36, step 3, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #36, step 4, discriminator loss=0.682 , generator loss=0.690\n",
      "Training progress in epoch #36, step 5, discriminator loss=0.686 , generator loss=0.683\n",
      "Training progress in epoch #36, step 6, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #36, step 7, discriminator loss=0.684 , generator loss=0.724\n",
      "Training progress in epoch #36, step 8, discriminator loss=0.689 , generator loss=0.752\n",
      "Training progress in epoch #36, step 9, discriminator loss=0.694 , generator loss=0.749\n",
      "Training progress in epoch #36, step 10, discriminator loss=0.686 , generator loss=0.724\n",
      "Training progress in epoch #36, step 11, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #36, step 12, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #36, step 13, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #36, step 14, discriminator loss=0.690 , generator loss=0.673\n",
      "Training progress in epoch #36, step 15, discriminator loss=0.692 , generator loss=0.668\n",
      "Training progress in epoch #36, step 16, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #36, step 17, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #36, step 18, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #36, step 19, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #36, step 20, discriminator loss=0.695 , generator loss=0.757\n",
      "Training progress in epoch #36, step 21, discriminator loss=0.691 , generator loss=0.748\n",
      "Training progress in epoch #36, step 22, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #36, step 23, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #36, step 24, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #36, step 25, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #36, step 26, discriminator loss=0.683 , generator loss=0.720\n",
      "Training progress in epoch #36, step 27, discriminator loss=0.683 , generator loss=0.708\n",
      "Training progress in epoch #36, step 28, discriminator loss=0.689 , generator loss=0.678\n",
      "Training progress in epoch #36, step 29, discriminator loss=0.682 , generator loss=0.700\n",
      "Training progress in epoch #36, step 30, discriminator loss=0.683 , generator loss=0.708\n",
      "Training progress in epoch #36, step 31, discriminator loss=0.688 , generator loss=0.725\n",
      "Training progress in epoch #36, step 32, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #36, step 33, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #36, step 34, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #36, step 35, discriminator loss=0.689 , generator loss=0.750\n",
      "Training progress in epoch #36, step 36, discriminator loss=0.688 , generator loss=0.758\n",
      "Training progress in epoch #36, step 37, discriminator loss=0.690 , generator loss=0.729\n",
      "Training progress in epoch #36, step 38, discriminator loss=0.683 , generator loss=0.700\n",
      "Training progress in epoch #36, step 39, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #36, step 40, discriminator loss=0.696 , generator loss=0.707\n",
      "Training progress in epoch #36, step 41, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #36, step 42, discriminator loss=0.701 , generator loss=0.691\n",
      "Training progress in epoch #36, step 43, discriminator loss=0.697 , generator loss=0.682\n",
      "Training progress in epoch #36, step 44, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #36, step 45, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #36, step 46, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #36, step 47, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #36, step 48, discriminator loss=0.682 , generator loss=0.729\n",
      "Training progress in epoch #36, step 49, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #36, step 50, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #36, step 51, discriminator loss=0.681 , generator loss=0.715\n",
      "Training progress in epoch #36, step 52, discriminator loss=0.681 , generator loss=0.730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #36, step 53, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #36, step 54, discriminator loss=0.679 , generator loss=0.695\n",
      "Training progress in epoch #36, step 55, discriminator loss=0.686 , generator loss=0.748\n",
      "Training progress in epoch #36, step 56, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #36, step 57, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #36, step 58, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #36, step 59, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #36, step 60, discriminator loss=0.696 , generator loss=0.689\n",
      "Training progress in epoch #36, step 61, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #36, step 62, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #36, step 63, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #36, step 64, discriminator loss=0.694 , generator loss=0.729\n",
      "Training progress in epoch #36, step 65, discriminator loss=0.697 , generator loss=0.751\n",
      "Training progress in epoch #36, step 66, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #36, step 67, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #36, step 68, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #36, step 69, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #36, step 70, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #36, step 71, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #36, step 72, discriminator loss=0.681 , generator loss=0.702\n",
      "Training progress in epoch #36, step 73, discriminator loss=0.684 , generator loss=0.722\n",
      "Training progress in epoch #36, step 74, discriminator loss=0.690 , generator loss=0.736\n",
      "Training progress in epoch #36, step 75, discriminator loss=0.682 , generator loss=0.713\n",
      "Training progress in epoch #36, step 76, discriminator loss=0.683 , generator loss=0.728\n",
      "Training progress in epoch #36, step 77, discriminator loss=0.693 , generator loss=0.733\n",
      "Training progress in epoch #36, step 78, discriminator loss=0.680 , generator loss=0.706\n",
      "Training progress in epoch #36, step 79, discriminator loss=0.693 , generator loss=0.673\n",
      "Training progress in epoch #36, step 80, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #36, step 81, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #36, step 82, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #36, step 83, discriminator loss=0.696 , generator loss=0.738\n",
      "Training progress in epoch #36, step 84, discriminator loss=0.688 , generator loss=0.732\n",
      "Training progress in epoch #36, step 85, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #36, step 86, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #36, step 87, discriminator loss=0.698 , generator loss=0.700\n",
      "Training progress in epoch #36, step 88, discriminator loss=0.696 , generator loss=0.679\n",
      "Training progress in epoch #36, step 89, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #36, step 90, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #36, step 91, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #36, step 92, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #36, step 93, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #36, step 94, discriminator loss=0.688 , generator loss=0.747\n",
      "Training progress in epoch #36, step 95, discriminator loss=0.684 , generator loss=0.716\n",
      "Training progress in epoch #36, step 96, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #36, step 97, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #36, step 98, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #36, step 99, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #36, step 100, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #36, step 101, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #36, step 102, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #36, step 103, discriminator loss=0.684 , generator loss=0.697\n",
      "Training progress in epoch #36, step 104, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #36, step 105, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #36, step 106, discriminator loss=0.698 , generator loss=0.732\n",
      "Training progress in epoch #36, step 107, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #36, step 108, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #36, step 109, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #36, step 110, discriminator loss=0.696 , generator loss=0.729\n",
      "Training progress in epoch #36, step 111, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #36, step 112, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #36, step 113, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #36, step 114, discriminator loss=0.691 , generator loss=0.735\n",
      "Training progress in epoch #36, step 115, discriminator loss=0.693 , generator loss=0.759\n",
      "Training progress in epoch #36, step 116, discriminator loss=0.698 , generator loss=0.704\n",
      "Training progress in epoch #36, step 117, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #36, step 118, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #36, step 119, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #36, step 120, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #36, step 121, discriminator loss=0.688 , generator loss=0.682\n",
      "Training progress in epoch #36, step 122, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #36, step 123, discriminator loss=0.698 , generator loss=0.709\n",
      "Training progress in epoch #36, step 124, discriminator loss=0.691 , generator loss=0.731\n",
      "Training progress in epoch #36, step 125, discriminator loss=0.685 , generator loss=0.733\n",
      "Training progress in epoch #36, step 126, discriminator loss=0.685 , generator loss=0.737\n",
      "Training progress in epoch #36, step 127, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #36, step 128, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #36, step 129, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #36, step 130, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #36, step 131, discriminator loss=0.687 , generator loss=0.677\n",
      "Training progress in epoch #36, step 132, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #36, step 133, discriminator loss=0.685 , generator loss=0.714\n",
      "Training progress in epoch #36, step 134, discriminator loss=0.683 , generator loss=0.728\n",
      "Training progress in epoch #36, step 135, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #36, step 136, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #36, step 137, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #36, step 138, discriminator loss=0.696 , generator loss=0.725\n",
      "Training progress in epoch #36, step 139, discriminator loss=0.681 , generator loss=0.732\n",
      "Training progress in epoch #36, step 140, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #36, step 141, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #36, step 142, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #36, step 143, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #36, step 144, discriminator loss=0.681 , generator loss=0.715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #36, step 145, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #36, step 146, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #36, step 147, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #36, step 148, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #36, step 149, discriminator loss=0.685 , generator loss=0.721\n",
      "Training progress in epoch #36, step 150, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #36, step 151, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #36, step 152, discriminator loss=0.684 , generator loss=0.677\n",
      "Training progress in epoch #36, step 153, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #36, step 154, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #36, step 155, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #36, step 156, discriminator loss=0.685 , generator loss=0.738\n",
      "Training progress in epoch #36, step 157, discriminator loss=0.681 , generator loss=0.697\n",
      "Training progress in epoch #36, step 158, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #36, step 159, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #36, step 160, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #36, step 161, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #36, step 162, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #36, step 163, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #36, step 164, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #36, step 165, discriminator loss=0.688 , generator loss=0.732\n",
      "Training progress in epoch #36, step 166, discriminator loss=0.689 , generator loss=0.759\n",
      "Training progress in epoch #36, step 167, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #36, step 168, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #36, step 169, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #36, step 170, discriminator loss=0.682 , generator loss=0.717\n",
      "Training progress in epoch #36, step 171, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #36, step 172, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #36, step 173, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #36, step 174, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #36, step 175, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #36, step 176, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #36, step 177, discriminator loss=0.694 , generator loss=0.728\n",
      "Training progress in epoch #36, step 178, discriminator loss=0.683 , generator loss=0.710\n",
      "Training progress in epoch #36, step 179, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #36, step 180, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #36, step 181, discriminator loss=0.688 , generator loss=0.672\n",
      "Training progress in epoch #36, step 182, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #36, step 183, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #36, step 184, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #36, step 185, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #36, step 186, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #36, step 187, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #36, step 188, discriminator loss=0.684 , generator loss=0.730\n",
      "Training progress in epoch #36, step 189, discriminator loss=0.688 , generator loss=0.749\n",
      "Training progress in epoch #36, step 190, discriminator loss=0.676 , generator loss=0.740\n",
      "Training progress in epoch #36, step 191, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #36, step 192, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #36, step 193, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #36, step 194, discriminator loss=0.681 , generator loss=0.675\n",
      "Training progress in epoch #36, step 195, discriminator loss=0.682 , generator loss=0.681\n",
      "Training progress in epoch #36, step 196, discriminator loss=0.684 , generator loss=0.686\n",
      "Training progress in epoch #36, step 197, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #36, step 198, discriminator loss=0.691 , generator loss=0.739\n",
      "Training progress in epoch #36, step 199, discriminator loss=0.693 , generator loss=0.739\n",
      "Training progress in epoch #36, step 200, discriminator loss=0.685 , generator loss=0.733\n",
      "Training progress in epoch #36, step 201, discriminator loss=0.699 , generator loss=0.714\n",
      "Training progress in epoch #36, step 202, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #36, step 203, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #36, step 204, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #36, step 205, discriminator loss=0.693 , generator loss=0.655\n",
      "Training progress in epoch #36, step 206, discriminator loss=0.695 , generator loss=0.674\n",
      "Training progress in epoch #36, step 207, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #36, step 208, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #36, step 209, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #36, step 210, discriminator loss=0.688 , generator loss=0.740\n",
      "Training progress in epoch #36, step 211, discriminator loss=0.689 , generator loss=0.763\n",
      "Training progress in epoch #36, step 212, discriminator loss=0.681 , generator loss=0.753\n",
      "Training progress in epoch #36, step 213, discriminator loss=0.685 , generator loss=0.758\n",
      "Training progress in epoch #36, step 214, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #36, step 215, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #36, step 216, discriminator loss=0.698 , generator loss=0.703\n",
      "Training progress in epoch #36, step 217, discriminator loss=0.686 , generator loss=0.669\n",
      "Training progress in epoch #36, step 218, discriminator loss=0.696 , generator loss=0.659\n",
      "Training progress in epoch #36, step 219, discriminator loss=0.688 , generator loss=0.669\n",
      "Training progress in epoch #36, step 220, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #36, step 221, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #36, step 222, discriminator loss=0.692 , generator loss=0.754\n",
      "Training progress in epoch #36, step 223, discriminator loss=0.692 , generator loss=0.770\n",
      "Training progress in epoch #36, step 224, discriminator loss=0.698 , generator loss=0.741\n",
      "Training progress in epoch #36, step 225, discriminator loss=0.693 , generator loss=0.741\n",
      "Training progress in epoch #36, step 226, discriminator loss=0.684 , generator loss=0.698\n",
      "Training progress in epoch #36, step 227, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #36, step 228, discriminator loss=0.694 , generator loss=0.671\n",
      "Training progress in epoch #36, step 229, discriminator loss=0.691 , generator loss=0.669\n",
      "Training progress in epoch #36, step 230, discriminator loss=0.688 , generator loss=0.674\n",
      "Training progress in epoch #36, step 231, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #36, step 232, discriminator loss=0.685 , generator loss=0.727\n",
      "Training progress in epoch #36, step 233, discriminator loss=0.686 , generator loss=0.737\n",
      "Disciminator Accuracy on real images: 21%, on fake images: 93%\n",
      "Training progress in epoch #37, step 0, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #37, step 1, discriminator loss=0.685 , generator loss=0.713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #37, step 2, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #37, step 3, discriminator loss=0.688 , generator loss=0.741\n",
      "Training progress in epoch #37, step 4, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #37, step 5, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #37, step 6, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #37, step 7, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #37, step 8, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #37, step 9, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #37, step 10, discriminator loss=0.696 , generator loss=0.718\n",
      "Training progress in epoch #37, step 11, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #37, step 12, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #37, step 13, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #37, step 14, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #37, step 15, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #37, step 16, discriminator loss=0.683 , generator loss=0.692\n",
      "Training progress in epoch #37, step 17, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #37, step 18, discriminator loss=0.688 , generator loss=0.724\n",
      "Training progress in epoch #37, step 19, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #37, step 20, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #37, step 21, discriminator loss=0.687 , generator loss=0.675\n",
      "Training progress in epoch #37, step 22, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #37, step 23, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #37, step 24, discriminator loss=0.688 , generator loss=0.731\n",
      "Training progress in epoch #37, step 25, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #37, step 26, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #37, step 27, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #37, step 28, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #37, step 29, discriminator loss=0.698 , generator loss=0.700\n",
      "Training progress in epoch #37, step 30, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #37, step 31, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #37, step 32, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #37, step 33, discriminator loss=0.699 , generator loss=0.705\n",
      "Training progress in epoch #37, step 34, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #37, step 35, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #37, step 36, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #37, step 37, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #37, step 38, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #37, step 39, discriminator loss=0.691 , generator loss=0.733\n",
      "Training progress in epoch #37, step 40, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #37, step 41, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #37, step 42, discriminator loss=0.691 , generator loss=0.660\n",
      "Training progress in epoch #37, step 43, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #37, step 44, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #37, step 45, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #37, step 46, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #37, step 47, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #37, step 48, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #37, step 49, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #37, step 50, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #37, step 51, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #37, step 52, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #37, step 53, discriminator loss=0.683 , generator loss=0.690\n",
      "Training progress in epoch #37, step 54, discriminator loss=0.682 , generator loss=0.716\n",
      "Training progress in epoch #37, step 55, discriminator loss=0.688 , generator loss=0.735\n",
      "Training progress in epoch #37, step 56, discriminator loss=0.685 , generator loss=0.733\n",
      "Training progress in epoch #37, step 57, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #37, step 58, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #37, step 59, discriminator loss=0.697 , generator loss=0.710\n",
      "Training progress in epoch #37, step 60, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #37, step 61, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #37, step 62, discriminator loss=0.693 , generator loss=0.676\n",
      "Training progress in epoch #37, step 63, discriminator loss=0.696 , generator loss=0.662\n",
      "Training progress in epoch #37, step 64, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #37, step 65, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #37, step 66, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #37, step 67, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #37, step 68, discriminator loss=0.696 , generator loss=0.727\n",
      "Training progress in epoch #37, step 69, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #37, step 70, discriminator loss=0.682 , generator loss=0.721\n",
      "Training progress in epoch #37, step 71, discriminator loss=0.688 , generator loss=0.763\n",
      "Training progress in epoch #37, step 72, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #37, step 73, discriminator loss=0.685 , generator loss=0.685\n",
      "Training progress in epoch #37, step 74, discriminator loss=0.685 , generator loss=0.682\n",
      "Training progress in epoch #37, step 75, discriminator loss=0.684 , generator loss=0.683\n",
      "Training progress in epoch #37, step 76, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #37, step 77, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #37, step 78, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #37, step 79, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #37, step 80, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #37, step 81, discriminator loss=0.692 , generator loss=0.736\n",
      "Training progress in epoch #37, step 82, discriminator loss=0.691 , generator loss=0.727\n",
      "Training progress in epoch #37, step 83, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #37, step 84, discriminator loss=0.695 , generator loss=0.682\n",
      "Training progress in epoch #37, step 85, discriminator loss=0.694 , generator loss=0.677\n",
      "Training progress in epoch #37, step 86, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #37, step 87, discriminator loss=0.690 , generator loss=0.671\n",
      "Training progress in epoch #37, step 88, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #37, step 89, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #37, step 90, discriminator loss=0.691 , generator loss=0.758\n",
      "Training progress in epoch #37, step 91, discriminator loss=0.689 , generator loss=0.734\n",
      "Training progress in epoch #37, step 92, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #37, step 93, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #37, step 94, discriminator loss=0.684 , generator loss=0.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #37, step 95, discriminator loss=0.684 , generator loss=0.727\n",
      "Training progress in epoch #37, step 96, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #37, step 97, discriminator loss=0.687 , generator loss=0.673\n",
      "Training progress in epoch #37, step 98, discriminator loss=0.684 , generator loss=0.655\n",
      "Training progress in epoch #37, step 99, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #37, step 100, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #37, step 101, discriminator loss=0.696 , generator loss=0.724\n",
      "Training progress in epoch #37, step 102, discriminator loss=0.693 , generator loss=0.728\n",
      "Training progress in epoch #37, step 103, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #37, step 104, discriminator loss=0.693 , generator loss=0.738\n",
      "Training progress in epoch #37, step 105, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #37, step 106, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #37, step 107, discriminator loss=0.681 , generator loss=0.676\n",
      "Training progress in epoch #37, step 108, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #37, step 109, discriminator loss=0.693 , generator loss=0.677\n",
      "Training progress in epoch #37, step 110, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #37, step 111, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #37, step 112, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #37, step 113, discriminator loss=0.685 , generator loss=0.731\n",
      "Training progress in epoch #37, step 114, discriminator loss=0.681 , generator loss=0.724\n",
      "Training progress in epoch #37, step 115, discriminator loss=0.682 , generator loss=0.722\n",
      "Training progress in epoch #37, step 116, discriminator loss=0.687 , generator loss=0.738\n",
      "Training progress in epoch #37, step 117, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #37, step 118, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #37, step 119, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #37, step 120, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #37, step 121, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #37, step 122, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #37, step 123, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #37, step 124, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #37, step 125, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #37, step 126, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #37, step 127, discriminator loss=0.691 , generator loss=0.747\n",
      "Training progress in epoch #37, step 128, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #37, step 129, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #37, step 130, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #37, step 131, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #37, step 132, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #37, step 133, discriminator loss=0.688 , generator loss=0.674\n",
      "Training progress in epoch #37, step 134, discriminator loss=0.689 , generator loss=0.677\n",
      "Training progress in epoch #37, step 135, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #37, step 136, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #37, step 137, discriminator loss=0.686 , generator loss=0.757\n",
      "Training progress in epoch #37, step 138, discriminator loss=0.694 , generator loss=0.734\n",
      "Training progress in epoch #37, step 139, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #37, step 140, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #37, step 141, discriminator loss=0.681 , generator loss=0.715\n",
      "Training progress in epoch #37, step 142, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #37, step 143, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #37, step 144, discriminator loss=0.687 , generator loss=0.674\n",
      "Training progress in epoch #37, step 145, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #37, step 146, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #37, step 147, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #37, step 148, discriminator loss=0.688 , generator loss=0.733\n",
      "Training progress in epoch #37, step 149, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #37, step 150, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #37, step 151, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #37, step 152, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #37, step 153, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #37, step 154, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #37, step 155, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #37, step 156, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #37, step 157, discriminator loss=0.688 , generator loss=0.671\n",
      "Training progress in epoch #37, step 158, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #37, step 159, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #37, step 160, discriminator loss=0.683 , generator loss=0.701\n",
      "Training progress in epoch #37, step 161, discriminator loss=0.691 , generator loss=0.746\n",
      "Training progress in epoch #37, step 162, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #37, step 163, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #37, step 164, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #37, step 165, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #37, step 166, discriminator loss=0.684 , generator loss=0.699\n",
      "Training progress in epoch #37, step 167, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #37, step 168, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #37, step 169, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #37, step 170, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #37, step 171, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #37, step 172, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #37, step 173, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #37, step 174, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #37, step 175, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #37, step 176, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #37, step 177, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #37, step 178, discriminator loss=0.681 , generator loss=0.709\n",
      "Training progress in epoch #37, step 179, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #37, step 180, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #37, step 181, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #37, step 182, discriminator loss=0.684 , generator loss=0.709\n",
      "Training progress in epoch #37, step 183, discriminator loss=0.685 , generator loss=0.726\n",
      "Training progress in epoch #37, step 184, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #37, step 185, discriminator loss=0.682 , generator loss=0.723\n",
      "Training progress in epoch #37, step 186, discriminator loss=0.696 , generator loss=0.690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #37, step 187, discriminator loss=0.698 , generator loss=0.705\n",
      "Training progress in epoch #37, step 188, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #37, step 189, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #37, step 190, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #37, step 191, discriminator loss=0.682 , generator loss=0.712\n",
      "Training progress in epoch #37, step 192, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #37, step 193, discriminator loss=0.688 , generator loss=0.738\n",
      "Training progress in epoch #37, step 194, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #37, step 195, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #37, step 196, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #37, step 197, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #37, step 198, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #37, step 199, discriminator loss=0.684 , generator loss=0.713\n",
      "Training progress in epoch #37, step 200, discriminator loss=0.684 , generator loss=0.701\n",
      "Training progress in epoch #37, step 201, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #37, step 202, discriminator loss=0.683 , generator loss=0.679\n",
      "Training progress in epoch #37, step 203, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #37, step 204, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #37, step 205, discriminator loss=0.692 , generator loss=0.731\n",
      "Training progress in epoch #37, step 206, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #37, step 207, discriminator loss=0.700 , generator loss=0.719\n",
      "Training progress in epoch #37, step 208, discriminator loss=0.691 , generator loss=0.746\n",
      "Training progress in epoch #37, step 209, discriminator loss=0.691 , generator loss=0.755\n",
      "Training progress in epoch #37, step 210, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #37, step 211, discriminator loss=0.695 , generator loss=0.680\n",
      "Training progress in epoch #37, step 212, discriminator loss=0.694 , generator loss=0.671\n",
      "Training progress in epoch #37, step 213, discriminator loss=0.685 , generator loss=0.679\n",
      "Training progress in epoch #37, step 214, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #37, step 215, discriminator loss=0.698 , generator loss=0.715\n",
      "Training progress in epoch #37, step 216, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #37, step 217, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #37, step 218, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #37, step 219, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #37, step 220, discriminator loss=0.692 , generator loss=0.735\n",
      "Training progress in epoch #37, step 221, discriminator loss=0.697 , generator loss=0.752\n",
      "Training progress in epoch #37, step 222, discriminator loss=0.689 , generator loss=0.735\n",
      "Training progress in epoch #37, step 223, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #37, step 224, discriminator loss=0.689 , generator loss=0.668\n",
      "Training progress in epoch #37, step 225, discriminator loss=0.687 , generator loss=0.664\n",
      "Training progress in epoch #37, step 226, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #37, step 227, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #37, step 228, discriminator loss=0.690 , generator loss=0.738\n",
      "Training progress in epoch #37, step 229, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #37, step 230, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #37, step 231, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #37, step 232, discriminator loss=0.679 , generator loss=0.723\n",
      "Training progress in epoch #37, step 233, discriminator loss=0.681 , generator loss=0.712\n",
      "Disciminator Accuracy on real images: 63%, on fake images: 65%\n",
      "Training progress in epoch #38, step 0, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #38, step 1, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #38, step 2, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #38, step 3, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #38, step 4, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #38, step 5, discriminator loss=0.699 , generator loss=0.723\n",
      "Training progress in epoch #38, step 6, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #38, step 7, discriminator loss=0.697 , generator loss=0.725\n",
      "Training progress in epoch #38, step 8, discriminator loss=0.685 , generator loss=0.714\n",
      "Training progress in epoch #38, step 9, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #38, step 10, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #38, step 11, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #38, step 12, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #38, step 13, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #38, step 14, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #38, step 15, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #38, step 16, discriminator loss=0.687 , generator loss=0.670\n",
      "Training progress in epoch #38, step 17, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #38, step 18, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #38, step 19, discriminator loss=0.682 , generator loss=0.749\n",
      "Training progress in epoch #38, step 20, discriminator loss=0.686 , generator loss=0.728\n",
      "Training progress in epoch #38, step 21, discriminator loss=0.682 , generator loss=0.733\n",
      "Training progress in epoch #38, step 22, discriminator loss=0.683 , generator loss=0.700\n",
      "Training progress in epoch #38, step 23, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #38, step 24, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #38, step 25, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #38, step 26, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #38, step 27, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #38, step 28, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #38, step 29, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #38, step 30, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #38, step 31, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #38, step 32, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #38, step 33, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #38, step 34, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #38, step 35, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #38, step 36, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #38, step 37, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #38, step 38, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #38, step 39, discriminator loss=0.684 , generator loss=0.709\n",
      "Training progress in epoch #38, step 40, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #38, step 41, discriminator loss=0.684 , generator loss=0.688\n",
      "Training progress in epoch #38, step 42, discriminator loss=0.683 , generator loss=0.680\n",
      "Training progress in epoch #38, step 43, discriminator loss=0.688 , generator loss=0.717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #38, step 44, discriminator loss=0.684 , generator loss=0.716\n",
      "Training progress in epoch #38, step 45, discriminator loss=0.682 , generator loss=0.736\n",
      "Training progress in epoch #38, step 46, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #38, step 47, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #38, step 48, discriminator loss=0.689 , generator loss=0.727\n",
      "Training progress in epoch #38, step 49, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #38, step 50, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #38, step 51, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #38, step 52, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #38, step 53, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #38, step 54, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #38, step 55, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #38, step 56, discriminator loss=0.688 , generator loss=0.737\n",
      "Training progress in epoch #38, step 57, discriminator loss=0.691 , generator loss=0.739\n",
      "Training progress in epoch #38, step 58, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #38, step 59, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #38, step 60, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #38, step 61, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #38, step 62, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #38, step 63, discriminator loss=0.689 , generator loss=0.658\n",
      "Training progress in epoch #38, step 64, discriminator loss=0.686 , generator loss=0.681\n",
      "Training progress in epoch #38, step 65, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #38, step 66, discriminator loss=0.684 , generator loss=0.733\n",
      "Training progress in epoch #38, step 67, discriminator loss=0.696 , generator loss=0.723\n",
      "Training progress in epoch #38, step 68, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #38, step 69, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #38, step 70, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #38, step 71, discriminator loss=0.699 , generator loss=0.740\n",
      "Training progress in epoch #38, step 72, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #38, step 73, discriminator loss=0.688 , generator loss=0.682\n",
      "Training progress in epoch #38, step 74, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #38, step 75, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #38, step 76, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #38, step 77, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #38, step 78, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #38, step 79, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #38, step 80, discriminator loss=0.693 , generator loss=0.728\n",
      "Training progress in epoch #38, step 81, discriminator loss=0.692 , generator loss=0.736\n",
      "Training progress in epoch #38, step 82, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #38, step 83, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #38, step 84, discriminator loss=0.685 , generator loss=0.684\n",
      "Training progress in epoch #38, step 85, discriminator loss=0.691 , generator loss=0.657\n",
      "Training progress in epoch #38, step 86, discriminator loss=0.692 , generator loss=0.664\n",
      "Training progress in epoch #38, step 87, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #38, step 88, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #38, step 89, discriminator loss=0.693 , generator loss=0.754\n",
      "Training progress in epoch #38, step 90, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #38, step 91, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #38, step 92, discriminator loss=0.699 , generator loss=0.719\n",
      "Training progress in epoch #38, step 93, discriminator loss=0.684 , generator loss=0.743\n",
      "Training progress in epoch #38, step 94, discriminator loss=0.685 , generator loss=0.721\n",
      "Training progress in epoch #38, step 95, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #38, step 96, discriminator loss=0.685 , generator loss=0.662\n",
      "Training progress in epoch #38, step 97, discriminator loss=0.686 , generator loss=0.671\n",
      "Training progress in epoch #38, step 98, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #38, step 99, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #38, step 100, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #38, step 101, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #38, step 102, discriminator loss=0.686 , generator loss=0.737\n",
      "Training progress in epoch #38, step 103, discriminator loss=0.683 , generator loss=0.750\n",
      "Training progress in epoch #38, step 104, discriminator loss=0.688 , generator loss=0.767\n",
      "Training progress in epoch #38, step 105, discriminator loss=0.683 , generator loss=0.720\n",
      "Training progress in epoch #38, step 106, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #38, step 107, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #38, step 108, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #38, step 109, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #38, step 110, discriminator loss=0.688 , generator loss=0.665\n",
      "Training progress in epoch #38, step 111, discriminator loss=0.693 , generator loss=0.674\n",
      "Training progress in epoch #38, step 112, discriminator loss=0.684 , generator loss=0.723\n",
      "Training progress in epoch #38, step 113, discriminator loss=0.688 , generator loss=0.753\n",
      "Training progress in epoch #38, step 114, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #38, step 115, discriminator loss=0.699 , generator loss=0.725\n",
      "Training progress in epoch #38, step 116, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #38, step 117, discriminator loss=0.688 , generator loss=0.725\n",
      "Training progress in epoch #38, step 118, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #38, step 119, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #38, step 120, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #38, step 121, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #38, step 122, discriminator loss=0.690 , generator loss=0.669\n",
      "Training progress in epoch #38, step 123, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #38, step 124, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #38, step 125, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #38, step 126, discriminator loss=0.696 , generator loss=0.724\n",
      "Training progress in epoch #38, step 127, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #38, step 128, discriminator loss=0.696 , generator loss=0.729\n",
      "Training progress in epoch #38, step 129, discriminator loss=0.696 , generator loss=0.727\n",
      "Training progress in epoch #38, step 130, discriminator loss=0.697 , generator loss=0.710\n",
      "Training progress in epoch #38, step 131, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #38, step 132, discriminator loss=0.698 , generator loss=0.683\n",
      "Training progress in epoch #38, step 133, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #38, step 134, discriminator loss=0.684 , generator loss=0.694\n",
      "Training progress in epoch #38, step 135, discriminator loss=0.689 , generator loss=0.703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #38, step 136, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #38, step 137, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #38, step 138, discriminator loss=0.696 , generator loss=0.721\n",
      "Training progress in epoch #38, step 139, discriminator loss=0.686 , generator loss=0.728\n",
      "Training progress in epoch #38, step 140, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #38, step 141, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #38, step 142, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #38, step 143, discriminator loss=0.679 , generator loss=0.701\n",
      "Training progress in epoch #38, step 144, discriminator loss=0.696 , generator loss=0.679\n",
      "Training progress in epoch #38, step 145, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #38, step 146, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #38, step 147, discriminator loss=0.682 , generator loss=0.706\n",
      "Training progress in epoch #38, step 148, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #38, step 149, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #38, step 150, discriminator loss=0.697 , generator loss=0.695\n",
      "Training progress in epoch #38, step 151, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #38, step 152, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #38, step 153, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #38, step 154, discriminator loss=0.689 , generator loss=0.728\n",
      "Training progress in epoch #38, step 155, discriminator loss=0.690 , generator loss=0.738\n",
      "Training progress in epoch #38, step 156, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #38, step 157, discriminator loss=0.694 , generator loss=0.724\n",
      "Training progress in epoch #38, step 158, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #38, step 159, discriminator loss=0.687 , generator loss=0.669\n",
      "Training progress in epoch #38, step 160, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #38, step 161, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #38, step 162, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #38, step 163, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #38, step 164, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #38, step 165, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #38, step 166, discriminator loss=0.693 , generator loss=0.730\n",
      "Training progress in epoch #38, step 167, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #38, step 168, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #38, step 169, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #38, step 170, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #38, step 171, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #38, step 172, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #38, step 173, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #38, step 174, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #38, step 175, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #38, step 176, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #38, step 177, discriminator loss=0.698 , generator loss=0.719\n",
      "Training progress in epoch #38, step 178, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #38, step 179, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #38, step 180, discriminator loss=0.683 , generator loss=0.677\n",
      "Training progress in epoch #38, step 181, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #38, step 182, discriminator loss=0.681 , generator loss=0.703\n",
      "Training progress in epoch #38, step 183, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #38, step 184, discriminator loss=0.682 , generator loss=0.709\n",
      "Training progress in epoch #38, step 185, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #38, step 186, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #38, step 187, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #38, step 188, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #38, step 189, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #38, step 190, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #38, step 191, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #38, step 192, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #38, step 193, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #38, step 194, discriminator loss=0.686 , generator loss=0.726\n",
      "Training progress in epoch #38, step 195, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #38, step 196, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #38, step 197, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #38, step 198, discriminator loss=0.679 , generator loss=0.705\n",
      "Training progress in epoch #38, step 199, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #38, step 200, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #38, step 201, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #38, step 202, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #38, step 203, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #38, step 204, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #38, step 205, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #38, step 206, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #38, step 207, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #38, step 208, discriminator loss=0.685 , generator loss=0.698\n",
      "Training progress in epoch #38, step 209, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #38, step 210, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #38, step 211, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #38, step 212, discriminator loss=0.690 , generator loss=0.678\n",
      "Training progress in epoch #38, step 213, discriminator loss=0.697 , generator loss=0.710\n",
      "Training progress in epoch #38, step 214, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #38, step 215, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #38, step 216, discriminator loss=0.689 , generator loss=0.735\n",
      "Training progress in epoch #38, step 217, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #38, step 218, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #38, step 219, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #38, step 220, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #38, step 221, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #38, step 222, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #38, step 223, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #38, step 224, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #38, step 225, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #38, step 226, discriminator loss=0.680 , generator loss=0.712\n",
      "Training progress in epoch #38, step 227, discriminator loss=0.691 , generator loss=0.724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #38, step 228, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #38, step 229, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #38, step 230, discriminator loss=0.685 , generator loss=0.730\n",
      "Training progress in epoch #38, step 231, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #38, step 232, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #38, step 233, discriminator loss=0.693 , generator loss=0.678\n",
      "Disciminator Accuracy on real images: 83%, on fake images: 20%\n",
      "Training progress in epoch #39, step 0, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #39, step 1, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #39, step 2, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #39, step 3, discriminator loss=0.696 , generator loss=0.707\n",
      "Training progress in epoch #39, step 4, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #39, step 5, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #39, step 6, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #39, step 7, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #39, step 8, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #39, step 9, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #39, step 10, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #39, step 11, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #39, step 12, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #39, step 13, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #39, step 14, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #39, step 15, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #39, step 16, discriminator loss=0.685 , generator loss=0.718\n",
      "Training progress in epoch #39, step 17, discriminator loss=0.688 , generator loss=0.736\n",
      "Training progress in epoch #39, step 18, discriminator loss=0.687 , generator loss=0.740\n",
      "Training progress in epoch #39, step 19, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #39, step 20, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #39, step 21, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #39, step 22, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #39, step 23, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #39, step 24, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #39, step 25, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #39, step 26, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #39, step 27, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #39, step 28, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #39, step 29, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #39, step 30, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #39, step 31, discriminator loss=0.697 , generator loss=0.704\n",
      "Training progress in epoch #39, step 32, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #39, step 33, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #39, step 34, discriminator loss=0.684 , generator loss=0.731\n",
      "Training progress in epoch #39, step 35, discriminator loss=0.690 , generator loss=0.740\n",
      "Training progress in epoch #39, step 36, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #39, step 37, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #39, step 38, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #39, step 39, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #39, step 40, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #39, step 41, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #39, step 42, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #39, step 43, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #39, step 44, discriminator loss=0.698 , generator loss=0.736\n",
      "Training progress in epoch #39, step 45, discriminator loss=0.693 , generator loss=0.738\n",
      "Training progress in epoch #39, step 46, discriminator loss=0.691 , generator loss=0.737\n",
      "Training progress in epoch #39, step 47, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #39, step 48, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #39, step 49, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #39, step 50, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #39, step 51, discriminator loss=0.684 , generator loss=0.697\n",
      "Training progress in epoch #39, step 52, discriminator loss=0.681 , generator loss=0.691\n",
      "Training progress in epoch #39, step 53, discriminator loss=0.683 , generator loss=0.698\n",
      "Training progress in epoch #39, step 54, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #39, step 55, discriminator loss=0.688 , generator loss=0.736\n",
      "Training progress in epoch #39, step 56, discriminator loss=0.684 , generator loss=0.731\n",
      "Training progress in epoch #39, step 57, discriminator loss=0.688 , generator loss=0.737\n",
      "Training progress in epoch #39, step 58, discriminator loss=0.689 , generator loss=0.747\n",
      "Training progress in epoch #39, step 59, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #39, step 60, discriminator loss=0.688 , generator loss=0.674\n",
      "Training progress in epoch #39, step 61, discriminator loss=0.696 , generator loss=0.658\n",
      "Training progress in epoch #39, step 62, discriminator loss=0.692 , generator loss=0.674\n",
      "Training progress in epoch #39, step 63, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #39, step 64, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #39, step 65, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #39, step 66, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #39, step 67, discriminator loss=0.684 , generator loss=0.703\n",
      "Training progress in epoch #39, step 68, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #39, step 69, discriminator loss=0.691 , generator loss=0.735\n",
      "Training progress in epoch #39, step 70, discriminator loss=0.696 , generator loss=0.732\n",
      "Training progress in epoch #39, step 71, discriminator loss=0.683 , generator loss=0.745\n",
      "Training progress in epoch #39, step 72, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #39, step 73, discriminator loss=0.696 , generator loss=0.665\n",
      "Training progress in epoch #39, step 74, discriminator loss=0.688 , generator loss=0.661\n",
      "Training progress in epoch #39, step 75, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #39, step 76, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #39, step 77, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #39, step 78, discriminator loss=0.692 , generator loss=0.731\n",
      "Training progress in epoch #39, step 79, discriminator loss=0.688 , generator loss=0.732\n",
      "Training progress in epoch #39, step 80, discriminator loss=0.682 , generator loss=0.700\n",
      "Training progress in epoch #39, step 81, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #39, step 82, discriminator loss=0.681 , generator loss=0.696\n",
      "Training progress in epoch #39, step 83, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #39, step 84, discriminator loss=0.683 , generator loss=0.722\n",
      "Training progress in epoch #39, step 85, discriminator loss=0.687 , generator loss=0.717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #39, step 86, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #39, step 87, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #39, step 88, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #39, step 89, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #39, step 90, discriminator loss=0.700 , generator loss=0.709\n",
      "Training progress in epoch #39, step 91, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #39, step 92, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #39, step 93, discriminator loss=0.688 , generator loss=0.732\n",
      "Training progress in epoch #39, step 94, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #39, step 95, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #39, step 96, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #39, step 97, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #39, step 98, discriminator loss=0.687 , generator loss=0.725\n",
      "Training progress in epoch #39, step 99, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #39, step 100, discriminator loss=0.685 , generator loss=0.679\n",
      "Training progress in epoch #39, step 101, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #39, step 102, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #39, step 103, discriminator loss=0.680 , generator loss=0.740\n",
      "Training progress in epoch #39, step 104, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #39, step 105, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #39, step 106, discriminator loss=0.692 , generator loss=0.672\n",
      "Training progress in epoch #39, step 107, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #39, step 108, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #39, step 109, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #39, step 110, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #39, step 111, discriminator loss=0.697 , generator loss=0.720\n",
      "Training progress in epoch #39, step 112, discriminator loss=0.687 , generator loss=0.750\n",
      "Training progress in epoch #39, step 113, discriminator loss=0.692 , generator loss=0.743\n",
      "Training progress in epoch #39, step 114, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #39, step 115, discriminator loss=0.682 , generator loss=0.692\n",
      "Training progress in epoch #39, step 116, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #39, step 117, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #39, step 118, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #39, step 119, discriminator loss=0.691 , generator loss=0.676\n",
      "Training progress in epoch #39, step 120, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #39, step 121, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #39, step 122, discriminator loss=0.691 , generator loss=0.752\n",
      "Training progress in epoch #39, step 123, discriminator loss=0.692 , generator loss=0.754\n",
      "Training progress in epoch #39, step 124, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #39, step 125, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #39, step 126, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #39, step 127, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #39, step 128, discriminator loss=0.689 , generator loss=0.657\n",
      "Training progress in epoch #39, step 129, discriminator loss=0.687 , generator loss=0.667\n",
      "Training progress in epoch #39, step 130, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #39, step 131, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #39, step 132, discriminator loss=0.692 , generator loss=0.753\n",
      "Training progress in epoch #39, step 133, discriminator loss=0.697 , generator loss=0.741\n",
      "Training progress in epoch #39, step 134, discriminator loss=0.697 , generator loss=0.713\n",
      "Training progress in epoch #39, step 135, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #39, step 136, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #39, step 137, discriminator loss=0.685 , generator loss=0.701\n",
      "Training progress in epoch #39, step 138, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #39, step 139, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #39, step 140, discriminator loss=0.685 , generator loss=0.683\n",
      "Training progress in epoch #39, step 141, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #39, step 142, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #39, step 143, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #39, step 144, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #39, step 145, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #39, step 146, discriminator loss=0.689 , generator loss=0.745\n",
      "Training progress in epoch #39, step 147, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #39, step 148, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #39, step 149, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #39, step 150, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #39, step 151, discriminator loss=0.696 , generator loss=0.706\n",
      "Training progress in epoch #39, step 152, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #39, step 153, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #39, step 154, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #39, step 155, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #39, step 156, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #39, step 157, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #39, step 158, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #39, step 159, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #39, step 160, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #39, step 161, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #39, step 162, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #39, step 163, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #39, step 164, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #39, step 165, discriminator loss=0.681 , generator loss=0.700\n",
      "Training progress in epoch #39, step 166, discriminator loss=0.683 , generator loss=0.693\n",
      "Training progress in epoch #39, step 167, discriminator loss=0.682 , generator loss=0.692\n",
      "Training progress in epoch #39, step 168, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #39, step 169, discriminator loss=0.691 , generator loss=0.743\n",
      "Training progress in epoch #39, step 170, discriminator loss=0.687 , generator loss=0.735\n",
      "Training progress in epoch #39, step 171, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #39, step 172, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #39, step 173, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #39, step 174, discriminator loss=0.691 , generator loss=0.676\n",
      "Training progress in epoch #39, step 175, discriminator loss=0.688 , generator loss=0.680\n",
      "Training progress in epoch #39, step 176, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #39, step 177, discriminator loss=0.686 , generator loss=0.712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #39, step 178, discriminator loss=0.691 , generator loss=0.723\n",
      "Training progress in epoch #39, step 179, discriminator loss=0.693 , generator loss=0.728\n",
      "Training progress in epoch #39, step 180, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #39, step 181, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #39, step 182, discriminator loss=0.683 , generator loss=0.708\n",
      "Training progress in epoch #39, step 183, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #39, step 184, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #39, step 185, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #39, step 186, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #39, step 187, discriminator loss=0.684 , generator loss=0.732\n",
      "Training progress in epoch #39, step 188, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #39, step 189, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #39, step 190, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #39, step 191, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #39, step 192, discriminator loss=0.683 , generator loss=0.715\n",
      "Training progress in epoch #39, step 193, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #39, step 194, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #39, step 195, discriminator loss=0.691 , generator loss=0.733\n",
      "Training progress in epoch #39, step 196, discriminator loss=0.695 , generator loss=0.715\n",
      "Training progress in epoch #39, step 197, discriminator loss=0.697 , generator loss=0.726\n",
      "Training progress in epoch #39, step 198, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #39, step 199, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #39, step 200, discriminator loss=0.689 , generator loss=0.676\n",
      "Training progress in epoch #39, step 201, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #39, step 202, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #39, step 203, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #39, step 204, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #39, step 205, discriminator loss=0.695 , generator loss=0.723\n",
      "Training progress in epoch #39, step 206, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #39, step 207, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #39, step 208, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #39, step 209, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #39, step 210, discriminator loss=0.684 , generator loss=0.725\n",
      "Training progress in epoch #39, step 211, discriminator loss=0.684 , generator loss=0.697\n",
      "Training progress in epoch #39, step 212, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #39, step 213, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #39, step 214, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #39, step 215, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #39, step 216, discriminator loss=0.682 , generator loss=0.699\n",
      "Training progress in epoch #39, step 217, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #39, step 218, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #39, step 219, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #39, step 220, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #39, step 221, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #39, step 222, discriminator loss=0.697 , generator loss=0.704\n",
      "Training progress in epoch #39, step 223, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #39, step 224, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #39, step 225, discriminator loss=0.687 , generator loss=0.720\n",
      "Training progress in epoch #39, step 226, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #39, step 227, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #39, step 228, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #39, step 229, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #39, step 230, discriminator loss=0.681 , generator loss=0.711\n",
      "Training progress in epoch #39, step 231, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #39, step 232, discriminator loss=0.684 , generator loss=0.705\n",
      "Training progress in epoch #39, step 233, discriminator loss=0.682 , generator loss=0.719\n",
      "Disciminator Accuracy on real images: 52%, on fake images: 81%\n",
      "Training progress in epoch #40, step 0, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #40, step 1, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #40, step 2, discriminator loss=0.695 , generator loss=0.689\n",
      "Training progress in epoch #40, step 3, discriminator loss=0.684 , generator loss=0.720\n",
      "Training progress in epoch #40, step 4, discriminator loss=0.692 , generator loss=0.732\n",
      "Training progress in epoch #40, step 5, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #40, step 6, discriminator loss=0.687 , generator loss=0.665\n",
      "Training progress in epoch #40, step 7, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #40, step 8, discriminator loss=0.698 , generator loss=0.720\n",
      "Training progress in epoch #40, step 9, discriminator loss=0.687 , generator loss=0.739\n",
      "Training progress in epoch #40, step 10, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #40, step 11, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #40, step 12, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #40, step 13, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #40, step 14, discriminator loss=0.696 , generator loss=0.713\n",
      "Training progress in epoch #40, step 15, discriminator loss=0.680 , generator loss=0.687\n",
      "Training progress in epoch #40, step 16, discriminator loss=0.694 , generator loss=0.665\n",
      "Training progress in epoch #40, step 17, discriminator loss=0.688 , generator loss=0.682\n",
      "Training progress in epoch #40, step 18, discriminator loss=0.693 , generator loss=0.729\n",
      "Training progress in epoch #40, step 19, discriminator loss=0.698 , generator loss=0.712\n",
      "Training progress in epoch #40, step 20, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #40, step 21, discriminator loss=0.680 , generator loss=0.726\n",
      "Training progress in epoch #40, step 22, discriminator loss=0.685 , generator loss=0.739\n",
      "Training progress in epoch #40, step 23, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #40, step 24, discriminator loss=0.687 , generator loss=0.658\n",
      "Training progress in epoch #40, step 25, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #40, step 26, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #40, step 27, discriminator loss=0.686 , generator loss=0.744\n",
      "Training progress in epoch #40, step 28, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #40, step 29, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #40, step 30, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #40, step 31, discriminator loss=0.683 , generator loss=0.733\n",
      "Training progress in epoch #40, step 32, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #40, step 33, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #40, step 34, discriminator loss=0.692 , generator loss=0.678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #40, step 35, discriminator loss=0.696 , generator loss=0.686\n",
      "Training progress in epoch #40, step 36, discriminator loss=0.697 , generator loss=0.716\n",
      "Training progress in epoch #40, step 37, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #40, step 38, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #40, step 39, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #40, step 40, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #40, step 41, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #40, step 42, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #40, step 43, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #40, step 44, discriminator loss=0.686 , generator loss=0.735\n",
      "Training progress in epoch #40, step 45, discriminator loss=0.683 , generator loss=0.713\n",
      "Training progress in epoch #40, step 46, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #40, step 47, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #40, step 48, discriminator loss=0.684 , generator loss=0.687\n",
      "Training progress in epoch #40, step 49, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #40, step 50, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #40, step 51, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #40, step 52, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #40, step 53, discriminator loss=0.691 , generator loss=0.734\n",
      "Training progress in epoch #40, step 54, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #40, step 55, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #40, step 56, discriminator loss=0.697 , generator loss=0.706\n",
      "Training progress in epoch #40, step 57, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #40, step 58, discriminator loss=0.695 , generator loss=0.681\n",
      "Training progress in epoch #40, step 59, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #40, step 60, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #40, step 61, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #40, step 62, discriminator loss=0.696 , generator loss=0.722\n",
      "Training progress in epoch #40, step 63, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #40, step 64, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #40, step 65, discriminator loss=0.685 , generator loss=0.682\n",
      "Training progress in epoch #40, step 66, discriminator loss=0.682 , generator loss=0.718\n",
      "Training progress in epoch #40, step 67, discriminator loss=0.685 , generator loss=0.740\n",
      "Training progress in epoch #40, step 68, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #40, step 69, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #40, step 70, discriminator loss=0.682 , generator loss=0.696\n",
      "Training progress in epoch #40, step 71, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #40, step 72, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #40, step 73, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #40, step 74, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #40, step 75, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #40, step 76, discriminator loss=0.686 , generator loss=0.733\n",
      "Training progress in epoch #40, step 77, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #40, step 78, discriminator loss=0.689 , generator loss=0.675\n",
      "Training progress in epoch #40, step 79, discriminator loss=0.689 , generator loss=0.676\n",
      "Training progress in epoch #40, step 80, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #40, step 81, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #40, step 82, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #40, step 83, discriminator loss=0.685 , generator loss=0.727\n",
      "Training progress in epoch #40, step 84, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #40, step 85, discriminator loss=0.683 , generator loss=0.733\n",
      "Training progress in epoch #40, step 86, discriminator loss=0.695 , generator loss=0.726\n",
      "Training progress in epoch #40, step 87, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #40, step 88, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #40, step 89, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #40, step 90, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #40, step 91, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #40, step 92, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #40, step 93, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #40, step 94, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #40, step 95, discriminator loss=0.678 , generator loss=0.731\n",
      "Training progress in epoch #40, step 96, discriminator loss=0.685 , generator loss=0.691\n",
      "Training progress in epoch #40, step 97, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #40, step 98, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #40, step 99, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #40, step 100, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #40, step 101, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #40, step 102, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #40, step 103, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #40, step 104, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #40, step 105, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #40, step 106, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #40, step 107, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #40, step 108, discriminator loss=0.697 , generator loss=0.708\n",
      "Training progress in epoch #40, step 109, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #40, step 110, discriminator loss=0.684 , generator loss=0.705\n",
      "Training progress in epoch #40, step 111, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #40, step 112, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #40, step 113, discriminator loss=0.682 , generator loss=0.697\n",
      "Training progress in epoch #40, step 114, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #40, step 115, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #40, step 116, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #40, step 117, discriminator loss=0.684 , generator loss=0.720\n",
      "Training progress in epoch #40, step 118, discriminator loss=0.687 , generator loss=0.736\n",
      "Training progress in epoch #40, step 119, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #40, step 120, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #40, step 121, discriminator loss=0.680 , generator loss=0.686\n",
      "Training progress in epoch #40, step 122, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #40, step 123, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #40, step 124, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #40, step 125, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #40, step 126, discriminator loss=0.688 , generator loss=0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #40, step 127, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #40, step 128, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #40, step 129, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #40, step 130, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #40, step 131, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #40, step 132, discriminator loss=0.687 , generator loss=0.720\n",
      "Training progress in epoch #40, step 133, discriminator loss=0.696 , generator loss=0.672\n",
      "Training progress in epoch #40, step 134, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #40, step 135, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #40, step 136, discriminator loss=0.685 , generator loss=0.734\n",
      "Training progress in epoch #40, step 137, discriminator loss=0.684 , generator loss=0.721\n",
      "Training progress in epoch #40, step 138, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #40, step 139, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #40, step 140, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #40, step 141, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #40, step 142, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #40, step 143, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #40, step 144, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #40, step 145, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #40, step 146, discriminator loss=0.684 , generator loss=0.714\n",
      "Training progress in epoch #40, step 147, discriminator loss=0.698 , generator loss=0.707\n",
      "Training progress in epoch #40, step 148, discriminator loss=0.684 , generator loss=0.696\n",
      "Training progress in epoch #40, step 149, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #40, step 150, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #40, step 151, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #40, step 152, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #40, step 153, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #40, step 154, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #40, step 155, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #40, step 156, discriminator loss=0.684 , generator loss=0.701\n",
      "Training progress in epoch #40, step 157, discriminator loss=0.685 , generator loss=0.709\n",
      "Training progress in epoch #40, step 158, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #40, step 159, discriminator loss=0.682 , generator loss=0.721\n",
      "Training progress in epoch #40, step 160, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #40, step 161, discriminator loss=0.698 , generator loss=0.695\n",
      "Training progress in epoch #40, step 162, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #40, step 163, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #40, step 164, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #40, step 165, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #40, step 166, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #40, step 167, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #40, step 168, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #40, step 169, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #40, step 170, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #40, step 171, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #40, step 172, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #40, step 173, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #40, step 174, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #40, step 175, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #40, step 176, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #40, step 177, discriminator loss=0.682 , generator loss=0.703\n",
      "Training progress in epoch #40, step 178, discriminator loss=0.691 , generator loss=0.727\n",
      "Training progress in epoch #40, step 179, discriminator loss=0.697 , generator loss=0.737\n",
      "Training progress in epoch #40, step 180, discriminator loss=0.686 , generator loss=0.738\n",
      "Training progress in epoch #40, step 181, discriminator loss=0.700 , generator loss=0.723\n",
      "Training progress in epoch #40, step 182, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #40, step 183, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #40, step 184, discriminator loss=0.696 , generator loss=0.707\n",
      "Training progress in epoch #40, step 185, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #40, step 186, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #40, step 187, discriminator loss=0.684 , generator loss=0.680\n",
      "Training progress in epoch #40, step 188, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #40, step 189, discriminator loss=0.691 , generator loss=0.742\n",
      "Training progress in epoch #40, step 190, discriminator loss=0.685 , generator loss=0.743\n",
      "Training progress in epoch #40, step 191, discriminator loss=0.678 , generator loss=0.754\n",
      "Training progress in epoch #40, step 192, discriminator loss=0.682 , generator loss=0.756\n",
      "Training progress in epoch #40, step 193, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #40, step 194, discriminator loss=0.689 , generator loss=0.676\n",
      "Training progress in epoch #40, step 195, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #40, step 196, discriminator loss=0.688 , generator loss=0.724\n",
      "Training progress in epoch #40, step 197, discriminator loss=0.681 , generator loss=0.734\n",
      "Training progress in epoch #40, step 198, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #40, step 199, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #40, step 200, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #40, step 201, discriminator loss=0.701 , generator loss=0.705\n",
      "Training progress in epoch #40, step 202, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #40, step 203, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #40, step 204, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #40, step 205, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #40, step 206, discriminator loss=0.693 , generator loss=0.729\n",
      "Training progress in epoch #40, step 207, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #40, step 208, discriminator loss=0.679 , generator loss=0.713\n",
      "Training progress in epoch #40, step 209, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #40, step 210, discriminator loss=0.683 , generator loss=0.702\n",
      "Training progress in epoch #40, step 211, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #40, step 212, discriminator loss=0.679 , generator loss=0.717\n",
      "Training progress in epoch #40, step 213, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #40, step 214, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #40, step 215, discriminator loss=0.695 , generator loss=0.738\n",
      "Training progress in epoch #40, step 216, discriminator loss=0.689 , generator loss=0.750\n",
      "Training progress in epoch #40, step 217, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #40, step 218, discriminator loss=0.690 , generator loss=0.683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #40, step 219, discriminator loss=0.699 , generator loss=0.692\n",
      "Training progress in epoch #40, step 220, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #40, step 221, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #40, step 222, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #40, step 223, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #40, step 224, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #40, step 225, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #40, step 226, discriminator loss=0.691 , generator loss=0.731\n",
      "Training progress in epoch #40, step 227, discriminator loss=0.681 , generator loss=0.747\n",
      "Training progress in epoch #40, step 228, discriminator loss=0.678 , generator loss=0.740\n",
      "Training progress in epoch #40, step 229, discriminator loss=0.684 , generator loss=0.741\n",
      "Training progress in epoch #40, step 230, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #40, step 231, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #40, step 232, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #40, step 233, discriminator loss=0.682 , generator loss=0.720\n",
      "Disciminator Accuracy on real images: 76%, on fake images: 71%\n",
      "Training progress in epoch #41, step 0, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #41, step 1, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #41, step 2, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #41, step 3, discriminator loss=0.696 , generator loss=0.732\n",
      "Training progress in epoch #41, step 4, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #41, step 5, discriminator loss=0.684 , generator loss=0.735\n",
      "Training progress in epoch #41, step 6, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #41, step 7, discriminator loss=0.693 , generator loss=0.726\n",
      "Training progress in epoch #41, step 8, discriminator loss=0.702 , generator loss=0.719\n",
      "Training progress in epoch #41, step 9, discriminator loss=0.700 , generator loss=0.715\n",
      "Training progress in epoch #41, step 10, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #41, step 11, discriminator loss=0.684 , generator loss=0.733\n",
      "Training progress in epoch #41, step 12, discriminator loss=0.679 , generator loss=0.731\n",
      "Training progress in epoch #41, step 13, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #41, step 14, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #41, step 15, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #41, step 16, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #41, step 17, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #41, step 18, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #41, step 19, discriminator loss=0.699 , generator loss=0.738\n",
      "Training progress in epoch #41, step 20, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #41, step 21, discriminator loss=0.693 , generator loss=0.739\n",
      "Training progress in epoch #41, step 22, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #41, step 23, discriminator loss=0.679 , generator loss=0.747\n",
      "Training progress in epoch #41, step 24, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #41, step 25, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #41, step 26, discriminator loss=0.697 , generator loss=0.682\n",
      "Training progress in epoch #41, step 27, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #41, step 28, discriminator loss=0.686 , generator loss=0.681\n",
      "Training progress in epoch #41, step 29, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #41, step 30, discriminator loss=0.697 , generator loss=0.716\n",
      "Training progress in epoch #41, step 31, discriminator loss=0.683 , generator loss=0.730\n",
      "Training progress in epoch #41, step 32, discriminator loss=0.689 , generator loss=0.748\n",
      "Training progress in epoch #41, step 33, discriminator loss=0.688 , generator loss=0.736\n",
      "Training progress in epoch #41, step 34, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #41, step 35, discriminator loss=0.696 , generator loss=0.730\n",
      "Training progress in epoch #41, step 36, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #41, step 37, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #41, step 38, discriminator loss=0.693 , generator loss=0.672\n",
      "Training progress in epoch #41, step 39, discriminator loss=0.687 , generator loss=0.680\n",
      "Training progress in epoch #41, step 40, discriminator loss=0.679 , generator loss=0.712\n",
      "Training progress in epoch #41, step 41, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #41, step 42, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #41, step 43, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #41, step 44, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #41, step 45, discriminator loss=0.690 , generator loss=0.755\n",
      "Training progress in epoch #41, step 46, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #41, step 47, discriminator loss=0.687 , generator loss=0.676\n",
      "Training progress in epoch #41, step 48, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #41, step 49, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #41, step 50, discriminator loss=0.680 , generator loss=0.706\n",
      "Training progress in epoch #41, step 51, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #41, step 52, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #41, step 53, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #41, step 54, discriminator loss=0.683 , generator loss=0.713\n",
      "Training progress in epoch #41, step 55, discriminator loss=0.686 , generator loss=0.737\n",
      "Training progress in epoch #41, step 56, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #41, step 57, discriminator loss=0.692 , generator loss=0.672\n",
      "Training progress in epoch #41, step 58, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #41, step 59, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #41, step 60, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #41, step 61, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #41, step 62, discriminator loss=0.696 , generator loss=0.721\n",
      "Training progress in epoch #41, step 63, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #41, step 64, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #41, step 65, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #41, step 66, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #41, step 67, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #41, step 68, discriminator loss=0.698 , generator loss=0.716\n",
      "Training progress in epoch #41, step 69, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #41, step 70, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #41, step 71, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #41, step 72, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #41, step 73, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #41, step 74, discriminator loss=0.689 , generator loss=0.743\n",
      "Training progress in epoch #41, step 75, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #41, step 76, discriminator loss=0.684 , generator loss=0.712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #41, step 77, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #41, step 78, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #41, step 79, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #41, step 80, discriminator loss=0.684 , generator loss=0.703\n",
      "Training progress in epoch #41, step 81, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #41, step 82, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #41, step 83, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #41, step 84, discriminator loss=0.696 , generator loss=0.684\n",
      "Training progress in epoch #41, step 85, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #41, step 86, discriminator loss=0.696 , generator loss=0.729\n",
      "Training progress in epoch #41, step 87, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #41, step 88, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #41, step 89, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #41, step 90, discriminator loss=0.687 , generator loss=0.726\n",
      "Training progress in epoch #41, step 91, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #41, step 92, discriminator loss=0.687 , generator loss=0.679\n",
      "Training progress in epoch #41, step 93, discriminator loss=0.684 , generator loss=0.701\n",
      "Training progress in epoch #41, step 94, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #41, step 95, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #41, step 96, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #41, step 97, discriminator loss=0.687 , generator loss=0.682\n",
      "Training progress in epoch #41, step 98, discriminator loss=0.687 , generator loss=0.676\n",
      "Training progress in epoch #41, step 99, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #41, step 100, discriminator loss=0.695 , generator loss=0.736\n",
      "Training progress in epoch #41, step 101, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #41, step 102, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #41, step 103, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #41, step 104, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #41, step 105, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #41, step 106, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #41, step 107, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #41, step 108, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #41, step 109, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #41, step 110, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #41, step 111, discriminator loss=0.693 , generator loss=0.741\n",
      "Training progress in epoch #41, step 112, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #41, step 113, discriminator loss=0.682 , generator loss=0.711\n",
      "Training progress in epoch #41, step 114, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #41, step 115, discriminator loss=0.682 , generator loss=0.698\n",
      "Training progress in epoch #41, step 116, discriminator loss=0.683 , generator loss=0.699\n",
      "Training progress in epoch #41, step 117, discriminator loss=0.682 , generator loss=0.699\n",
      "Training progress in epoch #41, step 118, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #41, step 119, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #41, step 120, discriminator loss=0.700 , generator loss=0.730\n",
      "Training progress in epoch #41, step 121, discriminator loss=0.699 , generator loss=0.727\n",
      "Training progress in epoch #41, step 122, discriminator loss=0.697 , generator loss=0.706\n",
      "Training progress in epoch #41, step 123, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #41, step 124, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #41, step 125, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #41, step 126, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #41, step 127, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #41, step 128, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #41, step 129, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #41, step 130, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #41, step 131, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #41, step 132, discriminator loss=0.683 , generator loss=0.711\n",
      "Training progress in epoch #41, step 133, discriminator loss=0.686 , generator loss=0.684\n",
      "Training progress in epoch #41, step 134, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #41, step 135, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #41, step 136, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #41, step 137, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #41, step 138, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #41, step 139, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #41, step 140, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #41, step 141, discriminator loss=0.699 , generator loss=0.726\n",
      "Training progress in epoch #41, step 142, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #41, step 143, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #41, step 144, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #41, step 145, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #41, step 146, discriminator loss=0.682 , generator loss=0.709\n",
      "Training progress in epoch #41, step 147, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #41, step 148, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #41, step 149, discriminator loss=0.684 , generator loss=0.730\n",
      "Training progress in epoch #41, step 150, discriminator loss=0.683 , generator loss=0.732\n",
      "Training progress in epoch #41, step 151, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #41, step 152, discriminator loss=0.684 , generator loss=0.713\n",
      "Training progress in epoch #41, step 153, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #41, step 154, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #41, step 155, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #41, step 156, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #41, step 157, discriminator loss=0.695 , generator loss=0.716\n",
      "Training progress in epoch #41, step 158, discriminator loss=0.697 , generator loss=0.678\n",
      "Training progress in epoch #41, step 159, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #41, step 160, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #41, step 161, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #41, step 162, discriminator loss=0.702 , generator loss=0.708\n",
      "Training progress in epoch #41, step 163, discriminator loss=0.695 , generator loss=0.732\n",
      "Training progress in epoch #41, step 164, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #41, step 165, discriminator loss=0.684 , generator loss=0.724\n",
      "Training progress in epoch #41, step 166, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #41, step 167, discriminator loss=0.680 , generator loss=0.707\n",
      "Training progress in epoch #41, step 168, discriminator loss=0.684 , generator loss=0.691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #41, step 169, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #41, step 170, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #41, step 171, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #41, step 172, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #41, step 173, discriminator loss=0.684 , generator loss=0.687\n",
      "Training progress in epoch #41, step 174, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #41, step 175, discriminator loss=0.689 , generator loss=0.733\n",
      "Training progress in epoch #41, step 176, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #41, step 177, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #41, step 178, discriminator loss=0.698 , generator loss=0.698\n",
      "Training progress in epoch #41, step 179, discriminator loss=0.698 , generator loss=0.693\n",
      "Training progress in epoch #41, step 180, discriminator loss=0.698 , generator loss=0.707\n",
      "Training progress in epoch #41, step 181, discriminator loss=0.698 , generator loss=0.708\n",
      "Training progress in epoch #41, step 182, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #41, step 183, discriminator loss=0.700 , generator loss=0.665\n",
      "Training progress in epoch #41, step 184, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #41, step 185, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #41, step 186, discriminator loss=0.684 , generator loss=0.746\n",
      "Training progress in epoch #41, step 187, discriminator loss=0.692 , generator loss=0.729\n",
      "Training progress in epoch #41, step 188, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #41, step 189, discriminator loss=0.683 , generator loss=0.687\n",
      "Training progress in epoch #41, step 190, discriminator loss=0.685 , generator loss=0.688\n",
      "Training progress in epoch #41, step 191, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #41, step 192, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #41, step 193, discriminator loss=0.683 , generator loss=0.710\n",
      "Training progress in epoch #41, step 194, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #41, step 195, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #41, step 196, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #41, step 197, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #41, step 198, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #41, step 199, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #41, step 200, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #41, step 201, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #41, step 202, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #41, step 203, discriminator loss=0.701 , generator loss=0.701\n",
      "Training progress in epoch #41, step 204, discriminator loss=0.706 , generator loss=0.730\n",
      "Training progress in epoch #41, step 205, discriminator loss=0.691 , generator loss=0.737\n",
      "Training progress in epoch #41, step 206, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #41, step 207, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #41, step 208, discriminator loss=0.685 , generator loss=0.681\n",
      "Training progress in epoch #41, step 209, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #41, step 210, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #41, step 211, discriminator loss=0.690 , generator loss=0.737\n",
      "Training progress in epoch #41, step 212, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #41, step 213, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #41, step 214, discriminator loss=0.685 , generator loss=0.725\n",
      "Training progress in epoch #41, step 215, discriminator loss=0.697 , generator loss=0.732\n",
      "Training progress in epoch #41, step 216, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #41, step 217, discriminator loss=0.695 , generator loss=0.715\n",
      "Training progress in epoch #41, step 218, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #41, step 219, discriminator loss=0.686 , generator loss=0.675\n",
      "Training progress in epoch #41, step 220, discriminator loss=0.685 , generator loss=0.687\n",
      "Training progress in epoch #41, step 221, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #41, step 222, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #41, step 223, discriminator loss=0.681 , generator loss=0.712\n",
      "Training progress in epoch #41, step 224, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #41, step 225, discriminator loss=0.696 , generator loss=0.736\n",
      "Training progress in epoch #41, step 226, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #41, step 227, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #41, step 228, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #41, step 229, discriminator loss=0.695 , generator loss=0.689\n",
      "Training progress in epoch #41, step 230, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #41, step 231, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #41, step 232, discriminator loss=0.689 , generator loss=0.735\n",
      "Training progress in epoch #41, step 233, discriminator loss=0.683 , generator loss=0.730\n",
      "Disciminator Accuracy on real images: 43%, on fake images: 96%\n",
      "Training progress in epoch #42, step 0, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #42, step 1, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #42, step 2, discriminator loss=0.682 , generator loss=0.713\n",
      "Training progress in epoch #42, step 3, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #42, step 4, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #42, step 5, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #42, step 6, discriminator loss=0.679 , generator loss=0.683\n",
      "Training progress in epoch #42, step 7, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #42, step 8, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #42, step 9, discriminator loss=0.682 , generator loss=0.721\n",
      "Training progress in epoch #42, step 10, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #42, step 11, discriminator loss=0.683 , generator loss=0.710\n",
      "Training progress in epoch #42, step 12, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #42, step 13, discriminator loss=0.694 , generator loss=0.733\n",
      "Training progress in epoch #42, step 14, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #42, step 15, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #42, step 16, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #42, step 17, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #42, step 18, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #42, step 19, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #42, step 20, discriminator loss=0.684 , generator loss=0.713\n",
      "Training progress in epoch #42, step 21, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #42, step 22, discriminator loss=0.688 , generator loss=0.675\n",
      "Training progress in epoch #42, step 23, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #42, step 24, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #42, step 25, discriminator loss=0.691 , generator loss=0.726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #42, step 26, discriminator loss=0.681 , generator loss=0.708\n",
      "Training progress in epoch #42, step 27, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #42, step 28, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #42, step 29, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #42, step 30, discriminator loss=0.684 , generator loss=0.712\n",
      "Training progress in epoch #42, step 31, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #42, step 32, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #42, step 33, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #42, step 34, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #42, step 35, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #42, step 36, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #42, step 37, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #42, step 38, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #42, step 39, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #42, step 40, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #42, step 41, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #42, step 42, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #42, step 43, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #42, step 44, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #42, step 45, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #42, step 46, discriminator loss=0.688 , generator loss=0.677\n",
      "Training progress in epoch #42, step 47, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #42, step 48, discriminator loss=0.684 , generator loss=0.720\n",
      "Training progress in epoch #42, step 49, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #42, step 50, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #42, step 51, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #42, step 52, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #42, step 53, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #42, step 54, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #42, step 55, discriminator loss=0.696 , generator loss=0.675\n",
      "Training progress in epoch #42, step 56, discriminator loss=0.697 , generator loss=0.680\n",
      "Training progress in epoch #42, step 57, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #42, step 58, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #42, step 59, discriminator loss=0.694 , generator loss=0.736\n",
      "Training progress in epoch #42, step 60, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #42, step 61, discriminator loss=0.701 , generator loss=0.710\n",
      "Training progress in epoch #42, step 62, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #42, step 63, discriminator loss=0.692 , generator loss=0.729\n",
      "Training progress in epoch #42, step 64, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #42, step 65, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #42, step 66, discriminator loss=0.683 , generator loss=0.699\n",
      "Training progress in epoch #42, step 67, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #42, step 68, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #42, step 69, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #42, step 70, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #42, step 71, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #42, step 72, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #42, step 73, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #42, step 74, discriminator loss=0.693 , generator loss=0.742\n",
      "Training progress in epoch #42, step 75, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #42, step 76, discriminator loss=0.696 , generator loss=0.689\n",
      "Training progress in epoch #42, step 77, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #42, step 78, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #42, step 79, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #42, step 80, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #42, step 81, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #42, step 82, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #42, step 83, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #42, step 84, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #42, step 85, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #42, step 86, discriminator loss=0.699 , generator loss=0.703\n",
      "Training progress in epoch #42, step 87, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #42, step 88, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #42, step 89, discriminator loss=0.684 , generator loss=0.702\n",
      "Training progress in epoch #42, step 90, discriminator loss=0.685 , generator loss=0.687\n",
      "Training progress in epoch #42, step 91, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #42, step 92, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #42, step 93, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #42, step 94, discriminator loss=0.685 , generator loss=0.721\n",
      "Training progress in epoch #42, step 95, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #42, step 96, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #42, step 97, discriminator loss=0.694 , generator loss=0.740\n",
      "Training progress in epoch #42, step 98, discriminator loss=0.688 , generator loss=0.742\n",
      "Training progress in epoch #42, step 99, discriminator loss=0.698 , generator loss=0.716\n",
      "Training progress in epoch #42, step 100, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #42, step 101, discriminator loss=0.695 , generator loss=0.682\n",
      "Training progress in epoch #42, step 102, discriminator loss=0.686 , generator loss=0.682\n",
      "Training progress in epoch #42, step 103, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #42, step 104, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #42, step 105, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #42, step 106, discriminator loss=0.684 , generator loss=0.699\n",
      "Training progress in epoch #42, step 107, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #42, step 108, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #42, step 109, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #42, step 110, discriminator loss=0.689 , generator loss=0.733\n",
      "Training progress in epoch #42, step 111, discriminator loss=0.686 , generator loss=0.750\n",
      "Training progress in epoch #42, step 112, discriminator loss=0.683 , generator loss=0.734\n",
      "Training progress in epoch #42, step 113, discriminator loss=0.683 , generator loss=0.700\n",
      "Training progress in epoch #42, step 114, discriminator loss=0.690 , generator loss=0.675\n",
      "Training progress in epoch #42, step 115, discriminator loss=0.682 , generator loss=0.687\n",
      "Training progress in epoch #42, step 116, discriminator loss=0.681 , generator loss=0.693\n",
      "Training progress in epoch #42, step 117, discriminator loss=0.684 , generator loss=0.691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #42, step 118, discriminator loss=0.683 , generator loss=0.702\n",
      "Training progress in epoch #42, step 119, discriminator loss=0.697 , generator loss=0.727\n",
      "Training progress in epoch #42, step 120, discriminator loss=0.693 , generator loss=0.737\n",
      "Training progress in epoch #42, step 121, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #42, step 122, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #42, step 123, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #42, step 124, discriminator loss=0.698 , generator loss=0.669\n",
      "Training progress in epoch #42, step 125, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #42, step 126, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #42, step 127, discriminator loss=0.697 , generator loss=0.721\n",
      "Training progress in epoch #42, step 128, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #42, step 129, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #42, step 130, discriminator loss=0.687 , generator loss=0.727\n",
      "Training progress in epoch #42, step 131, discriminator loss=0.678 , generator loss=0.721\n",
      "Training progress in epoch #42, step 132, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #42, step 133, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #42, step 134, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #42, step 135, discriminator loss=0.685 , generator loss=0.711\n",
      "Training progress in epoch #42, step 136, discriminator loss=0.686 , generator loss=0.726\n",
      "Training progress in epoch #42, step 137, discriminator loss=0.688 , generator loss=0.735\n",
      "Training progress in epoch #42, step 138, discriminator loss=0.685 , generator loss=0.728\n",
      "Training progress in epoch #42, step 139, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #42, step 140, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #42, step 141, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #42, step 142, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #42, step 143, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #42, step 144, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #42, step 145, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #42, step 146, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #42, step 147, discriminator loss=0.697 , generator loss=0.709\n",
      "Training progress in epoch #42, step 148, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #42, step 149, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #42, step 150, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #42, step 151, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #42, step 152, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #42, step 153, discriminator loss=0.682 , generator loss=0.719\n",
      "Training progress in epoch #42, step 154, discriminator loss=0.688 , generator loss=0.758\n",
      "Training progress in epoch #42, step 155, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #42, step 156, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #42, step 157, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #42, step 158, discriminator loss=0.688 , generator loss=0.677\n",
      "Training progress in epoch #42, step 159, discriminator loss=0.689 , generator loss=0.671\n",
      "Training progress in epoch #42, step 160, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #42, step 161, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #42, step 162, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #42, step 163, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #42, step 164, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #42, step 165, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #42, step 166, discriminator loss=0.693 , generator loss=0.726\n",
      "Training progress in epoch #42, step 167, discriminator loss=0.699 , generator loss=0.738\n",
      "Training progress in epoch #42, step 168, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #42, step 169, discriminator loss=0.698 , generator loss=0.688\n",
      "Training progress in epoch #42, step 170, discriminator loss=0.690 , generator loss=0.674\n",
      "Training progress in epoch #42, step 171, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #42, step 172, discriminator loss=0.692 , generator loss=0.668\n",
      "Training progress in epoch #42, step 173, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #42, step 174, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #42, step 175, discriminator loss=0.686 , generator loss=0.730\n",
      "Training progress in epoch #42, step 176, discriminator loss=0.691 , generator loss=0.735\n",
      "Training progress in epoch #42, step 177, discriminator loss=0.682 , generator loss=0.710\n",
      "Training progress in epoch #42, step 178, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #42, step 179, discriminator loss=0.683 , generator loss=0.697\n",
      "Training progress in epoch #42, step 180, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #42, step 181, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #42, step 182, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #42, step 183, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #42, step 184, discriminator loss=0.683 , generator loss=0.694\n",
      "Training progress in epoch #42, step 185, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #42, step 186, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #42, step 187, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #42, step 188, discriminator loss=0.700 , generator loss=0.715\n",
      "Training progress in epoch #42, step 189, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #42, step 190, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #42, step 191, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #42, step 192, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #42, step 193, discriminator loss=0.695 , generator loss=0.681\n",
      "Training progress in epoch #42, step 194, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #42, step 195, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #42, step 196, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #42, step 197, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #42, step 198, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #42, step 199, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #42, step 200, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #42, step 201, discriminator loss=0.691 , generator loss=0.723\n",
      "Training progress in epoch #42, step 202, discriminator loss=0.696 , generator loss=0.725\n",
      "Training progress in epoch #42, step 203, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #42, step 204, discriminator loss=0.686 , generator loss=0.684\n",
      "Training progress in epoch #42, step 205, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #42, step 206, discriminator loss=0.683 , generator loss=0.707\n",
      "Training progress in epoch #42, step 207, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #42, step 208, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #42, step 209, discriminator loss=0.689 , generator loss=0.701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #42, step 210, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #42, step 211, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #42, step 212, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #42, step 213, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #42, step 214, discriminator loss=0.684 , generator loss=0.713\n",
      "Training progress in epoch #42, step 215, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #42, step 216, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #42, step 217, discriminator loss=0.685 , generator loss=0.683\n",
      "Training progress in epoch #42, step 218, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #42, step 219, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #42, step 220, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #42, step 221, discriminator loss=0.691 , generator loss=0.669\n",
      "Training progress in epoch #42, step 222, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #42, step 223, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #42, step 224, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #42, step 225, discriminator loss=0.696 , generator loss=0.729\n",
      "Training progress in epoch #42, step 226, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #42, step 227, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #42, step 228, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #42, step 229, discriminator loss=0.700 , generator loss=0.713\n",
      "Training progress in epoch #42, step 230, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #42, step 231, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #42, step 232, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #42, step 233, discriminator loss=0.694 , generator loss=0.691\n",
      "Disciminator Accuracy on real images: 69%, on fake images: 54%\n",
      "Training progress in epoch #43, step 0, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #43, step 1, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #43, step 2, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #43, step 3, discriminator loss=0.700 , generator loss=0.700\n",
      "Training progress in epoch #43, step 4, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #43, step 5, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #43, step 6, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #43, step 7, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #43, step 8, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #43, step 9, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #43, step 10, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #43, step 11, discriminator loss=0.685 , generator loss=0.743\n",
      "Training progress in epoch #43, step 12, discriminator loss=0.685 , generator loss=0.729\n",
      "Training progress in epoch #43, step 13, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #43, step 14, discriminator loss=0.696 , generator loss=0.685\n",
      "Training progress in epoch #43, step 15, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #43, step 16, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #43, step 17, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #43, step 18, discriminator loss=0.699 , generator loss=0.709\n",
      "Training progress in epoch #43, step 19, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #43, step 20, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #43, step 21, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #43, step 22, discriminator loss=0.691 , generator loss=0.723\n",
      "Training progress in epoch #43, step 23, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #43, step 24, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #43, step 25, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #43, step 26, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #43, step 27, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #43, step 28, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #43, step 29, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #43, step 30, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #43, step 31, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #43, step 32, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #43, step 33, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #43, step 34, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #43, step 35, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #43, step 36, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #43, step 37, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #43, step 38, discriminator loss=0.697 , generator loss=0.732\n",
      "Training progress in epoch #43, step 39, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #43, step 40, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #43, step 41, discriminator loss=0.696 , generator loss=0.718\n",
      "Training progress in epoch #43, step 42, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #43, step 43, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #43, step 44, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #43, step 45, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #43, step 46, discriminator loss=0.686 , generator loss=0.732\n",
      "Training progress in epoch #43, step 47, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #43, step 48, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #43, step 49, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #43, step 50, discriminator loss=0.684 , generator loss=0.695\n",
      "Training progress in epoch #43, step 51, discriminator loss=0.685 , generator loss=0.698\n",
      "Training progress in epoch #43, step 52, discriminator loss=0.682 , generator loss=0.715\n",
      "Training progress in epoch #43, step 53, discriminator loss=0.683 , generator loss=0.709\n",
      "Training progress in epoch #43, step 54, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #43, step 55, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #43, step 56, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #43, step 57, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #43, step 58, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #43, step 59, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #43, step 60, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #43, step 61, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #43, step 62, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #43, step 63, discriminator loss=0.694 , generator loss=0.675\n",
      "Training progress in epoch #43, step 64, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #43, step 65, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #43, step 66, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #43, step 67, discriminator loss=0.692 , generator loss=0.720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #43, step 68, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #43, step 69, discriminator loss=0.684 , generator loss=0.687\n",
      "Training progress in epoch #43, step 70, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #43, step 71, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #43, step 72, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #43, step 73, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #43, step 74, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #43, step 75, discriminator loss=0.684 , generator loss=0.697\n",
      "Training progress in epoch #43, step 76, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #43, step 77, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #43, step 78, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #43, step 79, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #43, step 80, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #43, step 81, discriminator loss=0.690 , generator loss=0.751\n",
      "Training progress in epoch #43, step 82, discriminator loss=0.689 , generator loss=0.753\n",
      "Training progress in epoch #43, step 83, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #43, step 84, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #43, step 85, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #43, step 86, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #43, step 87, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #43, step 88, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #43, step 89, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #43, step 90, discriminator loss=0.685 , generator loss=0.709\n",
      "Training progress in epoch #43, step 91, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #43, step 92, discriminator loss=0.683 , generator loss=0.709\n",
      "Training progress in epoch #43, step 93, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #43, step 94, discriminator loss=0.691 , generator loss=0.739\n",
      "Training progress in epoch #43, step 95, discriminator loss=0.691 , generator loss=0.738\n",
      "Training progress in epoch #43, step 96, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #43, step 97, discriminator loss=0.698 , generator loss=0.697\n",
      "Training progress in epoch #43, step 98, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #43, step 99, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #43, step 100, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #43, step 101, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #43, step 102, discriminator loss=0.696 , generator loss=0.718\n",
      "Training progress in epoch #43, step 103, discriminator loss=0.690 , generator loss=0.742\n",
      "Training progress in epoch #43, step 104, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #43, step 105, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #43, step 106, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #43, step 107, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #43, step 108, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #43, step 109, discriminator loss=0.685 , generator loss=0.698\n",
      "Training progress in epoch #43, step 110, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #43, step 111, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #43, step 112, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #43, step 113, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #43, step 114, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #43, step 115, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #43, step 116, discriminator loss=0.690 , generator loss=0.673\n",
      "Training progress in epoch #43, step 117, discriminator loss=0.689 , generator loss=0.677\n",
      "Training progress in epoch #43, step 118, discriminator loss=0.681 , generator loss=0.722\n",
      "Training progress in epoch #43, step 119, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #43, step 120, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #43, step 121, discriminator loss=0.698 , generator loss=0.693\n",
      "Training progress in epoch #43, step 122, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #43, step 123, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #43, step 124, discriminator loss=0.692 , generator loss=0.672\n",
      "Training progress in epoch #43, step 125, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #43, step 126, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #43, step 127, discriminator loss=0.686 , generator loss=0.732\n",
      "Training progress in epoch #43, step 128, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #43, step 129, discriminator loss=0.686 , generator loss=0.681\n",
      "Training progress in epoch #43, step 130, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #43, step 131, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #43, step 132, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #43, step 133, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #43, step 134, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #43, step 135, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #43, step 136, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #43, step 137, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #43, step 138, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #43, step 139, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #43, step 140, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #43, step 141, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #43, step 142, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #43, step 143, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #43, step 144, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #43, step 145, discriminator loss=0.689 , generator loss=0.673\n",
      "Training progress in epoch #43, step 146, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #43, step 147, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #43, step 148, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #43, step 149, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #43, step 150, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #43, step 151, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #43, step 152, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #43, step 153, discriminator loss=0.699 , generator loss=0.692\n",
      "Training progress in epoch #43, step 154, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #43, step 155, discriminator loss=0.684 , generator loss=0.698\n",
      "Training progress in epoch #43, step 156, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #43, step 157, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #43, step 158, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #43, step 159, discriminator loss=0.691 , generator loss=0.718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #43, step 160, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #43, step 161, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #43, step 162, discriminator loss=0.684 , generator loss=0.680\n",
      "Training progress in epoch #43, step 163, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #43, step 164, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #43, step 165, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #43, step 166, discriminator loss=0.685 , generator loss=0.685\n",
      "Training progress in epoch #43, step 167, discriminator loss=0.697 , generator loss=0.708\n",
      "Training progress in epoch #43, step 168, discriminator loss=0.690 , generator loss=0.737\n",
      "Training progress in epoch #43, step 169, discriminator loss=0.692 , generator loss=0.747\n",
      "Training progress in epoch #43, step 170, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #43, step 171, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #43, step 172, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #43, step 173, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #43, step 174, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #43, step 175, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #43, step 176, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #43, step 177, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #43, step 178, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #43, step 179, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #43, step 180, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #43, step 181, discriminator loss=0.684 , generator loss=0.685\n",
      "Training progress in epoch #43, step 182, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #43, step 183, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #43, step 184, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #43, step 185, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #43, step 186, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #43, step 187, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #43, step 188, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #43, step 189, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #43, step 190, discriminator loss=0.695 , generator loss=0.726\n",
      "Training progress in epoch #43, step 191, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #43, step 192, discriminator loss=0.698 , generator loss=0.744\n",
      "Training progress in epoch #43, step 193, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #43, step 194, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #43, step 195, discriminator loss=0.691 , generator loss=0.674\n",
      "Training progress in epoch #43, step 196, discriminator loss=0.697 , generator loss=0.689\n",
      "Training progress in epoch #43, step 197, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #43, step 198, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #43, step 199, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #43, step 200, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #43, step 201, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #43, step 202, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #43, step 203, discriminator loss=0.698 , generator loss=0.712\n",
      "Training progress in epoch #43, step 204, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #43, step 205, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #43, step 206, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #43, step 207, discriminator loss=0.695 , generator loss=0.682\n",
      "Training progress in epoch #43, step 208, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #43, step 209, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #43, step 210, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #43, step 211, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #43, step 212, discriminator loss=0.694 , generator loss=0.726\n",
      "Training progress in epoch #43, step 213, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #43, step 214, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #43, step 215, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #43, step 216, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #43, step 217, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #43, step 218, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #43, step 219, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #43, step 220, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #43, step 221, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #43, step 222, discriminator loss=0.690 , generator loss=0.755\n",
      "Training progress in epoch #43, step 223, discriminator loss=0.690 , generator loss=0.741\n",
      "Training progress in epoch #43, step 224, discriminator loss=0.683 , generator loss=0.698\n",
      "Training progress in epoch #43, step 225, discriminator loss=0.690 , generator loss=0.676\n",
      "Training progress in epoch #43, step 226, discriminator loss=0.694 , generator loss=0.668\n",
      "Training progress in epoch #43, step 227, discriminator loss=0.698 , generator loss=0.691\n",
      "Training progress in epoch #43, step 228, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #43, step 229, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #43, step 230, discriminator loss=0.691 , generator loss=0.727\n",
      "Training progress in epoch #43, step 231, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #43, step 232, discriminator loss=0.695 , generator loss=0.743\n",
      "Training progress in epoch #43, step 233, discriminator loss=0.686 , generator loss=0.743\n",
      "Disciminator Accuracy on real images: 15%, on fake images: 100%\n",
      "Training progress in epoch #44, step 0, discriminator loss=0.698 , generator loss=0.712\n",
      "Training progress in epoch #44, step 1, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #44, step 2, discriminator loss=0.691 , generator loss=0.673\n",
      "Training progress in epoch #44, step 3, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #44, step 4, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #44, step 5, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #44, step 6, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #44, step 7, discriminator loss=0.685 , generator loss=0.692\n",
      "Training progress in epoch #44, step 8, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #44, step 9, discriminator loss=0.684 , generator loss=0.695\n",
      "Training progress in epoch #44, step 10, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #44, step 11, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #44, step 12, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #44, step 13, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #44, step 14, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #44, step 15, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #44, step 16, discriminator loss=0.693 , generator loss=0.729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #44, step 17, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #44, step 18, discriminator loss=0.690 , generator loss=0.660\n",
      "Training progress in epoch #44, step 19, discriminator loss=0.688 , generator loss=0.670\n",
      "Training progress in epoch #44, step 20, discriminator loss=0.696 , generator loss=0.725\n",
      "Training progress in epoch #44, step 21, discriminator loss=0.693 , generator loss=0.787\n",
      "Training progress in epoch #44, step 22, discriminator loss=0.701 , generator loss=0.757\n",
      "Training progress in epoch #44, step 23, discriminator loss=0.685 , generator loss=0.715\n",
      "Training progress in epoch #44, step 24, discriminator loss=0.681 , generator loss=0.697\n",
      "Training progress in epoch #44, step 25, discriminator loss=0.697 , generator loss=0.688\n",
      "Training progress in epoch #44, step 26, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #44, step 27, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #44, step 28, discriminator loss=0.690 , generator loss=0.668\n",
      "Training progress in epoch #44, step 29, discriminator loss=0.696 , generator loss=0.679\n",
      "Training progress in epoch #44, step 30, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #44, step 31, discriminator loss=0.693 , generator loss=0.735\n",
      "Training progress in epoch #44, step 32, discriminator loss=0.695 , generator loss=0.741\n",
      "Training progress in epoch #44, step 33, discriminator loss=0.687 , generator loss=0.727\n",
      "Training progress in epoch #44, step 34, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #44, step 35, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #44, step 36, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #44, step 37, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #44, step 38, discriminator loss=0.684 , generator loss=0.678\n",
      "Training progress in epoch #44, step 39, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #44, step 40, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #44, step 41, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #44, step 42, discriminator loss=0.682 , generator loss=0.727\n",
      "Training progress in epoch #44, step 43, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #44, step 44, discriminator loss=0.689 , generator loss=0.728\n",
      "Training progress in epoch #44, step 45, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #44, step 46, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #44, step 47, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #44, step 48, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #44, step 49, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #44, step 50, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #44, step 51, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #44, step 52, discriminator loss=0.699 , generator loss=0.682\n",
      "Training progress in epoch #44, step 53, discriminator loss=0.694 , generator loss=0.726\n",
      "Training progress in epoch #44, step 54, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #44, step 55, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #44, step 56, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #44, step 57, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #44, step 58, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #44, step 59, discriminator loss=0.690 , generator loss=0.734\n",
      "Training progress in epoch #44, step 60, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #44, step 61, discriminator loss=0.684 , generator loss=0.698\n",
      "Training progress in epoch #44, step 62, discriminator loss=0.697 , generator loss=0.701\n",
      "Training progress in epoch #44, step 63, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #44, step 64, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #44, step 65, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #44, step 66, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #44, step 67, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #44, step 68, discriminator loss=0.694 , generator loss=0.736\n",
      "Training progress in epoch #44, step 69, discriminator loss=0.686 , generator loss=0.732\n",
      "Training progress in epoch #44, step 70, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #44, step 71, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #44, step 72, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #44, step 73, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #44, step 74, discriminator loss=0.683 , generator loss=0.694\n",
      "Training progress in epoch #44, step 75, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #44, step 76, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #44, step 77, discriminator loss=0.685 , generator loss=0.710\n",
      "Training progress in epoch #44, step 78, discriminator loss=0.689 , generator loss=0.747\n",
      "Training progress in epoch #44, step 79, discriminator loss=0.688 , generator loss=0.750\n",
      "Training progress in epoch #44, step 80, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #44, step 81, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #44, step 82, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #44, step 83, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #44, step 84, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #44, step 85, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #44, step 86, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #44, step 87, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #44, step 88, discriminator loss=0.686 , generator loss=0.735\n",
      "Training progress in epoch #44, step 89, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #44, step 90, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #44, step 91, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #44, step 92, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #44, step 93, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #44, step 94, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #44, step 95, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #44, step 96, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #44, step 97, discriminator loss=0.699 , generator loss=0.673\n",
      "Training progress in epoch #44, step 98, discriminator loss=0.694 , generator loss=0.677\n",
      "Training progress in epoch #44, step 99, discriminator loss=0.695 , generator loss=0.716\n",
      "Training progress in epoch #44, step 100, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #44, step 101, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #44, step 102, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #44, step 103, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #44, step 104, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #44, step 105, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #44, step 106, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #44, step 107, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #44, step 108, discriminator loss=0.694 , generator loss=0.680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #44, step 109, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #44, step 110, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #44, step 111, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #44, step 112, discriminator loss=0.693 , generator loss=0.754\n",
      "Training progress in epoch #44, step 113, discriminator loss=0.690 , generator loss=0.741\n",
      "Training progress in epoch #44, step 114, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #44, step 115, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #44, step 116, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #44, step 117, discriminator loss=0.683 , generator loss=0.701\n",
      "Training progress in epoch #44, step 118, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #44, step 119, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #44, step 120, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #44, step 121, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #44, step 122, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #44, step 123, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #44, step 124, discriminator loss=0.697 , generator loss=0.740\n",
      "Training progress in epoch #44, step 125, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #44, step 126, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #44, step 127, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #44, step 128, discriminator loss=0.697 , generator loss=0.702\n",
      "Training progress in epoch #44, step 129, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #44, step 130, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #44, step 131, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #44, step 132, discriminator loss=0.685 , generator loss=0.728\n",
      "Training progress in epoch #44, step 133, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #44, step 134, discriminator loss=0.681 , generator loss=0.695\n",
      "Training progress in epoch #44, step 135, discriminator loss=0.683 , generator loss=0.715\n",
      "Training progress in epoch #44, step 136, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #44, step 137, discriminator loss=0.687 , generator loss=0.730\n",
      "Training progress in epoch #44, step 138, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #44, step 139, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #44, step 140, discriminator loss=0.689 , generator loss=0.674\n",
      "Training progress in epoch #44, step 141, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #44, step 142, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #44, step 143, discriminator loss=0.688 , generator loss=0.732\n",
      "Training progress in epoch #44, step 144, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #44, step 145, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #44, step 146, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #44, step 147, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #44, step 148, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #44, step 149, discriminator loss=0.689 , generator loss=0.676\n",
      "Training progress in epoch #44, step 150, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #44, step 151, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #44, step 152, discriminator loss=0.691 , generator loss=0.739\n",
      "Training progress in epoch #44, step 153, discriminator loss=0.689 , generator loss=0.759\n",
      "Training progress in epoch #44, step 154, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #44, step 155, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #44, step 156, discriminator loss=0.685 , generator loss=0.669\n",
      "Training progress in epoch #44, step 157, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #44, step 158, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #44, step 159, discriminator loss=0.684 , generator loss=0.735\n",
      "Training progress in epoch #44, step 160, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #44, step 161, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #44, step 162, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #44, step 163, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #44, step 164, discriminator loss=0.696 , generator loss=0.735\n",
      "Training progress in epoch #44, step 165, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #44, step 166, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #44, step 167, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #44, step 168, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #44, step 169, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #44, step 170, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #44, step 171, discriminator loss=0.683 , generator loss=0.698\n",
      "Training progress in epoch #44, step 172, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #44, step 173, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #44, step 174, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #44, step 175, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #44, step 176, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #44, step 177, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #44, step 178, discriminator loss=0.686 , generator loss=0.685\n",
      "Training progress in epoch #44, step 179, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #44, step 180, discriminator loss=0.692 , generator loss=0.733\n",
      "Training progress in epoch #44, step 181, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #44, step 182, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #44, step 183, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #44, step 184, discriminator loss=0.686 , generator loss=0.684\n",
      "Training progress in epoch #44, step 185, discriminator loss=0.690 , generator loss=0.676\n",
      "Training progress in epoch #44, step 186, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #44, step 187, discriminator loss=0.682 , generator loss=0.715\n",
      "Training progress in epoch #44, step 188, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #44, step 189, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #44, step 190, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #44, step 191, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #44, step 192, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #44, step 193, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #44, step 194, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #44, step 195, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #44, step 196, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #44, step 197, discriminator loss=0.688 , generator loss=0.678\n",
      "Training progress in epoch #44, step 198, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #44, step 199, discriminator loss=0.685 , generator loss=0.744\n",
      "Training progress in epoch #44, step 200, discriminator loss=0.691 , generator loss=0.741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #44, step 201, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #44, step 202, discriminator loss=0.681 , generator loss=0.693\n",
      "Training progress in epoch #44, step 203, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #44, step 204, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #44, step 205, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #44, step 206, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #44, step 207, discriminator loss=0.683 , generator loss=0.710\n",
      "Training progress in epoch #44, step 208, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #44, step 209, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #44, step 210, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #44, step 211, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #44, step 212, discriminator loss=0.699 , generator loss=0.723\n",
      "Training progress in epoch #44, step 213, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #44, step 214, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #44, step 215, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #44, step 216, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #44, step 217, discriminator loss=0.703 , generator loss=0.686\n",
      "Training progress in epoch #44, step 218, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #44, step 219, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #44, step 220, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #44, step 221, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #44, step 222, discriminator loss=0.684 , generator loss=0.691\n",
      "Training progress in epoch #44, step 223, discriminator loss=0.683 , generator loss=0.727\n",
      "Training progress in epoch #44, step 224, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #44, step 225, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #44, step 226, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #44, step 227, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #44, step 228, discriminator loss=0.683 , generator loss=0.682\n",
      "Training progress in epoch #44, step 229, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #44, step 230, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #44, step 231, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #44, step 232, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #44, step 233, discriminator loss=0.692 , generator loss=0.713\n",
      "Disciminator Accuracy on real images: 39%, on fake images: 88%\n",
      "Training progress in epoch #45, step 0, discriminator loss=0.699 , generator loss=0.702\n",
      "Training progress in epoch #45, step 1, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #45, step 2, discriminator loss=0.703 , generator loss=0.696\n",
      "Training progress in epoch #45, step 3, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #45, step 4, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #45, step 5, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #45, step 6, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #45, step 7, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #45, step 8, discriminator loss=0.683 , generator loss=0.697\n",
      "Training progress in epoch #45, step 9, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #45, step 10, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #45, step 11, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #45, step 12, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #45, step 13, discriminator loss=0.688 , generator loss=0.748\n",
      "Training progress in epoch #45, step 14, discriminator loss=0.689 , generator loss=0.747\n",
      "Training progress in epoch #45, step 15, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #45, step 16, discriminator loss=0.685 , generator loss=0.675\n",
      "Training progress in epoch #45, step 17, discriminator loss=0.687 , generator loss=0.673\n",
      "Training progress in epoch #45, step 18, discriminator loss=0.692 , generator loss=0.664\n",
      "Training progress in epoch #45, step 19, discriminator loss=0.685 , generator loss=0.676\n",
      "Training progress in epoch #45, step 20, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #45, step 21, discriminator loss=0.694 , generator loss=0.729\n",
      "Training progress in epoch #45, step 22, discriminator loss=0.689 , generator loss=0.740\n",
      "Training progress in epoch #45, step 23, discriminator loss=0.691 , generator loss=0.733\n",
      "Training progress in epoch #45, step 24, discriminator loss=0.698 , generator loss=0.730\n",
      "Training progress in epoch #45, step 25, discriminator loss=0.686 , generator loss=0.728\n",
      "Training progress in epoch #45, step 26, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #45, step 27, discriminator loss=0.693 , generator loss=0.658\n",
      "Training progress in epoch #45, step 28, discriminator loss=0.694 , generator loss=0.666\n",
      "Training progress in epoch #45, step 29, discriminator loss=0.692 , generator loss=0.673\n",
      "Training progress in epoch #45, step 30, discriminator loss=0.695 , generator loss=0.719\n",
      "Training progress in epoch #45, step 31, discriminator loss=0.695 , generator loss=0.732\n",
      "Training progress in epoch #45, step 32, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #45, step 33, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #45, step 34, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #45, step 35, discriminator loss=0.693 , generator loss=0.730\n",
      "Training progress in epoch #45, step 36, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #45, step 37, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #45, step 38, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #45, step 39, discriminator loss=0.686 , generator loss=0.680\n",
      "Training progress in epoch #45, step 40, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #45, step 41, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #45, step 42, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #45, step 43, discriminator loss=0.685 , generator loss=0.715\n",
      "Training progress in epoch #45, step 44, discriminator loss=0.685 , generator loss=0.729\n",
      "Training progress in epoch #45, step 45, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #45, step 46, discriminator loss=0.690 , generator loss=0.679\n",
      "Training progress in epoch #45, step 47, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #45, step 48, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #45, step 49, discriminator loss=0.693 , generator loss=0.739\n",
      "Training progress in epoch #45, step 50, discriminator loss=0.690 , generator loss=0.744\n",
      "Training progress in epoch #45, step 51, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #45, step 52, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #45, step 53, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #45, step 54, discriminator loss=0.684 , generator loss=0.697\n",
      "Training progress in epoch #45, step 55, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #45, step 56, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #45, step 57, discriminator loss=0.684 , generator loss=0.707\n",
      "Training progress in epoch #45, step 58, discriminator loss=0.687 , generator loss=0.731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #45, step 59, discriminator loss=0.692 , generator loss=0.735\n",
      "Training progress in epoch #45, step 60, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #45, step 61, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #45, step 62, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #45, step 63, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #45, step 64, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #45, step 65, discriminator loss=0.684 , generator loss=0.686\n",
      "Training progress in epoch #45, step 66, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #45, step 67, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #45, step 68, discriminator loss=0.692 , generator loss=0.746\n",
      "Training progress in epoch #45, step 69, discriminator loss=0.689 , generator loss=0.734\n",
      "Training progress in epoch #45, step 70, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #45, step 71, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #45, step 72, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #45, step 73, discriminator loss=0.695 , generator loss=0.681\n",
      "Training progress in epoch #45, step 74, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #45, step 75, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #45, step 76, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #45, step 77, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #45, step 78, discriminator loss=0.695 , generator loss=0.736\n",
      "Training progress in epoch #45, step 79, discriminator loss=0.688 , generator loss=0.742\n",
      "Training progress in epoch #45, step 80, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #45, step 81, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #45, step 82, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #45, step 83, discriminator loss=0.684 , generator loss=0.684\n",
      "Training progress in epoch #45, step 84, discriminator loss=0.683 , generator loss=0.690\n",
      "Training progress in epoch #45, step 85, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #45, step 86, discriminator loss=0.683 , generator loss=0.724\n",
      "Training progress in epoch #45, step 87, discriminator loss=0.684 , generator loss=0.731\n",
      "Training progress in epoch #45, step 88, discriminator loss=0.683 , generator loss=0.727\n",
      "Training progress in epoch #45, step 89, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #45, step 90, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #45, step 91, discriminator loss=0.695 , generator loss=0.724\n",
      "Training progress in epoch #45, step 92, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #45, step 93, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #45, step 94, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #45, step 95, discriminator loss=0.684 , generator loss=0.693\n",
      "Training progress in epoch #45, step 96, discriminator loss=0.690 , generator loss=0.736\n",
      "Training progress in epoch #45, step 97, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #45, step 98, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #45, step 99, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #45, step 100, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #45, step 101, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #45, step 102, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #45, step 103, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #45, step 104, discriminator loss=0.676 , generator loss=0.709\n",
      "Training progress in epoch #45, step 105, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #45, step 106, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #45, step 107, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #45, step 108, discriminator loss=0.685 , generator loss=0.709\n",
      "Training progress in epoch #45, step 109, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #45, step 110, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #45, step 111, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #45, step 112, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #45, step 113, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #45, step 114, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #45, step 115, discriminator loss=0.684 , generator loss=0.720\n",
      "Training progress in epoch #45, step 116, discriminator loss=0.700 , generator loss=0.722\n",
      "Training progress in epoch #45, step 117, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #45, step 118, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #45, step 119, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #45, step 120, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #45, step 121, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #45, step 122, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #45, step 123, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #45, step 124, discriminator loss=0.685 , generator loss=0.711\n",
      "Training progress in epoch #45, step 125, discriminator loss=0.682 , generator loss=0.677\n",
      "Training progress in epoch #45, step 126, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #45, step 127, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #45, step 128, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #45, step 129, discriminator loss=0.695 , generator loss=0.715\n",
      "Training progress in epoch #45, step 130, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #45, step 131, discriminator loss=0.685 , generator loss=0.698\n",
      "Training progress in epoch #45, step 132, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #45, step 133, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #45, step 134, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #45, step 135, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #45, step 136, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #45, step 137, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #45, step 138, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #45, step 139, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #45, step 140, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #45, step 141, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #45, step 142, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #45, step 143, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #45, step 144, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #45, step 145, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #45, step 146, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #45, step 147, discriminator loss=0.698 , generator loss=0.679\n",
      "Training progress in epoch #45, step 148, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #45, step 149, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #45, step 150, discriminator loss=0.690 , generator loss=0.741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #45, step 151, discriminator loss=0.691 , generator loss=0.737\n",
      "Training progress in epoch #45, step 152, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #45, step 153, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #45, step 154, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #45, step 155, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #45, step 156, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #45, step 157, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #45, step 158, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #45, step 159, discriminator loss=0.693 , generator loss=0.662\n",
      "Training progress in epoch #45, step 160, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #45, step 161, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #45, step 162, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #45, step 163, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #45, step 164, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #45, step 165, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #45, step 166, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #45, step 167, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #45, step 168, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #45, step 169, discriminator loss=0.699 , generator loss=0.682\n",
      "Training progress in epoch #45, step 170, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #45, step 171, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #45, step 172, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #45, step 173, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #45, step 174, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #45, step 175, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #45, step 176, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #45, step 177, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #45, step 178, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #45, step 179, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #45, step 180, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #45, step 181, discriminator loss=0.684 , generator loss=0.696\n",
      "Training progress in epoch #45, step 182, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #45, step 183, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #45, step 184, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #45, step 185, discriminator loss=0.698 , generator loss=0.729\n",
      "Training progress in epoch #45, step 186, discriminator loss=0.696 , generator loss=0.719\n",
      "Training progress in epoch #45, step 187, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #45, step 188, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #45, step 189, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #45, step 190, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #45, step 191, discriminator loss=0.684 , generator loss=0.689\n",
      "Training progress in epoch #45, step 192, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #45, step 193, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #45, step 194, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #45, step 195, discriminator loss=0.692 , generator loss=0.754\n",
      "Training progress in epoch #45, step 196, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #45, step 197, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #45, step 198, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #45, step 199, discriminator loss=0.689 , generator loss=0.669\n",
      "Training progress in epoch #45, step 200, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #45, step 201, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #45, step 202, discriminator loss=0.685 , generator loss=0.733\n",
      "Training progress in epoch #45, step 203, discriminator loss=0.681 , generator loss=0.731\n",
      "Training progress in epoch #45, step 204, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #45, step 205, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #45, step 206, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #45, step 207, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #45, step 208, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #45, step 209, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #45, step 210, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #45, step 211, discriminator loss=0.683 , generator loss=0.698\n",
      "Training progress in epoch #45, step 212, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #45, step 213, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #45, step 214, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #45, step 215, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #45, step 216, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #45, step 217, discriminator loss=0.684 , generator loss=0.695\n",
      "Training progress in epoch #45, step 218, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #45, step 219, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #45, step 220, discriminator loss=0.693 , generator loss=0.736\n",
      "Training progress in epoch #45, step 221, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #45, step 222, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #45, step 223, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #45, step 224, discriminator loss=0.694 , generator loss=0.724\n",
      "Training progress in epoch #45, step 225, discriminator loss=0.684 , generator loss=0.731\n",
      "Training progress in epoch #45, step 226, discriminator loss=0.684 , generator loss=0.727\n",
      "Training progress in epoch #45, step 227, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #45, step 228, discriminator loss=0.693 , generator loss=0.671\n",
      "Training progress in epoch #45, step 229, discriminator loss=0.696 , generator loss=0.679\n",
      "Training progress in epoch #45, step 230, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #45, step 231, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #45, step 232, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #45, step 233, discriminator loss=0.692 , generator loss=0.717\n",
      "Disciminator Accuracy on real images: 31%, on fake images: 90%\n",
      "Training progress in epoch #46, step 0, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #46, step 1, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #46, step 2, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #46, step 3, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #46, step 4, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #46, step 5, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #46, step 6, discriminator loss=0.684 , generator loss=0.699\n",
      "Training progress in epoch #46, step 7, discriminator loss=0.685 , generator loss=0.723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #46, step 8, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #46, step 9, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #46, step 10, discriminator loss=0.680 , generator loss=0.689\n",
      "Training progress in epoch #46, step 11, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #46, step 12, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #46, step 13, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #46, step 14, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #46, step 15, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #46, step 16, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #46, step 17, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #46, step 18, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #46, step 19, discriminator loss=0.691 , generator loss=0.675\n",
      "Training progress in epoch #46, step 20, discriminator loss=0.689 , generator loss=0.668\n",
      "Training progress in epoch #46, step 21, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #46, step 22, discriminator loss=0.701 , generator loss=0.716\n",
      "Training progress in epoch #46, step 23, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #46, step 24, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #46, step 25, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #46, step 26, discriminator loss=0.691 , generator loss=0.759\n",
      "Training progress in epoch #46, step 27, discriminator loss=0.693 , generator loss=0.730\n",
      "Training progress in epoch #46, step 28, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #46, step 29, discriminator loss=0.686 , generator loss=0.687\n",
      "Training progress in epoch #46, step 30, discriminator loss=0.696 , generator loss=0.695\n",
      "Training progress in epoch #46, step 31, discriminator loss=0.685 , generator loss=0.692\n",
      "Training progress in epoch #46, step 32, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #46, step 33, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #46, step 34, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #46, step 35, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #46, step 36, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #46, step 37, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #46, step 38, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #46, step 39, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #46, step 40, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #46, step 41, discriminator loss=0.690 , generator loss=0.736\n",
      "Training progress in epoch #46, step 42, discriminator loss=0.688 , generator loss=0.733\n",
      "Training progress in epoch #46, step 43, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #46, step 44, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #46, step 45, discriminator loss=0.687 , generator loss=0.666\n",
      "Training progress in epoch #46, step 46, discriminator loss=0.681 , generator loss=0.684\n",
      "Training progress in epoch #46, step 47, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #46, step 48, discriminator loss=0.680 , generator loss=0.714\n",
      "Training progress in epoch #46, step 49, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #46, step 50, discriminator loss=0.685 , generator loss=0.714\n",
      "Training progress in epoch #46, step 51, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #46, step 52, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #46, step 53, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #46, step 54, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #46, step 55, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #46, step 56, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #46, step 57, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #46, step 58, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #46, step 59, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #46, step 60, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #46, step 61, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #46, step 62, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #46, step 63, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #46, step 64, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #46, step 65, discriminator loss=0.690 , generator loss=0.727\n",
      "Training progress in epoch #46, step 66, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #46, step 67, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #46, step 68, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #46, step 69, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #46, step 70, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #46, step 71, discriminator loss=0.689 , generator loss=0.733\n",
      "Training progress in epoch #46, step 72, discriminator loss=0.693 , generator loss=0.733\n",
      "Training progress in epoch #46, step 73, discriminator loss=0.685 , generator loss=0.735\n",
      "Training progress in epoch #46, step 74, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #46, step 75, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #46, step 76, discriminator loss=0.689 , generator loss=0.677\n",
      "Training progress in epoch #46, step 77, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #46, step 78, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #46, step 79, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #46, step 80, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #46, step 81, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #46, step 82, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #46, step 83, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #46, step 84, discriminator loss=0.694 , generator loss=0.733\n",
      "Training progress in epoch #46, step 85, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #46, step 86, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #46, step 87, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #46, step 88, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #46, step 89, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #46, step 90, discriminator loss=0.700 , generator loss=0.696\n",
      "Training progress in epoch #46, step 91, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #46, step 92, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #46, step 93, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #46, step 94, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #46, step 95, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #46, step 96, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #46, step 97, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #46, step 98, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #46, step 99, discriminator loss=0.698 , generator loss=0.720\n",
      "Training progress in epoch #46, step 100, discriminator loss=0.685 , generator loss=0.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #46, step 101, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #46, step 102, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #46, step 103, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #46, step 104, discriminator loss=0.687 , generator loss=0.725\n",
      "Training progress in epoch #46, step 105, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #46, step 106, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #46, step 107, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #46, step 108, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #46, step 109, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #46, step 110, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #46, step 111, discriminator loss=0.697 , generator loss=0.688\n",
      "Training progress in epoch #46, step 112, discriminator loss=0.683 , generator loss=0.690\n",
      "Training progress in epoch #46, step 113, discriminator loss=0.686 , generator loss=0.683\n",
      "Training progress in epoch #46, step 114, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #46, step 115, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #46, step 116, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #46, step 117, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #46, step 118, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #46, step 119, discriminator loss=0.682 , generator loss=0.679\n",
      "Training progress in epoch #46, step 120, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #46, step 121, discriminator loss=0.686 , generator loss=0.732\n",
      "Training progress in epoch #46, step 122, discriminator loss=0.683 , generator loss=0.737\n",
      "Training progress in epoch #46, step 123, discriminator loss=0.691 , generator loss=0.735\n",
      "Training progress in epoch #46, step 124, discriminator loss=0.682 , generator loss=0.722\n",
      "Training progress in epoch #46, step 125, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #46, step 126, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #46, step 127, discriminator loss=0.694 , generator loss=0.674\n",
      "Training progress in epoch #46, step 128, discriminator loss=0.687 , generator loss=0.677\n",
      "Training progress in epoch #46, step 129, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #46, step 130, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #46, step 131, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #46, step 132, discriminator loss=0.684 , generator loss=0.709\n",
      "Training progress in epoch #46, step 133, discriminator loss=0.693 , generator loss=0.757\n",
      "Training progress in epoch #46, step 134, discriminator loss=0.689 , generator loss=0.742\n",
      "Training progress in epoch #46, step 135, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #46, step 136, discriminator loss=0.687 , generator loss=0.665\n",
      "Training progress in epoch #46, step 137, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #46, step 138, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #46, step 139, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #46, step 140, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #46, step 141, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #46, step 142, discriminator loss=0.685 , generator loss=0.728\n",
      "Training progress in epoch #46, step 143, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #46, step 144, discriminator loss=0.682 , generator loss=0.712\n",
      "Training progress in epoch #46, step 145, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #46, step 146, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #46, step 147, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #46, step 148, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #46, step 149, discriminator loss=0.696 , generator loss=0.732\n",
      "Training progress in epoch #46, step 150, discriminator loss=0.697 , generator loss=0.703\n",
      "Training progress in epoch #46, step 151, discriminator loss=0.689 , generator loss=0.666\n",
      "Training progress in epoch #46, step 152, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #46, step 153, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #46, step 154, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #46, step 155, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #46, step 156, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #46, step 157, discriminator loss=0.685 , generator loss=0.720\n",
      "Training progress in epoch #46, step 158, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #46, step 159, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #46, step 160, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #46, step 161, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #46, step 162, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #46, step 163, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #46, step 164, discriminator loss=0.685 , generator loss=0.698\n",
      "Training progress in epoch #46, step 165, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #46, step 166, discriminator loss=0.684 , generator loss=0.700\n",
      "Training progress in epoch #46, step 167, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #46, step 168, discriminator loss=0.684 , generator loss=0.689\n",
      "Training progress in epoch #46, step 169, discriminator loss=0.684 , generator loss=0.692\n",
      "Training progress in epoch #46, step 170, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #46, step 171, discriminator loss=0.689 , generator loss=0.754\n",
      "Training progress in epoch #46, step 172, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #46, step 173, discriminator loss=0.690 , generator loss=0.677\n",
      "Training progress in epoch #46, step 174, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #46, step 175, discriminator loss=0.683 , generator loss=0.719\n",
      "Training progress in epoch #46, step 176, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #46, step 177, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #46, step 178, discriminator loss=0.684 , generator loss=0.690\n",
      "Training progress in epoch #46, step 179, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #46, step 180, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #46, step 181, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #46, step 182, discriminator loss=0.696 , generator loss=0.721\n",
      "Training progress in epoch #46, step 183, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #46, step 184, discriminator loss=0.699 , generator loss=0.707\n",
      "Training progress in epoch #46, step 185, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #46, step 186, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #46, step 187, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #46, step 188, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #46, step 189, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #46, step 190, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #46, step 191, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #46, step 192, discriminator loss=0.686 , generator loss=0.731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #46, step 193, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #46, step 194, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #46, step 195, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #46, step 196, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #46, step 197, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #46, step 198, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #46, step 199, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #46, step 200, discriminator loss=0.681 , generator loss=0.705\n",
      "Training progress in epoch #46, step 201, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #46, step 202, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #46, step 203, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #46, step 204, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #46, step 205, discriminator loss=0.686 , generator loss=0.687\n",
      "Training progress in epoch #46, step 206, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #46, step 207, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #46, step 208, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #46, step 209, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #46, step 210, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #46, step 211, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #46, step 212, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #46, step 213, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #46, step 214, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #46, step 215, discriminator loss=0.698 , generator loss=0.726\n",
      "Training progress in epoch #46, step 216, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #46, step 217, discriminator loss=0.698 , generator loss=0.730\n",
      "Training progress in epoch #46, step 218, discriminator loss=0.685 , generator loss=0.667\n",
      "Training progress in epoch #46, step 219, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #46, step 220, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #46, step 221, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #46, step 222, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #46, step 223, discriminator loss=0.686 , generator loss=0.726\n",
      "Training progress in epoch #46, step 224, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #46, step 225, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #46, step 226, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #46, step 227, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #46, step 228, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #46, step 229, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #46, step 230, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #46, step 231, discriminator loss=0.695 , generator loss=0.681\n",
      "Training progress in epoch #46, step 232, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #46, step 233, discriminator loss=0.694 , generator loss=0.729\n",
      "Disciminator Accuracy on real images: 26%, on fake images: 95%\n",
      "Training progress in epoch #47, step 0, discriminator loss=0.687 , generator loss=0.742\n",
      "Training progress in epoch #47, step 1, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #47, step 2, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #47, step 3, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #47, step 4, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #47, step 5, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #47, step 6, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #47, step 7, discriminator loss=0.696 , generator loss=0.682\n",
      "Training progress in epoch #47, step 8, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #47, step 9, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #47, step 10, discriminator loss=0.695 , generator loss=0.735\n",
      "Training progress in epoch #47, step 11, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #47, step 12, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #47, step 13, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #47, step 14, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #47, step 15, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #47, step 16, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #47, step 17, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #47, step 18, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #47, step 19, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #47, step 20, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #47, step 21, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #47, step 22, discriminator loss=0.684 , generator loss=0.710\n",
      "Training progress in epoch #47, step 23, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #47, step 24, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #47, step 25, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #47, step 26, discriminator loss=0.683 , generator loss=0.707\n",
      "Training progress in epoch #47, step 27, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #47, step 28, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #47, step 29, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #47, step 30, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #47, step 31, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #47, step 32, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #47, step 33, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #47, step 34, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #47, step 35, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #47, step 36, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #47, step 37, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #47, step 38, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #47, step 39, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #47, step 40, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #47, step 41, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #47, step 42, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #47, step 43, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #47, step 44, discriminator loss=0.684 , generator loss=0.698\n",
      "Training progress in epoch #47, step 45, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #47, step 46, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #47, step 47, discriminator loss=0.686 , generator loss=0.727\n",
      "Training progress in epoch #47, step 48, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #47, step 49, discriminator loss=0.687 , generator loss=0.726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #47, step 50, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #47, step 51, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #47, step 52, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #47, step 53, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #47, step 54, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #47, step 55, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #47, step 56, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #47, step 57, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #47, step 58, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #47, step 59, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #47, step 60, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #47, step 61, discriminator loss=0.689 , generator loss=0.676\n",
      "Training progress in epoch #47, step 62, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #47, step 63, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #47, step 64, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #47, step 65, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #47, step 66, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #47, step 67, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #47, step 68, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #47, step 69, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #47, step 70, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #47, step 71, discriminator loss=0.683 , generator loss=0.696\n",
      "Training progress in epoch #47, step 72, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #47, step 73, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #47, step 74, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #47, step 75, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #47, step 76, discriminator loss=0.693 , generator loss=0.678\n",
      "Training progress in epoch #47, step 77, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #47, step 78, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #47, step 79, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #47, step 80, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #47, step 81, discriminator loss=0.688 , generator loss=0.736\n",
      "Training progress in epoch #47, step 82, discriminator loss=0.687 , generator loss=0.727\n",
      "Training progress in epoch #47, step 83, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #47, step 84, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #47, step 85, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #47, step 86, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #47, step 87, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #47, step 88, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #47, step 89, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #47, step 90, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #47, step 91, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #47, step 92, discriminator loss=0.680 , generator loss=0.681\n",
      "Training progress in epoch #47, step 93, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #47, step 94, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #47, step 95, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #47, step 96, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #47, step 97, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #47, step 98, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #47, step 99, discriminator loss=0.689 , generator loss=0.673\n",
      "Training progress in epoch #47, step 100, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #47, step 101, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #47, step 102, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #47, step 103, discriminator loss=0.698 , generator loss=0.717\n",
      "Training progress in epoch #47, step 104, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #47, step 105, discriminator loss=0.697 , generator loss=0.707\n",
      "Training progress in epoch #47, step 106, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #47, step 107, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #47, step 108, discriminator loss=0.685 , generator loss=0.683\n",
      "Training progress in epoch #47, step 109, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #47, step 110, discriminator loss=0.692 , generator loss=0.731\n",
      "Training progress in epoch #47, step 111, discriminator loss=0.697 , generator loss=0.725\n",
      "Training progress in epoch #47, step 112, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #47, step 113, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #47, step 114, discriminator loss=0.685 , generator loss=0.710\n",
      "Training progress in epoch #47, step 115, discriminator loss=0.687 , generator loss=0.722\n",
      "Training progress in epoch #47, step 116, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #47, step 117, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #47, step 118, discriminator loss=0.695 , generator loss=0.673\n",
      "Training progress in epoch #47, step 119, discriminator loss=0.688 , generator loss=0.670\n",
      "Training progress in epoch #47, step 120, discriminator loss=0.698 , generator loss=0.684\n",
      "Training progress in epoch #47, step 121, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #47, step 122, discriminator loss=0.705 , generator loss=0.742\n",
      "Training progress in epoch #47, step 123, discriminator loss=0.683 , generator loss=0.728\n",
      "Training progress in epoch #47, step 124, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #47, step 125, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #47, step 126, discriminator loss=0.685 , generator loss=0.701\n",
      "Training progress in epoch #47, step 127, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #47, step 128, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #47, step 129, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #47, step 130, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #47, step 131, discriminator loss=0.685 , generator loss=0.709\n",
      "Training progress in epoch #47, step 132, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #47, step 133, discriminator loss=0.696 , generator loss=0.728\n",
      "Training progress in epoch #47, step 134, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #47, step 135, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #47, step 136, discriminator loss=0.693 , generator loss=0.730\n",
      "Training progress in epoch #47, step 137, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #47, step 138, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #47, step 139, discriminator loss=0.687 , generator loss=0.674\n",
      "Training progress in epoch #47, step 140, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #47, step 141, discriminator loss=0.684 , generator loss=0.701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #47, step 142, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #47, step 143, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #47, step 144, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #47, step 145, discriminator loss=0.689 , generator loss=0.733\n",
      "Training progress in epoch #47, step 146, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #47, step 147, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #47, step 148, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #47, step 149, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #47, step 150, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #47, step 151, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #47, step 152, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #47, step 153, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #47, step 154, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #47, step 155, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #47, step 156, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #47, step 157, discriminator loss=0.682 , generator loss=0.697\n",
      "Training progress in epoch #47, step 158, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #47, step 159, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #47, step 160, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #47, step 161, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #47, step 162, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #47, step 163, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #47, step 164, discriminator loss=0.696 , generator loss=0.695\n",
      "Training progress in epoch #47, step 165, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #47, step 166, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #47, step 167, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #47, step 168, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #47, step 169, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #47, step 170, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #47, step 171, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #47, step 172, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #47, step 173, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #47, step 174, discriminator loss=0.695 , generator loss=0.730\n",
      "Training progress in epoch #47, step 175, discriminator loss=0.684 , generator loss=0.745\n",
      "Training progress in epoch #47, step 176, discriminator loss=0.691 , generator loss=0.740\n",
      "Training progress in epoch #47, step 177, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #47, step 178, discriminator loss=0.691 , generator loss=0.670\n",
      "Training progress in epoch #47, step 179, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #47, step 180, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #47, step 181, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #47, step 182, discriminator loss=0.698 , generator loss=0.718\n",
      "Training progress in epoch #47, step 183, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #47, step 184, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #47, step 185, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #47, step 186, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #47, step 187, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #47, step 188, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #47, step 189, discriminator loss=0.701 , generator loss=0.738\n",
      "Training progress in epoch #47, step 190, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #47, step 191, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #47, step 192, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #47, step 193, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #47, step 194, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #47, step 195, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #47, step 196, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #47, step 197, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #47, step 198, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #47, step 199, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #47, step 200, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #47, step 201, discriminator loss=0.701 , generator loss=0.706\n",
      "Training progress in epoch #47, step 202, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #47, step 203, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #47, step 204, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #47, step 205, discriminator loss=0.689 , generator loss=0.662\n",
      "Training progress in epoch #47, step 206, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #47, step 207, discriminator loss=0.696 , generator loss=0.719\n",
      "Training progress in epoch #47, step 208, discriminator loss=0.688 , generator loss=0.741\n",
      "Training progress in epoch #47, step 209, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #47, step 210, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #47, step 211, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #47, step 212, discriminator loss=0.691 , generator loss=0.672\n",
      "Training progress in epoch #47, step 213, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #47, step 214, discriminator loss=0.694 , generator loss=0.733\n",
      "Training progress in epoch #47, step 215, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #47, step 216, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #47, step 217, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #47, step 218, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #47, step 219, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #47, step 220, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #47, step 221, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #47, step 222, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #47, step 223, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #47, step 224, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #47, step 225, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #47, step 226, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #47, step 227, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #47, step 228, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #47, step 229, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #47, step 230, discriminator loss=0.681 , generator loss=0.718\n",
      "Training progress in epoch #47, step 231, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #47, step 232, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #47, step 233, discriminator loss=0.693 , generator loss=0.688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disciminator Accuracy on real images: 84%, on fake images: 33%\n",
      "Training progress in epoch #48, step 0, discriminator loss=0.681 , generator loss=0.696\n",
      "Training progress in epoch #48, step 1, discriminator loss=0.686 , generator loss=0.728\n",
      "Training progress in epoch #48, step 2, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #48, step 3, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #48, step 4, discriminator loss=0.688 , generator loss=0.675\n",
      "Training progress in epoch #48, step 5, discriminator loss=0.692 , generator loss=0.676\n",
      "Training progress in epoch #48, step 6, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #48, step 7, discriminator loss=0.695 , generator loss=0.734\n",
      "Training progress in epoch #48, step 8, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #48, step 9, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #48, step 10, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #48, step 11, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #48, step 12, discriminator loss=0.699 , generator loss=0.699\n",
      "Training progress in epoch #48, step 13, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #48, step 14, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #48, step 15, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #48, step 16, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #48, step 17, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #48, step 18, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #48, step 19, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #48, step 20, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #48, step 21, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #48, step 22, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #48, step 23, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #48, step 24, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #48, step 25, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #48, step 26, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #48, step 27, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #48, step 28, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #48, step 29, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #48, step 30, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #48, step 31, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #48, step 32, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #48, step 33, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #48, step 34, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #48, step 35, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #48, step 36, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #48, step 37, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #48, step 38, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #48, step 39, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #48, step 40, discriminator loss=0.689 , generator loss=0.741\n",
      "Training progress in epoch #48, step 41, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #48, step 42, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #48, step 43, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #48, step 44, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #48, step 45, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #48, step 46, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #48, step 47, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #48, step 48, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #48, step 49, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #48, step 50, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #48, step 51, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #48, step 52, discriminator loss=0.694 , generator loss=0.735\n",
      "Training progress in epoch #48, step 53, discriminator loss=0.692 , generator loss=0.741\n",
      "Training progress in epoch #48, step 54, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #48, step 55, discriminator loss=0.698 , generator loss=0.688\n",
      "Training progress in epoch #48, step 56, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #48, step 57, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #48, step 58, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #48, step 59, discriminator loss=0.689 , generator loss=0.678\n",
      "Training progress in epoch #48, step 60, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #48, step 61, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #48, step 62, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #48, step 63, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #48, step 64, discriminator loss=0.683 , generator loss=0.713\n",
      "Training progress in epoch #48, step 65, discriminator loss=0.684 , generator loss=0.727\n",
      "Training progress in epoch #48, step 66, discriminator loss=0.694 , generator loss=0.724\n",
      "Training progress in epoch #48, step 67, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #48, step 68, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #48, step 69, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #48, step 70, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #48, step 71, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #48, step 72, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #48, step 73, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #48, step 74, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #48, step 75, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #48, step 76, discriminator loss=0.693 , generator loss=0.736\n",
      "Training progress in epoch #48, step 77, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #48, step 78, discriminator loss=0.695 , generator loss=0.675\n",
      "Training progress in epoch #48, step 79, discriminator loss=0.693 , generator loss=0.677\n",
      "Training progress in epoch #48, step 80, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #48, step 81, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #48, step 82, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #48, step 83, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #48, step 84, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #48, step 85, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #48, step 86, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #48, step 87, discriminator loss=0.682 , generator loss=0.724\n",
      "Training progress in epoch #48, step 88, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #48, step 89, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #48, step 90, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #48, step 91, discriminator loss=0.693 , generator loss=0.699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #48, step 92, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #48, step 93, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #48, step 94, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #48, step 95, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #48, step 96, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #48, step 97, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #48, step 98, discriminator loss=0.684 , generator loss=0.733\n",
      "Training progress in epoch #48, step 99, discriminator loss=0.691 , generator loss=0.738\n",
      "Training progress in epoch #48, step 100, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #48, step 101, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #48, step 102, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #48, step 103, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #48, step 104, discriminator loss=0.687 , generator loss=0.679\n",
      "Training progress in epoch #48, step 105, discriminator loss=0.699 , generator loss=0.699\n",
      "Training progress in epoch #48, step 106, discriminator loss=0.682 , generator loss=0.721\n",
      "Training progress in epoch #48, step 107, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #48, step 108, discriminator loss=0.683 , generator loss=0.714\n",
      "Training progress in epoch #48, step 109, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #48, step 110, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #48, step 111, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #48, step 112, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #48, step 113, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #48, step 114, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #48, step 115, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #48, step 116, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #48, step 117, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #48, step 118, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #48, step 119, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #48, step 120, discriminator loss=0.698 , generator loss=0.709\n",
      "Training progress in epoch #48, step 121, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #48, step 122, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #48, step 123, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #48, step 124, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #48, step 125, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #48, step 126, discriminator loss=0.684 , generator loss=0.732\n",
      "Training progress in epoch #48, step 127, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #48, step 128, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #48, step 129, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #48, step 130, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #48, step 131, discriminator loss=0.682 , generator loss=0.707\n",
      "Training progress in epoch #48, step 132, discriminator loss=0.685 , generator loss=0.692\n",
      "Training progress in epoch #48, step 133, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #48, step 134, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #48, step 135, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #48, step 136, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #48, step 137, discriminator loss=0.697 , generator loss=0.716\n",
      "Training progress in epoch #48, step 138, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #48, step 139, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #48, step 140, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #48, step 141, discriminator loss=0.695 , generator loss=0.715\n",
      "Training progress in epoch #48, step 142, discriminator loss=0.686 , generator loss=0.685\n",
      "Training progress in epoch #48, step 143, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #48, step 144, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #48, step 145, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #48, step 146, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #48, step 147, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #48, step 148, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #48, step 149, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #48, step 150, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #48, step 151, discriminator loss=0.692 , generator loss=0.733\n",
      "Training progress in epoch #48, step 152, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #48, step 153, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #48, step 154, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #48, step 155, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #48, step 156, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #48, step 157, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #48, step 158, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #48, step 159, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #48, step 160, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #48, step 161, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #48, step 162, discriminator loss=0.682 , generator loss=0.733\n",
      "Training progress in epoch #48, step 163, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #48, step 164, discriminator loss=0.693 , generator loss=0.676\n",
      "Training progress in epoch #48, step 165, discriminator loss=0.693 , generator loss=0.677\n",
      "Training progress in epoch #48, step 166, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #48, step 167, discriminator loss=0.686 , generator loss=0.731\n",
      "Training progress in epoch #48, step 168, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #48, step 169, discriminator loss=0.691 , generator loss=0.675\n",
      "Training progress in epoch #48, step 170, discriminator loss=0.688 , generator loss=0.673\n",
      "Training progress in epoch #48, step 171, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #48, step 172, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #48, step 173, discriminator loss=0.693 , generator loss=0.732\n",
      "Training progress in epoch #48, step 174, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #48, step 175, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #48, step 176, discriminator loss=0.681 , generator loss=0.702\n",
      "Training progress in epoch #48, step 177, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #48, step 178, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #48, step 179, discriminator loss=0.688 , generator loss=0.677\n",
      "Training progress in epoch #48, step 180, discriminator loss=0.685 , generator loss=0.676\n",
      "Training progress in epoch #48, step 181, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #48, step 182, discriminator loss=0.685 , generator loss=0.745\n",
      "Training progress in epoch #48, step 183, discriminator loss=0.692 , generator loss=0.719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #48, step 184, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #48, step 185, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #48, step 186, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #48, step 187, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #48, step 188, discriminator loss=0.690 , generator loss=0.678\n",
      "Training progress in epoch #48, step 189, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #48, step 190, discriminator loss=0.697 , generator loss=0.730\n",
      "Training progress in epoch #48, step 191, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #48, step 192, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #48, step 193, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #48, step 194, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #48, step 195, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #48, step 196, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #48, step 197, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #48, step 198, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #48, step 199, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #48, step 200, discriminator loss=0.691 , generator loss=0.727\n",
      "Training progress in epoch #48, step 201, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #48, step 202, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #48, step 203, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #48, step 204, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #48, step 205, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #48, step 206, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #48, step 207, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #48, step 208, discriminator loss=0.698 , generator loss=0.703\n",
      "Training progress in epoch #48, step 209, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #48, step 210, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #48, step 211, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #48, step 212, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #48, step 213, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #48, step 214, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #48, step 215, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #48, step 216, discriminator loss=0.685 , generator loss=0.712\n",
      "Training progress in epoch #48, step 217, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #48, step 218, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #48, step 219, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #48, step 220, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #48, step 221, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #48, step 222, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #48, step 223, discriminator loss=0.692 , generator loss=0.741\n",
      "Training progress in epoch #48, step 224, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #48, step 225, discriminator loss=0.698 , generator loss=0.693\n",
      "Training progress in epoch #48, step 226, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #48, step 227, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #48, step 228, discriminator loss=0.696 , generator loss=0.729\n",
      "Training progress in epoch #48, step 229, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #48, step 230, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #48, step 231, discriminator loss=0.686 , generator loss=0.673\n",
      "Training progress in epoch #48, step 232, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #48, step 233, discriminator loss=0.684 , generator loss=0.702\n",
      "Disciminator Accuracy on real images: 65%, on fake images: 78%\n",
      "Training progress in epoch #49, step 0, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #49, step 1, discriminator loss=0.694 , generator loss=0.746\n",
      "Training progress in epoch #49, step 2, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #49, step 3, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #49, step 4, discriminator loss=0.685 , generator loss=0.679\n",
      "Training progress in epoch #49, step 5, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #49, step 6, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #49, step 7, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #49, step 8, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #49, step 9, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #49, step 10, discriminator loss=0.696 , generator loss=0.681\n",
      "Training progress in epoch #49, step 11, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #49, step 12, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #49, step 13, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #49, step 14, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #49, step 15, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #49, step 16, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #49, step 17, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #49, step 18, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #49, step 19, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #49, step 20, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #49, step 21, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #49, step 22, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #49, step 23, discriminator loss=0.686 , generator loss=0.717\n",
      "Training progress in epoch #49, step 24, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #49, step 25, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #49, step 26, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #49, step 27, discriminator loss=0.686 , generator loss=0.682\n",
      "Training progress in epoch #49, step 28, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #49, step 29, discriminator loss=0.682 , generator loss=0.736\n",
      "Training progress in epoch #49, step 30, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #49, step 31, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #49, step 32, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #49, step 33, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #49, step 34, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #49, step 35, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #49, step 36, discriminator loss=0.683 , generator loss=0.708\n",
      "Training progress in epoch #49, step 37, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #49, step 38, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #49, step 39, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #49, step 40, discriminator loss=0.693 , generator loss=0.727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #49, step 41, discriminator loss=0.698 , generator loss=0.726\n",
      "Training progress in epoch #49, step 42, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #49, step 43, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #49, step 44, discriminator loss=0.690 , generator loss=0.679\n",
      "Training progress in epoch #49, step 45, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #49, step 46, discriminator loss=0.694 , generator loss=0.740\n",
      "Training progress in epoch #49, step 47, discriminator loss=0.683 , generator loss=0.727\n",
      "Training progress in epoch #49, step 48, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #49, step 49, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #49, step 50, discriminator loss=0.680 , generator loss=0.690\n",
      "Training progress in epoch #49, step 51, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #49, step 52, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #49, step 53, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #49, step 54, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #49, step 55, discriminator loss=0.693 , generator loss=0.743\n",
      "Training progress in epoch #49, step 56, discriminator loss=0.698 , generator loss=0.728\n",
      "Training progress in epoch #49, step 57, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #49, step 58, discriminator loss=0.689 , generator loss=0.676\n",
      "Training progress in epoch #49, step 59, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #49, step 60, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #49, step 61, discriminator loss=0.685 , generator loss=0.701\n",
      "Training progress in epoch #49, step 62, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #49, step 63, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #49, step 64, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #49, step 65, discriminator loss=0.696 , generator loss=0.734\n",
      "Training progress in epoch #49, step 66, discriminator loss=0.680 , generator loss=0.708\n",
      "Training progress in epoch #49, step 67, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #49, step 68, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #49, step 69, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #49, step 70, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #49, step 71, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #49, step 72, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #49, step 73, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #49, step 74, discriminator loss=0.699 , generator loss=0.689\n",
      "Training progress in epoch #49, step 75, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #49, step 76, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #49, step 77, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #49, step 78, discriminator loss=0.682 , generator loss=0.701\n",
      "Training progress in epoch #49, step 79, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #49, step 80, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #49, step 81, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #49, step 82, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #49, step 83, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #49, step 84, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #49, step 85, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #49, step 86, discriminator loss=0.686 , generator loss=0.730\n",
      "Training progress in epoch #49, step 87, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #49, step 88, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #49, step 89, discriminator loss=0.694 , generator loss=0.679\n",
      "Training progress in epoch #49, step 90, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #49, step 91, discriminator loss=0.694 , generator loss=0.721\n",
      "Training progress in epoch #49, step 92, discriminator loss=0.689 , generator loss=0.747\n",
      "Training progress in epoch #49, step 93, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #49, step 94, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #49, step 95, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #49, step 96, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #49, step 97, discriminator loss=0.696 , generator loss=0.728\n",
      "Training progress in epoch #49, step 98, discriminator loss=0.684 , generator loss=0.703\n",
      "Training progress in epoch #49, step 99, discriminator loss=0.693 , generator loss=0.673\n",
      "Training progress in epoch #49, step 100, discriminator loss=0.685 , generator loss=0.680\n",
      "Training progress in epoch #49, step 101, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #49, step 102, discriminator loss=0.684 , generator loss=0.728\n",
      "Training progress in epoch #49, step 103, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #49, step 104, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #49, step 105, discriminator loss=0.683 , generator loss=0.681\n",
      "Training progress in epoch #49, step 106, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #49, step 107, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #49, step 108, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #49, step 109, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #49, step 110, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #49, step 111, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #49, step 112, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #49, step 113, discriminator loss=0.683 , generator loss=0.695\n",
      "Training progress in epoch #49, step 114, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #49, step 115, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #49, step 116, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #49, step 117, discriminator loss=0.693 , generator loss=0.730\n",
      "Training progress in epoch #49, step 118, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #49, step 119, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #49, step 120, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #49, step 121, discriminator loss=0.700 , generator loss=0.705\n",
      "Training progress in epoch #49, step 122, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #49, step 123, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #49, step 124, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #49, step 125, discriminator loss=0.693 , generator loss=0.742\n",
      "Training progress in epoch #49, step 126, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #49, step 127, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #49, step 128, discriminator loss=0.695 , generator loss=0.715\n",
      "Training progress in epoch #49, step 129, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #49, step 130, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #49, step 131, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #49, step 132, discriminator loss=0.690 , generator loss=0.719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #49, step 133, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #49, step 134, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #49, step 135, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #49, step 136, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #49, step 137, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #49, step 138, discriminator loss=0.682 , generator loss=0.713\n",
      "Training progress in epoch #49, step 139, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #49, step 140, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #49, step 141, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #49, step 142, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #49, step 143, discriminator loss=0.697 , generator loss=0.694\n",
      "Training progress in epoch #49, step 144, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #49, step 145, discriminator loss=0.682 , generator loss=0.724\n",
      "Training progress in epoch #49, step 146, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #49, step 147, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #49, step 148, discriminator loss=0.688 , generator loss=0.682\n",
      "Training progress in epoch #49, step 149, discriminator loss=0.685 , generator loss=0.686\n",
      "Training progress in epoch #49, step 150, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #49, step 151, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #49, step 152, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #49, step 153, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #49, step 154, discriminator loss=0.680 , generator loss=0.709\n",
      "Training progress in epoch #49, step 155, discriminator loss=0.698 , generator loss=0.726\n",
      "Training progress in epoch #49, step 156, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #49, step 157, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #49, step 158, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #49, step 159, discriminator loss=0.700 , generator loss=0.708\n",
      "Training progress in epoch #49, step 160, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #49, step 161, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #49, step 162, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #49, step 163, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #49, step 164, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #49, step 165, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #49, step 166, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #49, step 167, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #49, step 168, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #49, step 169, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #49, step 170, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #49, step 171, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #49, step 172, discriminator loss=0.699 , generator loss=0.702\n",
      "Training progress in epoch #49, step 173, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #49, step 174, discriminator loss=0.686 , generator loss=0.739\n",
      "Training progress in epoch #49, step 175, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #49, step 176, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #49, step 177, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #49, step 178, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #49, step 179, discriminator loss=0.684 , generator loss=0.707\n",
      "Training progress in epoch #49, step 180, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #49, step 181, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #49, step 182, discriminator loss=0.697 , generator loss=0.727\n",
      "Training progress in epoch #49, step 183, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #49, step 184, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #49, step 185, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #49, step 186, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #49, step 187, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #49, step 188, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #49, step 189, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #49, step 190, discriminator loss=0.682 , generator loss=0.714\n",
      "Training progress in epoch #49, step 191, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #49, step 192, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #49, step 193, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #49, step 194, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #49, step 195, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #49, step 196, discriminator loss=0.689 , generator loss=0.741\n",
      "Training progress in epoch #49, step 197, discriminator loss=0.688 , generator loss=0.725\n",
      "Training progress in epoch #49, step 198, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #49, step 199, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #49, step 200, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #49, step 201, discriminator loss=0.686 , generator loss=0.685\n",
      "Training progress in epoch #49, step 202, discriminator loss=0.693 , generator loss=0.676\n",
      "Training progress in epoch #49, step 203, discriminator loss=0.699 , generator loss=0.680\n",
      "Training progress in epoch #49, step 204, discriminator loss=0.699 , generator loss=0.705\n",
      "Training progress in epoch #49, step 205, discriminator loss=0.693 , generator loss=0.735\n",
      "Training progress in epoch #49, step 206, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #49, step 207, discriminator loss=0.695 , generator loss=0.716\n",
      "Training progress in epoch #49, step 208, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #49, step 209, discriminator loss=0.685 , generator loss=0.721\n",
      "Training progress in epoch #49, step 210, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #49, step 211, discriminator loss=0.682 , generator loss=0.689\n",
      "Training progress in epoch #49, step 212, discriminator loss=0.688 , generator loss=0.680\n",
      "Training progress in epoch #49, step 213, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #49, step 214, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #49, step 215, discriminator loss=0.688 , generator loss=0.724\n",
      "Training progress in epoch #49, step 216, discriminator loss=0.680 , generator loss=0.704\n",
      "Training progress in epoch #49, step 217, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #49, step 218, discriminator loss=0.695 , generator loss=0.719\n",
      "Training progress in epoch #49, step 219, discriminator loss=0.695 , generator loss=0.732\n",
      "Training progress in epoch #49, step 220, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #49, step 221, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #49, step 222, discriminator loss=0.695 , generator loss=0.675\n",
      "Training progress in epoch #49, step 223, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #49, step 224, discriminator loss=0.692 , generator loss=0.682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #49, step 225, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #49, step 226, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #49, step 227, discriminator loss=0.692 , generator loss=0.732\n",
      "Training progress in epoch #49, step 228, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #49, step 229, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #49, step 230, discriminator loss=0.684 , generator loss=0.731\n",
      "Training progress in epoch #49, step 231, discriminator loss=0.686 , generator loss=0.724\n",
      "Training progress in epoch #49, step 232, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #49, step 233, discriminator loss=0.691 , generator loss=0.673\n",
      "Disciminator Accuracy on real images: 89%, on fake images: 36%\n",
      "Training progress in epoch #50, step 0, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #50, step 1, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #50, step 2, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #50, step 3, discriminator loss=0.684 , generator loss=0.734\n",
      "Training progress in epoch #50, step 4, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #50, step 5, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #50, step 6, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #50, step 7, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #50, step 8, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #50, step 9, discriminator loss=0.686 , generator loss=0.736\n",
      "Training progress in epoch #50, step 10, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #50, step 11, discriminator loss=0.689 , generator loss=0.672\n",
      "Training progress in epoch #50, step 12, discriminator loss=0.688 , generator loss=0.673\n",
      "Training progress in epoch #50, step 13, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #50, step 14, discriminator loss=0.690 , generator loss=0.746\n",
      "Training progress in epoch #50, step 15, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #50, step 16, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #50, step 17, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #50, step 18, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #50, step 19, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #50, step 20, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #50, step 21, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #50, step 22, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #50, step 23, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #50, step 24, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #50, step 25, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #50, step 26, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #50, step 27, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #50, step 28, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #50, step 29, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #50, step 30, discriminator loss=0.698 , generator loss=0.726\n",
      "Training progress in epoch #50, step 31, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #50, step 32, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #50, step 33, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #50, step 34, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #50, step 35, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #50, step 36, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #50, step 37, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #50, step 38, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #50, step 39, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #50, step 40, discriminator loss=0.696 , generator loss=0.725\n",
      "Training progress in epoch #50, step 41, discriminator loss=0.682 , generator loss=0.723\n",
      "Training progress in epoch #50, step 42, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #50, step 43, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #50, step 44, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #50, step 45, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #50, step 46, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #50, step 47, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #50, step 48, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #50, step 49, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #50, step 50, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #50, step 51, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #50, step 52, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #50, step 53, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #50, step 54, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #50, step 55, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #50, step 56, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #50, step 57, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #50, step 58, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #50, step 59, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #50, step 60, discriminator loss=0.688 , generator loss=0.676\n",
      "Training progress in epoch #50, step 61, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #50, step 62, discriminator loss=0.697 , generator loss=0.717\n",
      "Training progress in epoch #50, step 63, discriminator loss=0.688 , generator loss=0.742\n",
      "Training progress in epoch #50, step 64, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #50, step 65, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #50, step 66, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #50, step 67, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #50, step 68, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #50, step 69, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #50, step 70, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #50, step 71, discriminator loss=0.686 , generator loss=0.677\n",
      "Training progress in epoch #50, step 72, discriminator loss=0.686 , generator loss=0.677\n",
      "Training progress in epoch #50, step 73, discriminator loss=0.701 , generator loss=0.723\n",
      "Training progress in epoch #50, step 74, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #50, step 75, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #50, step 76, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #50, step 77, discriminator loss=0.697 , generator loss=0.692\n",
      "Training progress in epoch #50, step 78, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #50, step 79, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #50, step 80, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #50, step 81, discriminator loss=0.695 , generator loss=0.725\n",
      "Training progress in epoch #50, step 82, discriminator loss=0.690 , generator loss=0.738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #50, step 83, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #50, step 84, discriminator loss=0.697 , generator loss=0.715\n",
      "Training progress in epoch #50, step 85, discriminator loss=0.698 , generator loss=0.693\n",
      "Training progress in epoch #50, step 86, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #50, step 87, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #50, step 88, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #50, step 89, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #50, step 90, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #50, step 91, discriminator loss=0.686 , generator loss=0.724\n",
      "Training progress in epoch #50, step 92, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #50, step 93, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #50, step 94, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #50, step 95, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #50, step 96, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #50, step 97, discriminator loss=0.698 , generator loss=0.692\n",
      "Training progress in epoch #50, step 98, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #50, step 99, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #50, step 100, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #50, step 101, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #50, step 102, discriminator loss=0.695 , generator loss=0.731\n",
      "Training progress in epoch #50, step 103, discriminator loss=0.695 , generator loss=0.741\n",
      "Training progress in epoch #50, step 104, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #50, step 105, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #50, step 106, discriminator loss=0.694 , generator loss=0.675\n",
      "Training progress in epoch #50, step 107, discriminator loss=0.696 , generator loss=0.679\n",
      "Training progress in epoch #50, step 108, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #50, step 109, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #50, step 110, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #50, step 111, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #50, step 112, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #50, step 113, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #50, step 114, discriminator loss=0.685 , generator loss=0.711\n",
      "Training progress in epoch #50, step 115, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #50, step 116, discriminator loss=0.685 , generator loss=0.686\n",
      "Training progress in epoch #50, step 117, discriminator loss=0.684 , generator loss=0.672\n",
      "Training progress in epoch #50, step 118, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #50, step 119, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #50, step 120, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #50, step 121, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #50, step 122, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #50, step 123, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #50, step 124, discriminator loss=0.696 , generator loss=0.718\n",
      "Training progress in epoch #50, step 125, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #50, step 126, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #50, step 127, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #50, step 128, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #50, step 129, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #50, step 130, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #50, step 131, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #50, step 132, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #50, step 133, discriminator loss=0.683 , generator loss=0.686\n",
      "Training progress in epoch #50, step 134, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #50, step 135, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #50, step 136, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #50, step 137, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #50, step 138, discriminator loss=0.682 , generator loss=0.704\n",
      "Training progress in epoch #50, step 139, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #50, step 140, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #50, step 141, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #50, step 142, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #50, step 143, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #50, step 144, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #50, step 145, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #50, step 146, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #50, step 147, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #50, step 148, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #50, step 149, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #50, step 150, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #50, step 151, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #50, step 152, discriminator loss=0.694 , generator loss=0.729\n",
      "Training progress in epoch #50, step 153, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #50, step 154, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #50, step 155, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #50, step 156, discriminator loss=0.691 , generator loss=0.672\n",
      "Training progress in epoch #50, step 157, discriminator loss=0.690 , generator loss=0.676\n",
      "Training progress in epoch #50, step 158, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #50, step 159, discriminator loss=0.695 , generator loss=0.731\n",
      "Training progress in epoch #50, step 160, discriminator loss=0.683 , generator loss=0.732\n",
      "Training progress in epoch #50, step 161, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #50, step 162, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #50, step 163, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #50, step 164, discriminator loss=0.688 , generator loss=0.732\n",
      "Training progress in epoch #50, step 165, discriminator loss=0.693 , generator loss=0.731\n",
      "Training progress in epoch #50, step 166, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #50, step 167, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #50, step 168, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #50, step 169, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #50, step 170, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #50, step 171, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #50, step 172, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #50, step 173, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #50, step 174, discriminator loss=0.688 , generator loss=0.720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #50, step 175, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #50, step 176, discriminator loss=0.692 , generator loss=0.735\n",
      "Training progress in epoch #50, step 177, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #50, step 178, discriminator loss=0.691 , generator loss=0.667\n",
      "Training progress in epoch #50, step 179, discriminator loss=0.685 , generator loss=0.682\n",
      "Training progress in epoch #50, step 180, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #50, step 181, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #50, step 182, discriminator loss=0.682 , generator loss=0.746\n",
      "Training progress in epoch #50, step 183, discriminator loss=0.693 , generator loss=0.747\n",
      "Training progress in epoch #50, step 184, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #50, step 185, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #50, step 186, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #50, step 187, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #50, step 188, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #50, step 189, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #50, step 190, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #50, step 191, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #50, step 192, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #50, step 193, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #50, step 194, discriminator loss=0.695 , generator loss=0.732\n",
      "Training progress in epoch #50, step 195, discriminator loss=0.700 , generator loss=0.721\n",
      "Training progress in epoch #50, step 196, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #50, step 197, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #50, step 198, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #50, step 199, discriminator loss=0.688 , generator loss=0.733\n",
      "Training progress in epoch #50, step 200, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #50, step 201, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #50, step 202, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #50, step 203, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #50, step 204, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #50, step 205, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #50, step 206, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #50, step 207, discriminator loss=0.686 , generator loss=0.758\n",
      "Training progress in epoch #50, step 208, discriminator loss=0.683 , generator loss=0.739\n",
      "Training progress in epoch #50, step 209, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #50, step 210, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #50, step 211, discriminator loss=0.687 , generator loss=0.664\n",
      "Training progress in epoch #50, step 212, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #50, step 213, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #50, step 214, discriminator loss=0.701 , generator loss=0.702\n",
      "Training progress in epoch #50, step 215, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #50, step 216, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #50, step 217, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #50, step 218, discriminator loss=0.684 , generator loss=0.705\n",
      "Training progress in epoch #50, step 219, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #50, step 220, discriminator loss=0.685 , generator loss=0.727\n",
      "Training progress in epoch #50, step 221, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #50, step 222, discriminator loss=0.682 , generator loss=0.670\n",
      "Training progress in epoch #50, step 223, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #50, step 224, discriminator loss=0.684 , generator loss=0.724\n",
      "Training progress in epoch #50, step 225, discriminator loss=0.683 , generator loss=0.735\n",
      "Training progress in epoch #50, step 226, discriminator loss=0.689 , generator loss=0.735\n",
      "Training progress in epoch #50, step 227, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #50, step 228, discriminator loss=0.702 , generator loss=0.712\n",
      "Training progress in epoch #50, step 229, discriminator loss=0.700 , generator loss=0.701\n",
      "Training progress in epoch #50, step 230, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #50, step 231, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #50, step 232, discriminator loss=0.695 , generator loss=0.724\n",
      "Training progress in epoch #50, step 233, discriminator loss=0.696 , generator loss=0.698\n",
      "Disciminator Accuracy on real images: 52%, on fake images: 62%\n",
      "Training progress in epoch #51, step 0, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #51, step 1, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #51, step 2, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #51, step 3, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #51, step 4, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #51, step 5, discriminator loss=0.697 , generator loss=0.704\n",
      "Training progress in epoch #51, step 6, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #51, step 7, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #51, step 8, discriminator loss=0.686 , generator loss=0.724\n",
      "Training progress in epoch #51, step 9, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #51, step 10, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #51, step 11, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #51, step 12, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #51, step 13, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #51, step 14, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #51, step 15, discriminator loss=0.697 , generator loss=0.709\n",
      "Training progress in epoch #51, step 16, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #51, step 17, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #51, step 18, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #51, step 19, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #51, step 20, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #51, step 21, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #51, step 22, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #51, step 23, discriminator loss=0.686 , generator loss=0.678\n",
      "Training progress in epoch #51, step 24, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #51, step 25, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #51, step 26, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #51, step 27, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #51, step 28, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #51, step 29, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #51, step 30, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #51, step 31, discriminator loss=0.692 , generator loss=0.694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #51, step 32, discriminator loss=0.688 , generator loss=0.680\n",
      "Training progress in epoch #51, step 33, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #51, step 34, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #51, step 35, discriminator loss=0.695 , generator loss=0.730\n",
      "Training progress in epoch #51, step 36, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #51, step 37, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #51, step 38, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #51, step 39, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #51, step 40, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #51, step 41, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #51, step 42, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #51, step 43, discriminator loss=0.696 , generator loss=0.685\n",
      "Training progress in epoch #51, step 44, discriminator loss=0.696 , generator loss=0.694\n",
      "Training progress in epoch #51, step 45, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #51, step 46, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #51, step 47, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #51, step 48, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #51, step 49, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #51, step 50, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #51, step 51, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #51, step 52, discriminator loss=0.697 , generator loss=0.709\n",
      "Training progress in epoch #51, step 53, discriminator loss=0.690 , generator loss=0.746\n",
      "Training progress in epoch #51, step 54, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #51, step 55, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #51, step 56, discriminator loss=0.684 , generator loss=0.677\n",
      "Training progress in epoch #51, step 57, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #51, step 58, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #51, step 59, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #51, step 60, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #51, step 61, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #51, step 62, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #51, step 63, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #51, step 64, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #51, step 65, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #51, step 66, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #51, step 67, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #51, step 68, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #51, step 69, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #51, step 70, discriminator loss=0.685 , generator loss=0.712\n",
      "Training progress in epoch #51, step 71, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #51, step 72, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #51, step 73, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #51, step 74, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #51, step 75, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #51, step 76, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #51, step 77, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #51, step 78, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #51, step 79, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #51, step 80, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #51, step 81, discriminator loss=0.700 , generator loss=0.719\n",
      "Training progress in epoch #51, step 82, discriminator loss=0.682 , generator loss=0.732\n",
      "Training progress in epoch #51, step 83, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #51, step 84, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #51, step 85, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #51, step 86, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #51, step 87, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #51, step 88, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #51, step 89, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #51, step 90, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #51, step 91, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #51, step 92, discriminator loss=0.697 , generator loss=0.718\n",
      "Training progress in epoch #51, step 93, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #51, step 94, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #51, step 95, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #51, step 96, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #51, step 97, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #51, step 98, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #51, step 99, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #51, step 100, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #51, step 101, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #51, step 102, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #51, step 103, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #51, step 104, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #51, step 105, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #51, step 106, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #51, step 107, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #51, step 108, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #51, step 109, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #51, step 110, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #51, step 111, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #51, step 112, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #51, step 113, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #51, step 114, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #51, step 115, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #51, step 116, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #51, step 117, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #51, step 118, discriminator loss=0.692 , generator loss=0.738\n",
      "Training progress in epoch #51, step 119, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #51, step 120, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #51, step 121, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #51, step 122, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #51, step 123, discriminator loss=0.683 , generator loss=0.735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #51, step 124, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #51, step 125, discriminator loss=0.687 , generator loss=0.674\n",
      "Training progress in epoch #51, step 126, discriminator loss=0.690 , generator loss=0.674\n",
      "Training progress in epoch #51, step 127, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #51, step 128, discriminator loss=0.693 , generator loss=0.731\n",
      "Training progress in epoch #51, step 129, discriminator loss=0.686 , generator loss=0.739\n",
      "Training progress in epoch #51, step 130, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #51, step 131, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #51, step 132, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #51, step 133, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #51, step 134, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #51, step 135, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #51, step 136, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #51, step 137, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #51, step 138, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #51, step 139, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #51, step 140, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #51, step 141, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #51, step 142, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #51, step 143, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #51, step 144, discriminator loss=0.699 , generator loss=0.722\n",
      "Training progress in epoch #51, step 145, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #51, step 146, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #51, step 147, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #51, step 148, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #51, step 149, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #51, step 150, discriminator loss=0.683 , generator loss=0.697\n",
      "Training progress in epoch #51, step 151, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #51, step 152, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #51, step 153, discriminator loss=0.682 , generator loss=0.711\n",
      "Training progress in epoch #51, step 154, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #51, step 155, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #51, step 156, discriminator loss=0.699 , generator loss=0.697\n",
      "Training progress in epoch #51, step 157, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #51, step 158, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #51, step 159, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #51, step 160, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #51, step 161, discriminator loss=0.697 , generator loss=0.706\n",
      "Training progress in epoch #51, step 162, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #51, step 163, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #51, step 164, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #51, step 165, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #51, step 166, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #51, step 167, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #51, step 168, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #51, step 169, discriminator loss=0.682 , generator loss=0.722\n",
      "Training progress in epoch #51, step 170, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #51, step 171, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #51, step 172, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #51, step 173, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #51, step 174, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #51, step 175, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #51, step 176, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #51, step 177, discriminator loss=0.698 , generator loss=0.704\n",
      "Training progress in epoch #51, step 178, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #51, step 179, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #51, step 180, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #51, step 181, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #51, step 182, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #51, step 183, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #51, step 184, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #51, step 185, discriminator loss=0.683 , generator loss=0.700\n",
      "Training progress in epoch #51, step 186, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #51, step 187, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #51, step 188, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #51, step 189, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #51, step 190, discriminator loss=0.683 , generator loss=0.695\n",
      "Training progress in epoch #51, step 191, discriminator loss=0.685 , generator loss=0.698\n",
      "Training progress in epoch #51, step 192, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #51, step 193, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #51, step 194, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #51, step 195, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #51, step 196, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #51, step 197, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #51, step 198, discriminator loss=0.696 , generator loss=0.706\n",
      "Training progress in epoch #51, step 199, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #51, step 200, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #51, step 201, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #51, step 202, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #51, step 203, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #51, step 204, discriminator loss=0.689 , generator loss=0.676\n",
      "Training progress in epoch #51, step 205, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #51, step 206, discriminator loss=0.695 , generator loss=0.726\n",
      "Training progress in epoch #51, step 207, discriminator loss=0.688 , generator loss=0.741\n",
      "Training progress in epoch #51, step 208, discriminator loss=0.700 , generator loss=0.711\n",
      "Training progress in epoch #51, step 209, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #51, step 210, discriminator loss=0.688 , generator loss=0.665\n",
      "Training progress in epoch #51, step 211, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #51, step 212, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #51, step 213, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #51, step 214, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #51, step 215, discriminator loss=0.690 , generator loss=0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #51, step 216, discriminator loss=0.695 , generator loss=0.745\n",
      "Training progress in epoch #51, step 217, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #51, step 218, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #51, step 219, discriminator loss=0.694 , generator loss=0.676\n",
      "Training progress in epoch #51, step 220, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #51, step 221, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #51, step 222, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #51, step 223, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #51, step 224, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #51, step 225, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #51, step 226, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #51, step 227, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #51, step 228, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #51, step 229, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #51, step 230, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #51, step 231, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #51, step 232, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #51, step 233, discriminator loss=0.689 , generator loss=0.713\n",
      "Disciminator Accuracy on real images: 51%, on fake images: 80%\n",
      "Training progress in epoch #52, step 0, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #52, step 1, discriminator loss=0.689 , generator loss=0.727\n",
      "Training progress in epoch #52, step 2, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #52, step 3, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #52, step 4, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #52, step 5, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #52, step 6, discriminator loss=0.696 , generator loss=0.683\n",
      "Training progress in epoch #52, step 7, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #52, step 8, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #52, step 9, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #52, step 10, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #52, step 11, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #52, step 12, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #52, step 13, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #52, step 14, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #52, step 15, discriminator loss=0.679 , generator loss=0.719\n",
      "Training progress in epoch #52, step 16, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #52, step 17, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #52, step 18, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #52, step 19, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #52, step 20, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #52, step 21, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #52, step 22, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #52, step 23, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #52, step 24, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #52, step 25, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #52, step 26, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #52, step 27, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #52, step 28, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #52, step 29, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #52, step 30, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #52, step 31, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #52, step 32, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #52, step 33, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #52, step 34, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #52, step 35, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #52, step 36, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #52, step 37, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #52, step 38, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #52, step 39, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #52, step 40, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #52, step 41, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #52, step 42, discriminator loss=0.683 , generator loss=0.694\n",
      "Training progress in epoch #52, step 43, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #52, step 44, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #52, step 45, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #52, step 46, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #52, step 47, discriminator loss=0.684 , generator loss=0.707\n",
      "Training progress in epoch #52, step 48, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #52, step 49, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #52, step 50, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #52, step 51, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #52, step 52, discriminator loss=0.684 , generator loss=0.686\n",
      "Training progress in epoch #52, step 53, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #52, step 54, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #52, step 55, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #52, step 56, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #52, step 57, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #52, step 58, discriminator loss=0.697 , generator loss=0.690\n",
      "Training progress in epoch #52, step 59, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #52, step 60, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #52, step 61, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #52, step 62, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #52, step 63, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #52, step 64, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #52, step 65, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #52, step 66, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #52, step 67, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #52, step 68, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #52, step 69, discriminator loss=0.696 , generator loss=0.694\n",
      "Training progress in epoch #52, step 70, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #52, step 71, discriminator loss=0.693 , generator loss=0.733\n",
      "Training progress in epoch #52, step 72, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #52, step 73, discriminator loss=0.688 , generator loss=0.722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #52, step 74, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #52, step 75, discriminator loss=0.694 , generator loss=0.669\n",
      "Training progress in epoch #52, step 76, discriminator loss=0.686 , generator loss=0.684\n",
      "Training progress in epoch #52, step 77, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #52, step 78, discriminator loss=0.695 , generator loss=0.728\n",
      "Training progress in epoch #52, step 79, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #52, step 80, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #52, step 81, discriminator loss=0.698 , generator loss=0.672\n",
      "Training progress in epoch #52, step 82, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #52, step 83, discriminator loss=0.700 , generator loss=0.724\n",
      "Training progress in epoch #52, step 84, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #52, step 85, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #52, step 86, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #52, step 87, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #52, step 88, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #52, step 89, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #52, step 90, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #52, step 91, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #52, step 92, discriminator loss=0.696 , generator loss=0.674\n",
      "Training progress in epoch #52, step 93, discriminator loss=0.697 , generator loss=0.696\n",
      "Training progress in epoch #52, step 94, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #52, step 95, discriminator loss=0.691 , generator loss=0.743\n",
      "Training progress in epoch #52, step 96, discriminator loss=0.695 , generator loss=0.734\n",
      "Training progress in epoch #52, step 97, discriminator loss=0.698 , generator loss=0.712\n",
      "Training progress in epoch #52, step 98, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #52, step 99, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #52, step 100, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #52, step 101, discriminator loss=0.698 , generator loss=0.705\n",
      "Training progress in epoch #52, step 102, discriminator loss=0.698 , generator loss=0.682\n",
      "Training progress in epoch #52, step 103, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #52, step 104, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #52, step 105, discriminator loss=0.700 , generator loss=0.694\n",
      "Training progress in epoch #52, step 106, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #52, step 107, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #52, step 108, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #52, step 109, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #52, step 110, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #52, step 111, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #52, step 112, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #52, step 113, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #52, step 114, discriminator loss=0.693 , generator loss=0.668\n",
      "Training progress in epoch #52, step 115, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #52, step 116, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #52, step 117, discriminator loss=0.689 , generator loss=0.732\n",
      "Training progress in epoch #52, step 118, discriminator loss=0.692 , generator loss=0.731\n",
      "Training progress in epoch #52, step 119, discriminator loss=0.684 , generator loss=0.733\n",
      "Training progress in epoch #52, step 120, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #52, step 121, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #52, step 122, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #52, step 123, discriminator loss=0.686 , generator loss=0.687\n",
      "Training progress in epoch #52, step 124, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #52, step 125, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #52, step 126, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #52, step 127, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #52, step 128, discriminator loss=0.695 , generator loss=0.732\n",
      "Training progress in epoch #52, step 129, discriminator loss=0.691 , generator loss=0.740\n",
      "Training progress in epoch #52, step 130, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #52, step 131, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #52, step 132, discriminator loss=0.683 , generator loss=0.697\n",
      "Training progress in epoch #52, step 133, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #52, step 134, discriminator loss=0.681 , generator loss=0.702\n",
      "Training progress in epoch #52, step 135, discriminator loss=0.697 , generator loss=0.700\n",
      "Training progress in epoch #52, step 136, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #52, step 137, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #52, step 138, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #52, step 139, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #52, step 140, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #52, step 141, discriminator loss=0.685 , generator loss=0.686\n",
      "Training progress in epoch #52, step 142, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #52, step 143, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #52, step 144, discriminator loss=0.697 , generator loss=0.713\n",
      "Training progress in epoch #52, step 145, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #52, step 146, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #52, step 147, discriminator loss=0.697 , generator loss=0.687\n",
      "Training progress in epoch #52, step 148, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #52, step 149, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #52, step 150, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #52, step 151, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #52, step 152, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #52, step 153, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #52, step 154, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #52, step 155, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #52, step 156, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #52, step 157, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #52, step 158, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #52, step 159, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #52, step 160, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #52, step 161, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #52, step 162, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #52, step 163, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #52, step 164, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #52, step 165, discriminator loss=0.691 , generator loss=0.682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #52, step 166, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #52, step 167, discriminator loss=0.695 , generator loss=0.723\n",
      "Training progress in epoch #52, step 168, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #52, step 169, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #52, step 170, discriminator loss=0.689 , generator loss=0.732\n",
      "Training progress in epoch #52, step 171, discriminator loss=0.684 , generator loss=0.733\n",
      "Training progress in epoch #52, step 172, discriminator loss=0.685 , generator loss=0.710\n",
      "Training progress in epoch #52, step 173, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #52, step 174, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #52, step 175, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #52, step 176, discriminator loss=0.697 , generator loss=0.696\n",
      "Training progress in epoch #52, step 177, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #52, step 178, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #52, step 179, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #52, step 180, discriminator loss=0.694 , generator loss=0.742\n",
      "Training progress in epoch #52, step 181, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #52, step 182, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #52, step 183, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #52, step 184, discriminator loss=0.694 , generator loss=0.675\n",
      "Training progress in epoch #52, step 185, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #52, step 186, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #52, step 187, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #52, step 188, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #52, step 189, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #52, step 190, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #52, step 191, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #52, step 192, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #52, step 193, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #52, step 194, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #52, step 195, discriminator loss=0.687 , generator loss=0.670\n",
      "Training progress in epoch #52, step 196, discriminator loss=0.696 , generator loss=0.689\n",
      "Training progress in epoch #52, step 197, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #52, step 198, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #52, step 199, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #52, step 200, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #52, step 201, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #52, step 202, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #52, step 203, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #52, step 204, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #52, step 205, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #52, step 206, discriminator loss=0.683 , generator loss=0.697\n",
      "Training progress in epoch #52, step 207, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #52, step 208, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #52, step 209, discriminator loss=0.697 , generator loss=0.715\n",
      "Training progress in epoch #52, step 210, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #52, step 211, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #52, step 212, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #52, step 213, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #52, step 214, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #52, step 215, discriminator loss=0.697 , generator loss=0.714\n",
      "Training progress in epoch #52, step 216, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #52, step 217, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #52, step 218, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #52, step 219, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #52, step 220, discriminator loss=0.692 , generator loss=0.676\n",
      "Training progress in epoch #52, step 221, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #52, step 222, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #52, step 223, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #52, step 224, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #52, step 225, discriminator loss=0.697 , generator loss=0.693\n",
      "Training progress in epoch #52, step 226, discriminator loss=0.684 , generator loss=0.691\n",
      "Training progress in epoch #52, step 227, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #52, step 228, discriminator loss=0.683 , generator loss=0.710\n",
      "Training progress in epoch #52, step 229, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #52, step 230, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #52, step 231, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #52, step 232, discriminator loss=0.696 , generator loss=0.736\n",
      "Training progress in epoch #52, step 233, discriminator loss=0.686 , generator loss=0.729\n",
      "Disciminator Accuracy on real images: 15%, on fake images: 97%\n",
      "Training progress in epoch #53, step 0, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #53, step 1, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #53, step 2, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #53, step 3, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #53, step 4, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #53, step 5, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #53, step 6, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #53, step 7, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #53, step 8, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #53, step 9, discriminator loss=0.698 , generator loss=0.737\n",
      "Training progress in epoch #53, step 10, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #53, step 11, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #53, step 12, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #53, step 13, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #53, step 14, discriminator loss=0.687 , generator loss=0.675\n",
      "Training progress in epoch #53, step 15, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #53, step 16, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #53, step 17, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #53, step 18, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #53, step 19, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #53, step 20, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #53, step 21, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #53, step 22, discriminator loss=0.691 , generator loss=0.713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #53, step 23, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #53, step 24, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #53, step 25, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #53, step 26, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #53, step 27, discriminator loss=0.694 , generator loss=0.677\n",
      "Training progress in epoch #53, step 28, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #53, step 29, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #53, step 30, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #53, step 31, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #53, step 32, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #53, step 33, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #53, step 34, discriminator loss=0.691 , generator loss=0.723\n",
      "Training progress in epoch #53, step 35, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #53, step 36, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #53, step 37, discriminator loss=0.689 , generator loss=0.668\n",
      "Training progress in epoch #53, step 38, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #53, step 39, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #53, step 40, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #53, step 41, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #53, step 42, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #53, step 43, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #53, step 44, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #53, step 45, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #53, step 46, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #53, step 47, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #53, step 48, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #53, step 49, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #53, step 50, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #53, step 51, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #53, step 52, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #53, step 53, discriminator loss=0.684 , generator loss=0.680\n",
      "Training progress in epoch #53, step 54, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #53, step 55, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #53, step 56, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #53, step 57, discriminator loss=0.692 , generator loss=0.728\n",
      "Training progress in epoch #53, step 58, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #53, step 59, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #53, step 60, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #53, step 61, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #53, step 62, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #53, step 63, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #53, step 64, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #53, step 65, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #53, step 66, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #53, step 67, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #53, step 68, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #53, step 69, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #53, step 70, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #53, step 71, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #53, step 72, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #53, step 73, discriminator loss=0.700 , generator loss=0.698\n",
      "Training progress in epoch #53, step 74, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #53, step 75, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #53, step 76, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #53, step 77, discriminator loss=0.699 , generator loss=0.699\n",
      "Training progress in epoch #53, step 78, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #53, step 79, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #53, step 80, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #53, step 81, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #53, step 82, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #53, step 83, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #53, step 84, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #53, step 85, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #53, step 86, discriminator loss=0.694 , generator loss=0.724\n",
      "Training progress in epoch #53, step 87, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #53, step 88, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #53, step 89, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #53, step 90, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #53, step 91, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #53, step 92, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #53, step 93, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #53, step 94, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #53, step 95, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #53, step 96, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #53, step 97, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #53, step 98, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #53, step 99, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #53, step 100, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #53, step 101, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #53, step 102, discriminator loss=0.687 , generator loss=0.679\n",
      "Training progress in epoch #53, step 103, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #53, step 104, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #53, step 105, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #53, step 106, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #53, step 107, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #53, step 108, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #53, step 109, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #53, step 110, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #53, step 111, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #53, step 112, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #53, step 113, discriminator loss=0.698 , generator loss=0.706\n",
      "Training progress in epoch #53, step 114, discriminator loss=0.696 , generator loss=0.710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #53, step 115, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #53, step 116, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #53, step 117, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #53, step 118, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #53, step 119, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #53, step 120, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #53, step 121, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #53, step 122, discriminator loss=0.684 , generator loss=0.691\n",
      "Training progress in epoch #53, step 123, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #53, step 124, discriminator loss=0.686 , generator loss=0.753\n",
      "Training progress in epoch #53, step 125, discriminator loss=0.685 , generator loss=0.744\n",
      "Training progress in epoch #53, step 126, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #53, step 127, discriminator loss=0.684 , generator loss=0.673\n",
      "Training progress in epoch #53, step 128, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #53, step 129, discriminator loss=0.686 , generator loss=0.685\n",
      "Training progress in epoch #53, step 130, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #53, step 131, discriminator loss=0.689 , generator loss=0.727\n",
      "Training progress in epoch #53, step 132, discriminator loss=0.697 , generator loss=0.724\n",
      "Training progress in epoch #53, step 133, discriminator loss=0.692 , generator loss=0.729\n",
      "Training progress in epoch #53, step 134, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #53, step 135, discriminator loss=0.695 , generator loss=0.689\n",
      "Training progress in epoch #53, step 136, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #53, step 137, discriminator loss=0.684 , generator loss=0.693\n",
      "Training progress in epoch #53, step 138, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #53, step 139, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #53, step 140, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #53, step 141, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #53, step 142, discriminator loss=0.684 , generator loss=0.713\n",
      "Training progress in epoch #53, step 143, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #53, step 144, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #53, step 145, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #53, step 146, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #53, step 147, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #53, step 148, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #53, step 149, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #53, step 150, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #53, step 151, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #53, step 152, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #53, step 153, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #53, step 154, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #53, step 155, discriminator loss=0.702 , generator loss=0.719\n",
      "Training progress in epoch #53, step 156, discriminator loss=0.697 , generator loss=0.709\n",
      "Training progress in epoch #53, step 157, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #53, step 158, discriminator loss=0.698 , generator loss=0.705\n",
      "Training progress in epoch #53, step 159, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #53, step 160, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #53, step 161, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #53, step 162, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #53, step 163, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #53, step 164, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #53, step 165, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #53, step 166, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #53, step 167, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #53, step 168, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #53, step 169, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #53, step 170, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #53, step 171, discriminator loss=0.693 , generator loss=0.729\n",
      "Training progress in epoch #53, step 172, discriminator loss=0.691 , generator loss=0.675\n",
      "Training progress in epoch #53, step 173, discriminator loss=0.698 , generator loss=0.671\n",
      "Training progress in epoch #53, step 174, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #53, step 175, discriminator loss=0.696 , generator loss=0.706\n",
      "Training progress in epoch #53, step 176, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #53, step 177, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #53, step 178, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #53, step 179, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #53, step 180, discriminator loss=0.684 , generator loss=0.709\n",
      "Training progress in epoch #53, step 181, discriminator loss=0.686 , generator loss=0.683\n",
      "Training progress in epoch #53, step 182, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #53, step 183, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #53, step 184, discriminator loss=0.688 , generator loss=0.731\n",
      "Training progress in epoch #53, step 185, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #53, step 186, discriminator loss=0.696 , generator loss=0.720\n",
      "Training progress in epoch #53, step 187, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #53, step 188, discriminator loss=0.690 , generator loss=0.675\n",
      "Training progress in epoch #53, step 189, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #53, step 190, discriminator loss=0.695 , generator loss=0.726\n",
      "Training progress in epoch #53, step 191, discriminator loss=0.691 , generator loss=0.732\n",
      "Training progress in epoch #53, step 192, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #53, step 193, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #53, step 194, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #53, step 195, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #53, step 196, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #53, step 197, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #53, step 198, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #53, step 199, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #53, step 200, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #53, step 201, discriminator loss=0.694 , generator loss=0.732\n",
      "Training progress in epoch #53, step 202, discriminator loss=0.690 , generator loss=0.749\n",
      "Training progress in epoch #53, step 203, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #53, step 204, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #53, step 205, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #53, step 206, discriminator loss=0.694 , generator loss=0.703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #53, step 207, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #53, step 208, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #53, step 209, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #53, step 210, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #53, step 211, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #53, step 212, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #53, step 213, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #53, step 214, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #53, step 215, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #53, step 216, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #53, step 217, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #53, step 218, discriminator loss=0.690 , generator loss=0.679\n",
      "Training progress in epoch #53, step 219, discriminator loss=0.690 , generator loss=0.676\n",
      "Training progress in epoch #53, step 220, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #53, step 221, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #53, step 222, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #53, step 223, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #53, step 224, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #53, step 225, discriminator loss=0.692 , generator loss=0.729\n",
      "Training progress in epoch #53, step 226, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #53, step 227, discriminator loss=0.686 , generator loss=0.679\n",
      "Training progress in epoch #53, step 228, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #53, step 229, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #53, step 230, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #53, step 231, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #53, step 232, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #53, step 233, discriminator loss=0.687 , generator loss=0.687\n",
      "Disciminator Accuracy on real images: 66%, on fake images: 36%\n",
      "Training progress in epoch #54, step 0, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #54, step 1, discriminator loss=0.696 , generator loss=0.731\n",
      "Training progress in epoch #54, step 2, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #54, step 3, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #54, step 4, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #54, step 5, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #54, step 6, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #54, step 7, discriminator loss=0.683 , generator loss=0.708\n",
      "Training progress in epoch #54, step 8, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #54, step 9, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #54, step 10, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #54, step 11, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #54, step 12, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #54, step 13, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #54, step 14, discriminator loss=0.697 , generator loss=0.703\n",
      "Training progress in epoch #54, step 15, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #54, step 16, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #54, step 17, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #54, step 18, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #54, step 19, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #54, step 20, discriminator loss=0.699 , generator loss=0.689\n",
      "Training progress in epoch #54, step 21, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #54, step 22, discriminator loss=0.698 , generator loss=0.715\n",
      "Training progress in epoch #54, step 23, discriminator loss=0.697 , generator loss=0.715\n",
      "Training progress in epoch #54, step 24, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #54, step 25, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #54, step 26, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #54, step 27, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #54, step 28, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #54, step 29, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #54, step 30, discriminator loss=0.688 , generator loss=0.730\n",
      "Training progress in epoch #54, step 31, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #54, step 32, discriminator loss=0.688 , generator loss=0.735\n",
      "Training progress in epoch #54, step 33, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #54, step 34, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #54, step 35, discriminator loss=0.685 , generator loss=0.683\n",
      "Training progress in epoch #54, step 36, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #54, step 37, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #54, step 38, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #54, step 39, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #54, step 40, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #54, step 41, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #54, step 42, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #54, step 43, discriminator loss=0.686 , generator loss=0.673\n",
      "Training progress in epoch #54, step 44, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #54, step 45, discriminator loss=0.682 , generator loss=0.731\n",
      "Training progress in epoch #54, step 46, discriminator loss=0.684 , generator loss=0.739\n",
      "Training progress in epoch #54, step 47, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #54, step 48, discriminator loss=0.688 , generator loss=0.735\n",
      "Training progress in epoch #54, step 49, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #54, step 50, discriminator loss=0.686 , generator loss=0.682\n",
      "Training progress in epoch #54, step 51, discriminator loss=0.693 , generator loss=0.673\n",
      "Training progress in epoch #54, step 52, discriminator loss=0.692 , generator loss=0.671\n",
      "Training progress in epoch #54, step 53, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #54, step 54, discriminator loss=0.701 , generator loss=0.699\n",
      "Training progress in epoch #54, step 55, discriminator loss=0.683 , generator loss=0.700\n",
      "Training progress in epoch #54, step 56, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #54, step 57, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #54, step 58, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #54, step 59, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #54, step 60, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #54, step 61, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #54, step 62, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #54, step 63, discriminator loss=0.685 , generator loss=0.673\n",
      "Training progress in epoch #54, step 64, discriminator loss=0.689 , generator loss=0.678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #54, step 65, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #54, step 66, discriminator loss=0.681 , generator loss=0.723\n",
      "Training progress in epoch #54, step 67, discriminator loss=0.694 , generator loss=0.736\n",
      "Training progress in epoch #54, step 68, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #54, step 69, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #54, step 70, discriminator loss=0.698 , generator loss=0.692\n",
      "Training progress in epoch #54, step 71, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #54, step 72, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #54, step 73, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #54, step 74, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #54, step 75, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #54, step 76, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #54, step 77, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #54, step 78, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #54, step 79, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #54, step 80, discriminator loss=0.681 , generator loss=0.719\n",
      "Training progress in epoch #54, step 81, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #54, step 82, discriminator loss=0.683 , generator loss=0.730\n",
      "Training progress in epoch #54, step 83, discriminator loss=0.693 , generator loss=0.729\n",
      "Training progress in epoch #54, step 84, discriminator loss=0.684 , generator loss=0.712\n",
      "Training progress in epoch #54, step 85, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #54, step 86, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #54, step 87, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #54, step 88, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #54, step 89, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #54, step 90, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #54, step 91, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #54, step 92, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #54, step 93, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #54, step 94, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #54, step 95, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #54, step 96, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #54, step 97, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #54, step 98, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #54, step 99, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #54, step 100, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #54, step 101, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #54, step 102, discriminator loss=0.685 , generator loss=0.686\n",
      "Training progress in epoch #54, step 103, discriminator loss=0.695 , generator loss=0.727\n",
      "Training progress in epoch #54, step 104, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #54, step 105, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #54, step 106, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #54, step 107, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #54, step 108, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #54, step 109, discriminator loss=0.691 , generator loss=0.671\n",
      "Training progress in epoch #54, step 110, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #54, step 111, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #54, step 112, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #54, step 113, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #54, step 114, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #54, step 115, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #54, step 116, discriminator loss=0.695 , generator loss=0.682\n",
      "Training progress in epoch #54, step 117, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #54, step 118, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #54, step 119, discriminator loss=0.684 , generator loss=0.705\n",
      "Training progress in epoch #54, step 120, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #54, step 121, discriminator loss=0.702 , generator loss=0.727\n",
      "Training progress in epoch #54, step 122, discriminator loss=0.696 , generator loss=0.738\n",
      "Training progress in epoch #54, step 123, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #54, step 124, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #54, step 125, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #54, step 126, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #54, step 127, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #54, step 128, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #54, step 129, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #54, step 130, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #54, step 131, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #54, step 132, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #54, step 133, discriminator loss=0.692 , generator loss=0.745\n",
      "Training progress in epoch #54, step 134, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #54, step 135, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #54, step 136, discriminator loss=0.700 , generator loss=0.679\n",
      "Training progress in epoch #54, step 137, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #54, step 138, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #54, step 139, discriminator loss=0.684 , generator loss=0.716\n",
      "Training progress in epoch #54, step 140, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #54, step 141, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #54, step 142, discriminator loss=0.699 , generator loss=0.705\n",
      "Training progress in epoch #54, step 143, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #54, step 144, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #54, step 145, discriminator loss=0.686 , generator loss=0.717\n",
      "Training progress in epoch #54, step 146, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #54, step 147, discriminator loss=0.693 , generator loss=0.733\n",
      "Training progress in epoch #54, step 148, discriminator loss=0.686 , generator loss=0.738\n",
      "Training progress in epoch #54, step 149, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #54, step 150, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #54, step 151, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #54, step 152, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #54, step 153, discriminator loss=0.691 , generator loss=0.736\n",
      "Training progress in epoch #54, step 154, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #54, step 155, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #54, step 156, discriminator loss=0.692 , generator loss=0.671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #54, step 157, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #54, step 158, discriminator loss=0.696 , generator loss=0.739\n",
      "Training progress in epoch #54, step 159, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #54, step 160, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #54, step 161, discriminator loss=0.697 , generator loss=0.714\n",
      "Training progress in epoch #54, step 162, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #54, step 163, discriminator loss=0.691 , generator loss=0.733\n",
      "Training progress in epoch #54, step 164, discriminator loss=0.685 , generator loss=0.764\n",
      "Training progress in epoch #54, step 165, discriminator loss=0.684 , generator loss=0.727\n",
      "Training progress in epoch #54, step 166, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #54, step 167, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #54, step 168, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #54, step 169, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #54, step 170, discriminator loss=0.679 , generator loss=0.688\n",
      "Training progress in epoch #54, step 171, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #54, step 172, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #54, step 173, discriminator loss=0.692 , generator loss=0.736\n",
      "Training progress in epoch #54, step 174, discriminator loss=0.689 , generator loss=0.734\n",
      "Training progress in epoch #54, step 175, discriminator loss=0.683 , generator loss=0.725\n",
      "Training progress in epoch #54, step 176, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #54, step 177, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #54, step 178, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #54, step 179, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #54, step 180, discriminator loss=0.688 , generator loss=0.675\n",
      "Training progress in epoch #54, step 181, discriminator loss=0.687 , generator loss=0.675\n",
      "Training progress in epoch #54, step 182, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #54, step 183, discriminator loss=0.691 , generator loss=0.738\n",
      "Training progress in epoch #54, step 184, discriminator loss=0.691 , generator loss=0.737\n",
      "Training progress in epoch #54, step 185, discriminator loss=0.683 , generator loss=0.711\n",
      "Training progress in epoch #54, step 186, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #54, step 187, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #54, step 188, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #54, step 189, discriminator loss=0.696 , generator loss=0.690\n",
      "Training progress in epoch #54, step 190, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #54, step 191, discriminator loss=0.696 , generator loss=0.683\n",
      "Training progress in epoch #54, step 192, discriminator loss=0.699 , generator loss=0.692\n",
      "Training progress in epoch #54, step 193, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #54, step 194, discriminator loss=0.693 , generator loss=0.726\n",
      "Training progress in epoch #54, step 195, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #54, step 196, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #54, step 197, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #54, step 198, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #54, step 199, discriminator loss=0.685 , generator loss=0.715\n",
      "Training progress in epoch #54, step 200, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #54, step 201, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #54, step 202, discriminator loss=0.684 , generator loss=0.698\n",
      "Training progress in epoch #54, step 203, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #54, step 204, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #54, step 205, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #54, step 206, discriminator loss=0.699 , generator loss=0.685\n",
      "Training progress in epoch #54, step 207, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #54, step 208, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #54, step 209, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #54, step 210, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #54, step 211, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #54, step 212, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #54, step 213, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #54, step 214, discriminator loss=0.686 , generator loss=0.675\n",
      "Training progress in epoch #54, step 215, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #54, step 216, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #54, step 217, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #54, step 218, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #54, step 219, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #54, step 220, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #54, step 221, discriminator loss=0.691 , generator loss=0.736\n",
      "Training progress in epoch #54, step 222, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #54, step 223, discriminator loss=0.692 , generator loss=0.675\n",
      "Training progress in epoch #54, step 224, discriminator loss=0.694 , generator loss=0.676\n",
      "Training progress in epoch #54, step 225, discriminator loss=0.694 , generator loss=0.679\n",
      "Training progress in epoch #54, step 226, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #54, step 227, discriminator loss=0.694 , generator loss=0.721\n",
      "Training progress in epoch #54, step 228, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #54, step 229, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #54, step 230, discriminator loss=0.697 , generator loss=0.695\n",
      "Training progress in epoch #54, step 231, discriminator loss=0.692 , generator loss=0.673\n",
      "Training progress in epoch #54, step 232, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #54, step 233, discriminator loss=0.694 , generator loss=0.708\n",
      "Disciminator Accuracy on real images: 49%, on fake images: 77%\n",
      "Training progress in epoch #55, step 0, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #55, step 1, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #55, step 2, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #55, step 3, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #55, step 4, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #55, step 5, discriminator loss=0.685 , generator loss=0.715\n",
      "Training progress in epoch #55, step 6, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #55, step 7, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #55, step 8, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #55, step 9, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #55, step 10, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #55, step 11, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #55, step 12, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #55, step 13, discriminator loss=0.689 , generator loss=0.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #55, step 14, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #55, step 15, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #55, step 16, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #55, step 17, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #55, step 18, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #55, step 19, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #55, step 20, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #55, step 21, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #55, step 22, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #55, step 23, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #55, step 24, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #55, step 25, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #55, step 26, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #55, step 27, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #55, step 28, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #55, step 29, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #55, step 30, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #55, step 31, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #55, step 32, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #55, step 33, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #55, step 34, discriminator loss=0.686 , generator loss=0.727\n",
      "Training progress in epoch #55, step 35, discriminator loss=0.681 , generator loss=0.723\n",
      "Training progress in epoch #55, step 36, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #55, step 37, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #55, step 38, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #55, step 39, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #55, step 40, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #55, step 41, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #55, step 42, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #55, step 43, discriminator loss=0.685 , generator loss=0.720\n",
      "Training progress in epoch #55, step 44, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #55, step 45, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #55, step 46, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #55, step 47, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #55, step 48, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #55, step 49, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #55, step 50, discriminator loss=0.697 , generator loss=0.711\n",
      "Training progress in epoch #55, step 51, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #55, step 52, discriminator loss=0.681 , generator loss=0.707\n",
      "Training progress in epoch #55, step 53, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #55, step 54, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #55, step 55, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #55, step 56, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #55, step 57, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #55, step 58, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #55, step 59, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #55, step 60, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #55, step 61, discriminator loss=0.685 , generator loss=0.675\n",
      "Training progress in epoch #55, step 62, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #55, step 63, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #55, step 64, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #55, step 65, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #55, step 66, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #55, step 67, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #55, step 68, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #55, step 69, discriminator loss=0.697 , generator loss=0.696\n",
      "Training progress in epoch #55, step 70, discriminator loss=0.688 , generator loss=0.672\n",
      "Training progress in epoch #55, step 71, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #55, step 72, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #55, step 73, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #55, step 74, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #55, step 75, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #55, step 76, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #55, step 77, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #55, step 78, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #55, step 79, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #55, step 80, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #55, step 81, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #55, step 82, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #55, step 83, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #55, step 84, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #55, step 85, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #55, step 86, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #55, step 87, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #55, step 88, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #55, step 89, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #55, step 90, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #55, step 91, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #55, step 92, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #55, step 93, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #55, step 94, discriminator loss=0.684 , generator loss=0.725\n",
      "Training progress in epoch #55, step 95, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #55, step 96, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #55, step 97, discriminator loss=0.691 , generator loss=0.670\n",
      "Training progress in epoch #55, step 98, discriminator loss=0.691 , generator loss=0.664\n",
      "Training progress in epoch #55, step 99, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #55, step 100, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #55, step 101, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #55, step 102, discriminator loss=0.696 , generator loss=0.716\n",
      "Training progress in epoch #55, step 103, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #55, step 104, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #55, step 105, discriminator loss=0.695 , generator loss=0.726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #55, step 106, discriminator loss=0.696 , generator loss=0.749\n",
      "Training progress in epoch #55, step 107, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #55, step 108, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #55, step 109, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #55, step 110, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #55, step 111, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #55, step 112, discriminator loss=0.684 , generator loss=0.725\n",
      "Training progress in epoch #55, step 113, discriminator loss=0.696 , generator loss=0.695\n",
      "Training progress in epoch #55, step 114, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #55, step 115, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #55, step 116, discriminator loss=0.683 , generator loss=0.700\n",
      "Training progress in epoch #55, step 117, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #55, step 118, discriminator loss=0.693 , generator loss=0.727\n",
      "Training progress in epoch #55, step 119, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #55, step 120, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #55, step 121, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #55, step 122, discriminator loss=0.697 , generator loss=0.689\n",
      "Training progress in epoch #55, step 123, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #55, step 124, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #55, step 125, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #55, step 126, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #55, step 127, discriminator loss=0.688 , generator loss=0.731\n",
      "Training progress in epoch #55, step 128, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #55, step 129, discriminator loss=0.701 , generator loss=0.681\n",
      "Training progress in epoch #55, step 130, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #55, step 131, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #55, step 132, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #55, step 133, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #55, step 134, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #55, step 135, discriminator loss=0.697 , generator loss=0.725\n",
      "Training progress in epoch #55, step 136, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #55, step 137, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #55, step 138, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #55, step 139, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #55, step 140, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #55, step 141, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #55, step 142, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #55, step 143, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #55, step 144, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #55, step 145, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #55, step 146, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #55, step 147, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #55, step 148, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #55, step 149, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #55, step 150, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #55, step 151, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #55, step 152, discriminator loss=0.684 , generator loss=0.698\n",
      "Training progress in epoch #55, step 153, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #55, step 154, discriminator loss=0.683 , generator loss=0.710\n",
      "Training progress in epoch #55, step 155, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #55, step 156, discriminator loss=0.682 , generator loss=0.685\n",
      "Training progress in epoch #55, step 157, discriminator loss=0.697 , generator loss=0.691\n",
      "Training progress in epoch #55, step 158, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #55, step 159, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #55, step 160, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #55, step 161, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #55, step 162, discriminator loss=0.696 , generator loss=0.706\n",
      "Training progress in epoch #55, step 163, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #55, step 164, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #55, step 165, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #55, step 166, discriminator loss=0.684 , generator loss=0.709\n",
      "Training progress in epoch #55, step 167, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #55, step 168, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #55, step 169, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #55, step 170, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #55, step 171, discriminator loss=0.695 , generator loss=0.719\n",
      "Training progress in epoch #55, step 172, discriminator loss=0.689 , generator loss=0.733\n",
      "Training progress in epoch #55, step 173, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #55, step 174, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #55, step 175, discriminator loss=0.697 , generator loss=0.673\n",
      "Training progress in epoch #55, step 176, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #55, step 177, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #55, step 178, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #55, step 179, discriminator loss=0.695 , generator loss=0.729\n",
      "Training progress in epoch #55, step 180, discriminator loss=0.692 , generator loss=0.761\n",
      "Training progress in epoch #55, step 181, discriminator loss=0.695 , generator loss=0.733\n",
      "Training progress in epoch #55, step 182, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #55, step 183, discriminator loss=0.691 , generator loss=0.673\n",
      "Training progress in epoch #55, step 184, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #55, step 185, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #55, step 186, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #55, step 187, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #55, step 188, discriminator loss=0.692 , generator loss=0.673\n",
      "Training progress in epoch #55, step 189, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #55, step 190, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #55, step 191, discriminator loss=0.692 , generator loss=0.750\n",
      "Training progress in epoch #55, step 192, discriminator loss=0.694 , generator loss=0.732\n",
      "Training progress in epoch #55, step 193, discriminator loss=0.701 , generator loss=0.719\n",
      "Training progress in epoch #55, step 194, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #55, step 195, discriminator loss=0.686 , generator loss=0.682\n",
      "Training progress in epoch #55, step 196, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #55, step 197, discriminator loss=0.691 , generator loss=0.687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #55, step 198, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #55, step 199, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #55, step 200, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #55, step 201, discriminator loss=0.688 , generator loss=0.680\n",
      "Training progress in epoch #55, step 202, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #55, step 203, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #55, step 204, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #55, step 205, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #55, step 206, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #55, step 207, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #55, step 208, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #55, step 209, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #55, step 210, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #55, step 211, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #55, step 212, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #55, step 213, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #55, step 214, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #55, step 215, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #55, step 216, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #55, step 217, discriminator loss=0.689 , generator loss=0.738\n",
      "Training progress in epoch #55, step 218, discriminator loss=0.698 , generator loss=0.733\n",
      "Training progress in epoch #55, step 219, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #55, step 220, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #55, step 221, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #55, step 222, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #55, step 223, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #55, step 224, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #55, step 225, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #55, step 226, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #55, step 227, discriminator loss=0.683 , generator loss=0.731\n",
      "Training progress in epoch #55, step 228, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #55, step 229, discriminator loss=0.685 , generator loss=0.688\n",
      "Training progress in epoch #55, step 230, discriminator loss=0.684 , generator loss=0.692\n",
      "Training progress in epoch #55, step 231, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #55, step 232, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #55, step 233, discriminator loss=0.689 , generator loss=0.701\n",
      "Disciminator Accuracy on real images: 64%, on fake images: 54%\n",
      "Training progress in epoch #56, step 0, discriminator loss=0.701 , generator loss=0.690\n",
      "Training progress in epoch #56, step 1, discriminator loss=0.685 , generator loss=0.692\n",
      "Training progress in epoch #56, step 2, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #56, step 3, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #56, step 4, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #56, step 5, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #56, step 6, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #56, step 7, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #56, step 8, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #56, step 9, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #56, step 10, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #56, step 11, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #56, step 12, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #56, step 13, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #56, step 14, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #56, step 15, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #56, step 16, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #56, step 17, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #56, step 18, discriminator loss=0.685 , generator loss=0.715\n",
      "Training progress in epoch #56, step 19, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #56, step 20, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #56, step 21, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #56, step 22, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #56, step 23, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #56, step 24, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #56, step 25, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #56, step 26, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #56, step 27, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #56, step 28, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #56, step 29, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #56, step 30, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #56, step 31, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #56, step 32, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #56, step 33, discriminator loss=0.695 , generator loss=0.740\n",
      "Training progress in epoch #56, step 34, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #56, step 35, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #56, step 36, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #56, step 37, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #56, step 38, discriminator loss=0.695 , generator loss=0.680\n",
      "Training progress in epoch #56, step 39, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #56, step 40, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #56, step 41, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #56, step 42, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #56, step 43, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #56, step 44, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #56, step 45, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #56, step 46, discriminator loss=0.696 , generator loss=0.685\n",
      "Training progress in epoch #56, step 47, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #56, step 48, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #56, step 49, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #56, step 50, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #56, step 51, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #56, step 52, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #56, step 53, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #56, step 54, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #56, step 55, discriminator loss=0.690 , generator loss=0.702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #56, step 56, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #56, step 57, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #56, step 58, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #56, step 59, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #56, step 60, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #56, step 61, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #56, step 62, discriminator loss=0.687 , generator loss=0.677\n",
      "Training progress in epoch #56, step 63, discriminator loss=0.684 , generator loss=0.680\n",
      "Training progress in epoch #56, step 64, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #56, step 65, discriminator loss=0.693 , generator loss=0.740\n",
      "Training progress in epoch #56, step 66, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #56, step 67, discriminator loss=0.698 , generator loss=0.703\n",
      "Training progress in epoch #56, step 68, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #56, step 69, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #56, step 70, discriminator loss=0.686 , generator loss=0.683\n",
      "Training progress in epoch #56, step 71, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #56, step 72, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #56, step 73, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #56, step 74, discriminator loss=0.695 , generator loss=0.739\n",
      "Training progress in epoch #56, step 75, discriminator loss=0.696 , generator loss=0.736\n",
      "Training progress in epoch #56, step 76, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #56, step 77, discriminator loss=0.684 , generator loss=0.689\n",
      "Training progress in epoch #56, step 78, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #56, step 79, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #56, step 80, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #56, step 81, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #56, step 82, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #56, step 83, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #56, step 84, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #56, step 85, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #56, step 86, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #56, step 87, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #56, step 88, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #56, step 89, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #56, step 90, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #56, step 91, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #56, step 92, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #56, step 93, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #56, step 94, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #56, step 95, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #56, step 96, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #56, step 97, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #56, step 98, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #56, step 99, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #56, step 100, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #56, step 101, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #56, step 102, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #56, step 103, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #56, step 104, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #56, step 105, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #56, step 106, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #56, step 107, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #56, step 108, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #56, step 109, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #56, step 110, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #56, step 111, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #56, step 112, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #56, step 113, discriminator loss=0.695 , generator loss=0.730\n",
      "Training progress in epoch #56, step 114, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #56, step 115, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #56, step 116, discriminator loss=0.690 , generator loss=0.673\n",
      "Training progress in epoch #56, step 117, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #56, step 118, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #56, step 119, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #56, step 120, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #56, step 121, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #56, step 122, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #56, step 123, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #56, step 124, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #56, step 125, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #56, step 126, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #56, step 127, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #56, step 128, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #56, step 129, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #56, step 130, discriminator loss=0.694 , generator loss=0.730\n",
      "Training progress in epoch #56, step 131, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #56, step 132, discriminator loss=0.685 , generator loss=0.683\n",
      "Training progress in epoch #56, step 133, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #56, step 134, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #56, step 135, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #56, step 136, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #56, step 137, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #56, step 138, discriminator loss=0.696 , generator loss=0.730\n",
      "Training progress in epoch #56, step 139, discriminator loss=0.685 , generator loss=0.735\n",
      "Training progress in epoch #56, step 140, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #56, step 141, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #56, step 142, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #56, step 143, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #56, step 144, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #56, step 145, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #56, step 146, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #56, step 147, discriminator loss=0.689 , generator loss=0.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #56, step 148, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #56, step 149, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #56, step 150, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #56, step 151, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #56, step 152, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #56, step 153, discriminator loss=0.694 , generator loss=0.726\n",
      "Training progress in epoch #56, step 154, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #56, step 155, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #56, step 156, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #56, step 157, discriminator loss=0.688 , generator loss=0.666\n",
      "Training progress in epoch #56, step 158, discriminator loss=0.683 , generator loss=0.673\n",
      "Training progress in epoch #56, step 159, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #56, step 160, discriminator loss=0.697 , generator loss=0.722\n",
      "Training progress in epoch #56, step 161, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #56, step 162, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #56, step 163, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #56, step 164, discriminator loss=0.697 , generator loss=0.704\n",
      "Training progress in epoch #56, step 165, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #56, step 166, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #56, step 167, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #56, step 168, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #56, step 169, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #56, step 170, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #56, step 171, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #56, step 172, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #56, step 173, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #56, step 174, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #56, step 175, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #56, step 176, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #56, step 177, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #56, step 178, discriminator loss=0.697 , generator loss=0.688\n",
      "Training progress in epoch #56, step 179, discriminator loss=0.685 , generator loss=0.691\n",
      "Training progress in epoch #56, step 180, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #56, step 181, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #56, step 182, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #56, step 183, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #56, step 184, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #56, step 185, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #56, step 186, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #56, step 187, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #56, step 188, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #56, step 189, discriminator loss=0.685 , generator loss=0.714\n",
      "Training progress in epoch #56, step 190, discriminator loss=0.698 , generator loss=0.718\n",
      "Training progress in epoch #56, step 191, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #56, step 192, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #56, step 193, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #56, step 194, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #56, step 195, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #56, step 196, discriminator loss=0.697 , generator loss=0.709\n",
      "Training progress in epoch #56, step 197, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #56, step 198, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #56, step 199, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #56, step 200, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #56, step 201, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #56, step 202, discriminator loss=0.690 , generator loss=0.741\n",
      "Training progress in epoch #56, step 203, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #56, step 204, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #56, step 205, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #56, step 206, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #56, step 207, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #56, step 208, discriminator loss=0.684 , generator loss=0.705\n",
      "Training progress in epoch #56, step 209, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #56, step 210, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #56, step 211, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #56, step 212, discriminator loss=0.697 , generator loss=0.706\n",
      "Training progress in epoch #56, step 213, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #56, step 214, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #56, step 215, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #56, step 216, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #56, step 217, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #56, step 218, discriminator loss=0.697 , generator loss=0.687\n",
      "Training progress in epoch #56, step 219, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #56, step 220, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #56, step 221, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #56, step 222, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #56, step 223, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #56, step 224, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #56, step 225, discriminator loss=0.695 , generator loss=0.715\n",
      "Training progress in epoch #56, step 226, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #56, step 227, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #56, step 228, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #56, step 229, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #56, step 230, discriminator loss=0.684 , generator loss=0.709\n",
      "Training progress in epoch #56, step 231, discriminator loss=0.688 , generator loss=0.669\n",
      "Training progress in epoch #56, step 232, discriminator loss=0.690 , generator loss=0.654\n",
      "Training progress in epoch #56, step 233, discriminator loss=0.692 , generator loss=0.690\n",
      "Disciminator Accuracy on real images: 76%, on fake images: 40%\n",
      "Training progress in epoch #57, step 0, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #57, step 1, discriminator loss=0.693 , generator loss=0.737\n",
      "Training progress in epoch #57, step 2, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #57, step 3, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #57, step 4, discriminator loss=0.695 , generator loss=0.702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #57, step 5, discriminator loss=0.698 , generator loss=0.702\n",
      "Training progress in epoch #57, step 6, discriminator loss=0.698 , generator loss=0.694\n",
      "Training progress in epoch #57, step 7, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #57, step 8, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #57, step 9, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #57, step 10, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #57, step 11, discriminator loss=0.691 , generator loss=0.732\n",
      "Training progress in epoch #57, step 12, discriminator loss=0.692 , generator loss=0.734\n",
      "Training progress in epoch #57, step 13, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #57, step 14, discriminator loss=0.686 , generator loss=0.676\n",
      "Training progress in epoch #57, step 15, discriminator loss=0.690 , generator loss=0.673\n",
      "Training progress in epoch #57, step 16, discriminator loss=0.697 , generator loss=0.686\n",
      "Training progress in epoch #57, step 17, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #57, step 18, discriminator loss=0.697 , generator loss=0.714\n",
      "Training progress in epoch #57, step 19, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #57, step 20, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #57, step 21, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #57, step 22, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #57, step 23, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #57, step 24, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #57, step 25, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #57, step 26, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #57, step 27, discriminator loss=0.683 , generator loss=0.697\n",
      "Training progress in epoch #57, step 28, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #57, step 29, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #57, step 30, discriminator loss=0.685 , generator loss=0.712\n",
      "Training progress in epoch #57, step 31, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #57, step 32, discriminator loss=0.685 , generator loss=0.709\n",
      "Training progress in epoch #57, step 33, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #57, step 34, discriminator loss=0.681 , generator loss=0.704\n",
      "Training progress in epoch #57, step 35, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #57, step 36, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #57, step 37, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #57, step 38, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #57, step 39, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #57, step 40, discriminator loss=0.685 , generator loss=0.725\n",
      "Training progress in epoch #57, step 41, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #57, step 42, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #57, step 43, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #57, step 44, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #57, step 45, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #57, step 46, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #57, step 47, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #57, step 48, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #57, step 49, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #57, step 50, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #57, step 51, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #57, step 52, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #57, step 53, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #57, step 54, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #57, step 55, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #57, step 56, discriminator loss=0.683 , generator loss=0.695\n",
      "Training progress in epoch #57, step 57, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #57, step 58, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #57, step 59, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #57, step 60, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #57, step 61, discriminator loss=0.683 , generator loss=0.712\n",
      "Training progress in epoch #57, step 62, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #57, step 63, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #57, step 64, discriminator loss=0.690 , generator loss=0.670\n",
      "Training progress in epoch #57, step 65, discriminator loss=0.686 , generator loss=0.673\n",
      "Training progress in epoch #57, step 66, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #57, step 67, discriminator loss=0.695 , generator loss=0.719\n",
      "Training progress in epoch #57, step 68, discriminator loss=0.697 , generator loss=0.723\n",
      "Training progress in epoch #57, step 69, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #57, step 70, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #57, step 71, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #57, step 72, discriminator loss=0.700 , generator loss=0.688\n",
      "Training progress in epoch #57, step 73, discriminator loss=0.697 , generator loss=0.711\n",
      "Training progress in epoch #57, step 74, discriminator loss=0.695 , generator loss=0.738\n",
      "Training progress in epoch #57, step 75, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #57, step 76, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #57, step 77, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #57, step 78, discriminator loss=0.688 , generator loss=0.744\n",
      "Training progress in epoch #57, step 79, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #57, step 80, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #57, step 81, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #57, step 82, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #57, step 83, discriminator loss=0.699 , generator loss=0.685\n",
      "Training progress in epoch #57, step 84, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #57, step 85, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #57, step 86, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #57, step 87, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #57, step 88, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #57, step 89, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #57, step 90, discriminator loss=0.698 , generator loss=0.708\n",
      "Training progress in epoch #57, step 91, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #57, step 92, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #57, step 93, discriminator loss=0.690 , generator loss=0.668\n",
      "Training progress in epoch #57, step 94, discriminator loss=0.690 , generator loss=0.661\n",
      "Training progress in epoch #57, step 95, discriminator loss=0.692 , generator loss=0.675\n",
      "Training progress in epoch #57, step 96, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #57, step 97, discriminator loss=0.689 , generator loss=0.727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #57, step 98, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #57, step 99, discriminator loss=0.687 , generator loss=0.730\n",
      "Training progress in epoch #57, step 100, discriminator loss=0.690 , generator loss=0.734\n",
      "Training progress in epoch #57, step 101, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #57, step 102, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #57, step 103, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #57, step 104, discriminator loss=0.687 , generator loss=0.673\n",
      "Training progress in epoch #57, step 105, discriminator loss=0.685 , generator loss=0.690\n",
      "Training progress in epoch #57, step 106, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #57, step 107, discriminator loss=0.698 , generator loss=0.728\n",
      "Training progress in epoch #57, step 108, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #57, step 109, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #57, step 110, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #57, step 111, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #57, step 112, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #57, step 113, discriminator loss=0.686 , generator loss=0.682\n",
      "Training progress in epoch #57, step 114, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #57, step 115, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #57, step 116, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #57, step 117, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #57, step 118, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #57, step 119, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #57, step 120, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #57, step 121, discriminator loss=0.692 , generator loss=0.731\n",
      "Training progress in epoch #57, step 122, discriminator loss=0.683 , generator loss=0.744\n",
      "Training progress in epoch #57, step 123, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #57, step 124, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #57, step 125, discriminator loss=0.696 , generator loss=0.672\n",
      "Training progress in epoch #57, step 126, discriminator loss=0.689 , generator loss=0.673\n",
      "Training progress in epoch #57, step 127, discriminator loss=0.683 , generator loss=0.715\n",
      "Training progress in epoch #57, step 128, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #57, step 129, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #57, step 130, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #57, step 131, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #57, step 132, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #57, step 133, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #57, step 134, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #57, step 135, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #57, step 136, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #57, step 137, discriminator loss=0.700 , generator loss=0.696\n",
      "Training progress in epoch #57, step 138, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #57, step 139, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #57, step 140, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #57, step 141, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #57, step 142, discriminator loss=0.696 , generator loss=0.724\n",
      "Training progress in epoch #57, step 143, discriminator loss=0.700 , generator loss=0.741\n",
      "Training progress in epoch #57, step 144, discriminator loss=0.691 , generator loss=0.741\n",
      "Training progress in epoch #57, step 145, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #57, step 146, discriminator loss=0.687 , generator loss=0.727\n",
      "Training progress in epoch #57, step 147, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #57, step 148, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #57, step 149, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #57, step 150, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #57, step 151, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #57, step 152, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #57, step 153, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #57, step 154, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #57, step 155, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #57, step 156, discriminator loss=0.693 , generator loss=0.740\n",
      "Training progress in epoch #57, step 157, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #57, step 158, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #57, step 159, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #57, step 160, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #57, step 161, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #57, step 162, discriminator loss=0.683 , generator loss=0.692\n",
      "Training progress in epoch #57, step 163, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #57, step 164, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #57, step 165, discriminator loss=0.685 , generator loss=0.729\n",
      "Training progress in epoch #57, step 166, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #57, step 167, discriminator loss=0.689 , generator loss=0.736\n",
      "Training progress in epoch #57, step 168, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #57, step 169, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #57, step 170, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #57, step 171, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #57, step 172, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #57, step 173, discriminator loss=0.681 , generator loss=0.698\n",
      "Training progress in epoch #57, step 174, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #57, step 175, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #57, step 176, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #57, step 177, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #57, step 178, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #57, step 179, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #57, step 180, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #57, step 181, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #57, step 182, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #57, step 183, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #57, step 184, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #57, step 185, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #57, step 186, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #57, step 187, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #57, step 188, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #57, step 189, discriminator loss=0.694 , generator loss=0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #57, step 190, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #57, step 191, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #57, step 192, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #57, step 193, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #57, step 194, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #57, step 195, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #57, step 196, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #57, step 197, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #57, step 198, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #57, step 199, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #57, step 200, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #57, step 201, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #57, step 202, discriminator loss=0.687 , generator loss=0.676\n",
      "Training progress in epoch #57, step 203, discriminator loss=0.687 , generator loss=0.661\n",
      "Training progress in epoch #57, step 204, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #57, step 205, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #57, step 206, discriminator loss=0.696 , generator loss=0.728\n",
      "Training progress in epoch #57, step 207, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #57, step 208, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #57, step 209, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #57, step 210, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #57, step 211, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #57, step 212, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #57, step 213, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #57, step 214, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #57, step 215, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #57, step 216, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #57, step 217, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #57, step 218, discriminator loss=0.683 , generator loss=0.718\n",
      "Training progress in epoch #57, step 219, discriminator loss=0.685 , generator loss=0.690\n",
      "Training progress in epoch #57, step 220, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #57, step 221, discriminator loss=0.685 , generator loss=0.690\n",
      "Training progress in epoch #57, step 222, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #57, step 223, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #57, step 224, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #57, step 225, discriminator loss=0.699 , generator loss=0.714\n",
      "Training progress in epoch #57, step 226, discriminator loss=0.697 , generator loss=0.704\n",
      "Training progress in epoch #57, step 227, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #57, step 228, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #57, step 229, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #57, step 230, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #57, step 231, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #57, step 232, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #57, step 233, discriminator loss=0.692 , generator loss=0.714\n",
      "Disciminator Accuracy on real images: 12%, on fake images: 92%\n",
      "Training progress in epoch #58, step 0, discriminator loss=0.686 , generator loss=0.730\n",
      "Training progress in epoch #58, step 1, discriminator loss=0.685 , generator loss=0.712\n",
      "Training progress in epoch #58, step 2, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #58, step 3, discriminator loss=0.690 , generator loss=0.659\n",
      "Training progress in epoch #58, step 4, discriminator loss=0.686 , generator loss=0.675\n",
      "Training progress in epoch #58, step 5, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #58, step 6, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #58, step 7, discriminator loss=0.685 , generator loss=0.738\n",
      "Training progress in epoch #58, step 8, discriminator loss=0.694 , generator loss=0.733\n",
      "Training progress in epoch #58, step 9, discriminator loss=0.690 , generator loss=0.727\n",
      "Training progress in epoch #58, step 10, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #58, step 11, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #58, step 12, discriminator loss=0.694 , generator loss=0.674\n",
      "Training progress in epoch #58, step 13, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #58, step 14, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #58, step 15, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #58, step 16, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #58, step 17, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #58, step 18, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #58, step 19, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #58, step 20, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #58, step 21, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #58, step 22, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #58, step 23, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #58, step 24, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #58, step 25, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #58, step 26, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #58, step 27, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #58, step 28, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #58, step 29, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #58, step 30, discriminator loss=0.683 , generator loss=0.691\n",
      "Training progress in epoch #58, step 31, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #58, step 32, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #58, step 33, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #58, step 34, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #58, step 35, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #58, step 36, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #58, step 37, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #58, step 38, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #58, step 39, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #58, step 40, discriminator loss=0.695 , generator loss=0.732\n",
      "Training progress in epoch #58, step 41, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #58, step 42, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #58, step 43, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #58, step 44, discriminator loss=0.698 , generator loss=0.717\n",
      "Training progress in epoch #58, step 45, discriminator loss=0.686 , generator loss=0.740\n",
      "Training progress in epoch #58, step 46, discriminator loss=0.690 , generator loss=0.713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #58, step 47, discriminator loss=0.681 , generator loss=0.703\n",
      "Training progress in epoch #58, step 48, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #58, step 49, discriminator loss=0.686 , generator loss=0.683\n",
      "Training progress in epoch #58, step 50, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #58, step 51, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #58, step 52, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #58, step 53, discriminator loss=0.697 , generator loss=0.692\n",
      "Training progress in epoch #58, step 54, discriminator loss=0.695 , generator loss=0.689\n",
      "Training progress in epoch #58, step 55, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #58, step 56, discriminator loss=0.690 , generator loss=0.729\n",
      "Training progress in epoch #58, step 57, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #58, step 58, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #58, step 59, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #58, step 60, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #58, step 61, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #58, step 62, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #58, step 63, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #58, step 64, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #58, step 65, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #58, step 66, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #58, step 67, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #58, step 68, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #58, step 69, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #58, step 70, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #58, step 71, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #58, step 72, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #58, step 73, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #58, step 74, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #58, step 75, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #58, step 76, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #58, step 77, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #58, step 78, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #58, step 79, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #58, step 80, discriminator loss=0.695 , generator loss=0.676\n",
      "Training progress in epoch #58, step 81, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #58, step 82, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #58, step 83, discriminator loss=0.697 , generator loss=0.724\n",
      "Training progress in epoch #58, step 84, discriminator loss=0.692 , generator loss=0.749\n",
      "Training progress in epoch #58, step 85, discriminator loss=0.690 , generator loss=0.738\n",
      "Training progress in epoch #58, step 86, discriminator loss=0.698 , generator loss=0.697\n",
      "Training progress in epoch #58, step 87, discriminator loss=0.693 , generator loss=0.674\n",
      "Training progress in epoch #58, step 88, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #58, step 89, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #58, step 90, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #58, step 91, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #58, step 92, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #58, step 93, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #58, step 94, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #58, step 95, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #58, step 96, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #58, step 97, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #58, step 98, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #58, step 99, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #58, step 100, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #58, step 101, discriminator loss=0.700 , generator loss=0.689\n",
      "Training progress in epoch #58, step 102, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #58, step 103, discriminator loss=0.698 , generator loss=0.671\n",
      "Training progress in epoch #58, step 104, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #58, step 105, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #58, step 106, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #58, step 107, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #58, step 108, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #58, step 109, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #58, step 110, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #58, step 111, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #58, step 112, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #58, step 113, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #58, step 114, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #58, step 115, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #58, step 116, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #58, step 117, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #58, step 118, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #58, step 119, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #58, step 120, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #58, step 121, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #58, step 122, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #58, step 123, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #58, step 124, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #58, step 125, discriminator loss=0.692 , generator loss=0.674\n",
      "Training progress in epoch #58, step 126, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #58, step 127, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #58, step 128, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #58, step 129, discriminator loss=0.694 , generator loss=0.721\n",
      "Training progress in epoch #58, step 130, discriminator loss=0.684 , generator loss=0.689\n",
      "Training progress in epoch #58, step 131, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #58, step 132, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #58, step 133, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #58, step 134, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #58, step 135, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #58, step 136, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #58, step 137, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #58, step 138, discriminator loss=0.693 , generator loss=0.707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #58, step 139, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #58, step 140, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #58, step 141, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #58, step 142, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #58, step 143, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #58, step 144, discriminator loss=0.691 , generator loss=0.672\n",
      "Training progress in epoch #58, step 145, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #58, step 146, discriminator loss=0.685 , generator loss=0.686\n",
      "Training progress in epoch #58, step 147, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #58, step 148, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #58, step 149, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #58, step 150, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #58, step 151, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #58, step 152, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #58, step 153, discriminator loss=0.693 , generator loss=0.733\n",
      "Training progress in epoch #58, step 154, discriminator loss=0.686 , generator loss=0.747\n",
      "Training progress in epoch #58, step 155, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #58, step 156, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #58, step 157, discriminator loss=0.697 , generator loss=0.687\n",
      "Training progress in epoch #58, step 158, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #58, step 159, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #58, step 160, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #58, step 161, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #58, step 162, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #58, step 163, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #58, step 164, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #58, step 165, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #58, step 166, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #58, step 167, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #58, step 168, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #58, step 169, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #58, step 170, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #58, step 171, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #58, step 172, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #58, step 173, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #58, step 174, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #58, step 175, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #58, step 176, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #58, step 177, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #58, step 178, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #58, step 179, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #58, step 180, discriminator loss=0.699 , generator loss=0.692\n",
      "Training progress in epoch #58, step 181, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #58, step 182, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #58, step 183, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #58, step 184, discriminator loss=0.681 , generator loss=0.708\n",
      "Training progress in epoch #58, step 185, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #58, step 186, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #58, step 187, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #58, step 188, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #58, step 189, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #58, step 190, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #58, step 191, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #58, step 192, discriminator loss=0.697 , generator loss=0.709\n",
      "Training progress in epoch #58, step 193, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #58, step 194, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #58, step 195, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #58, step 196, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #58, step 197, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #58, step 198, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #58, step 199, discriminator loss=0.691 , generator loss=0.732\n",
      "Training progress in epoch #58, step 200, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #58, step 201, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #58, step 202, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #58, step 203, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #58, step 204, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #58, step 205, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #58, step 206, discriminator loss=0.696 , generator loss=0.694\n",
      "Training progress in epoch #58, step 207, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #58, step 208, discriminator loss=0.695 , generator loss=0.675\n",
      "Training progress in epoch #58, step 209, discriminator loss=0.682 , generator loss=0.681\n",
      "Training progress in epoch #58, step 210, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #58, step 211, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #58, step 212, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #58, step 213, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #58, step 214, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #58, step 215, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #58, step 216, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #58, step 217, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #58, step 218, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #58, step 219, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #58, step 220, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #58, step 221, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #58, step 222, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #58, step 223, discriminator loss=0.692 , generator loss=0.729\n",
      "Training progress in epoch #58, step 224, discriminator loss=0.693 , generator loss=0.726\n",
      "Training progress in epoch #58, step 225, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #58, step 226, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #58, step 227, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #58, step 228, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #58, step 229, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #58, step 230, discriminator loss=0.694 , generator loss=0.717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #58, step 231, discriminator loss=0.685 , generator loss=0.698\n",
      "Training progress in epoch #58, step 232, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #58, step 233, discriminator loss=0.694 , generator loss=0.686\n",
      "Disciminator Accuracy on real images: 77%, on fake images: 54%\n",
      "Training progress in epoch #59, step 0, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #59, step 1, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #59, step 2, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #59, step 3, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #59, step 4, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #59, step 5, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #59, step 6, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #59, step 7, discriminator loss=0.685 , generator loss=0.715\n",
      "Training progress in epoch #59, step 8, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #59, step 9, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #59, step 10, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #59, step 11, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #59, step 12, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #59, step 13, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #59, step 14, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #59, step 15, discriminator loss=0.687 , generator loss=0.726\n",
      "Training progress in epoch #59, step 16, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #59, step 17, discriminator loss=0.691 , generator loss=0.676\n",
      "Training progress in epoch #59, step 18, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #59, step 19, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #59, step 20, discriminator loss=0.697 , generator loss=0.686\n",
      "Training progress in epoch #59, step 21, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #59, step 22, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #59, step 23, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #59, step 24, discriminator loss=0.693 , generator loss=0.738\n",
      "Training progress in epoch #59, step 25, discriminator loss=0.689 , generator loss=0.734\n",
      "Training progress in epoch #59, step 26, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #59, step 27, discriminator loss=0.696 , generator loss=0.680\n",
      "Training progress in epoch #59, step 28, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #59, step 29, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #59, step 30, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #59, step 31, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #59, step 32, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #59, step 33, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #59, step 34, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #59, step 35, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #59, step 36, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #59, step 37, discriminator loss=0.694 , generator loss=0.732\n",
      "Training progress in epoch #59, step 38, discriminator loss=0.692 , generator loss=0.742\n",
      "Training progress in epoch #59, step 39, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #59, step 40, discriminator loss=0.697 , generator loss=0.694\n",
      "Training progress in epoch #59, step 41, discriminator loss=0.697 , generator loss=0.677\n",
      "Training progress in epoch #59, step 42, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #59, step 43, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #59, step 44, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #59, step 45, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #59, step 46, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #59, step 47, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #59, step 48, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #59, step 49, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #59, step 50, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #59, step 51, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #59, step 52, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #59, step 53, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #59, step 54, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #59, step 55, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #59, step 56, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #59, step 57, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #59, step 58, discriminator loss=0.681 , generator loss=0.707\n",
      "Training progress in epoch #59, step 59, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #59, step 60, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #59, step 61, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #59, step 62, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #59, step 63, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #59, step 64, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #59, step 65, discriminator loss=0.694 , generator loss=0.680\n",
      "Training progress in epoch #59, step 66, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #59, step 67, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #59, step 68, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #59, step 69, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #59, step 70, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #59, step 71, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #59, step 72, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #59, step 73, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #59, step 74, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #59, step 75, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #59, step 76, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #59, step 77, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #59, step 78, discriminator loss=0.701 , generator loss=0.710\n",
      "Training progress in epoch #59, step 79, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #59, step 80, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #59, step 81, discriminator loss=0.688 , generator loss=0.682\n",
      "Training progress in epoch #59, step 82, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #59, step 83, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #59, step 84, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #59, step 85, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #59, step 86, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #59, step 87, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #59, step 88, discriminator loss=0.694 , generator loss=0.714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #59, step 89, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #59, step 90, discriminator loss=0.685 , generator loss=0.688\n",
      "Training progress in epoch #59, step 91, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #59, step 92, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #59, step 93, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #59, step 94, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #59, step 95, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #59, step 96, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #59, step 97, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #59, step 98, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #59, step 99, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #59, step 100, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #59, step 101, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #59, step 102, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #59, step 103, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #59, step 104, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #59, step 105, discriminator loss=0.694 , generator loss=0.729\n",
      "Training progress in epoch #59, step 106, discriminator loss=0.686 , generator loss=0.734\n",
      "Training progress in epoch #59, step 107, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #59, step 108, discriminator loss=0.696 , generator loss=0.678\n",
      "Training progress in epoch #59, step 109, discriminator loss=0.695 , generator loss=0.670\n",
      "Training progress in epoch #59, step 110, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #59, step 111, discriminator loss=0.685 , generator loss=0.718\n",
      "Training progress in epoch #59, step 112, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #59, step 113, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #59, step 114, discriminator loss=0.697 , generator loss=0.713\n",
      "Training progress in epoch #59, step 115, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #59, step 116, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #59, step 117, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #59, step 118, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #59, step 119, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #59, step 120, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #59, step 121, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #59, step 122, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #59, step 123, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #59, step 124, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #59, step 125, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #59, step 126, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #59, step 127, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #59, step 128, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #59, step 129, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #59, step 130, discriminator loss=0.695 , generator loss=0.719\n",
      "Training progress in epoch #59, step 131, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #59, step 132, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #59, step 133, discriminator loss=0.687 , generator loss=0.679\n",
      "Training progress in epoch #59, step 134, discriminator loss=0.697 , generator loss=0.685\n",
      "Training progress in epoch #59, step 135, discriminator loss=0.696 , generator loss=0.684\n",
      "Training progress in epoch #59, step 136, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #59, step 137, discriminator loss=0.690 , generator loss=0.729\n",
      "Training progress in epoch #59, step 138, discriminator loss=0.693 , generator loss=0.743\n",
      "Training progress in epoch #59, step 139, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #59, step 140, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #59, step 141, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #59, step 142, discriminator loss=0.690 , generator loss=0.734\n",
      "Training progress in epoch #59, step 143, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #59, step 144, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #59, step 145, discriminator loss=0.696 , generator loss=0.690\n",
      "Training progress in epoch #59, step 146, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #59, step 147, discriminator loss=0.696 , generator loss=0.707\n",
      "Training progress in epoch #59, step 148, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #59, step 149, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #59, step 150, discriminator loss=0.694 , generator loss=0.721\n",
      "Training progress in epoch #59, step 151, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #59, step 152, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #59, step 153, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #59, step 154, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #59, step 155, discriminator loss=0.690 , generator loss=0.675\n",
      "Training progress in epoch #59, step 156, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #59, step 157, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #59, step 158, discriminator loss=0.698 , generator loss=0.723\n",
      "Training progress in epoch #59, step 159, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #59, step 160, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #59, step 161, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #59, step 162, discriminator loss=0.692 , generator loss=0.675\n",
      "Training progress in epoch #59, step 163, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #59, step 164, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #59, step 165, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #59, step 166, discriminator loss=0.691 , generator loss=0.727\n",
      "Training progress in epoch #59, step 167, discriminator loss=0.684 , generator loss=0.735\n",
      "Training progress in epoch #59, step 168, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #59, step 169, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #59, step 170, discriminator loss=0.690 , generator loss=0.669\n",
      "Training progress in epoch #59, step 171, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #59, step 172, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #59, step 173, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #59, step 174, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #59, step 175, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #59, step 176, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #59, step 177, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #59, step 178, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #59, step 179, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #59, step 180, discriminator loss=0.691 , generator loss=0.703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #59, step 181, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #59, step 182, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #59, step 183, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #59, step 184, discriminator loss=0.683 , generator loss=0.694\n",
      "Training progress in epoch #59, step 185, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #59, step 186, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #59, step 187, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #59, step 188, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #59, step 189, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #59, step 190, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #59, step 191, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #59, step 192, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #59, step 193, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #59, step 194, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #59, step 195, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #59, step 196, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #59, step 197, discriminator loss=0.700 , generator loss=0.688\n",
      "Training progress in epoch #59, step 198, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #59, step 199, discriminator loss=0.697 , generator loss=0.709\n",
      "Training progress in epoch #59, step 200, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #59, step 201, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #59, step 202, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #59, step 203, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #59, step 204, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #59, step 205, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #59, step 206, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #59, step 207, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #59, step 208, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #59, step 209, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #59, step 210, discriminator loss=0.689 , generator loss=0.735\n",
      "Training progress in epoch #59, step 211, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #59, step 212, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #59, step 213, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #59, step 214, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #59, step 215, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #59, step 216, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #59, step 217, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #59, step 218, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #59, step 219, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #59, step 220, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #59, step 221, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #59, step 222, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #59, step 223, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #59, step 224, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #59, step 225, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #59, step 226, discriminator loss=0.693 , generator loss=0.678\n",
      "Training progress in epoch #59, step 227, discriminator loss=0.689 , generator loss=0.676\n",
      "Training progress in epoch #59, step 228, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #59, step 229, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #59, step 230, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #59, step 231, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #59, step 232, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #59, step 233, discriminator loss=0.690 , generator loss=0.703\n",
      "Disciminator Accuracy on real images: 56%, on fake images: 78%\n",
      "Training progress in epoch #60, step 0, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #60, step 1, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #60, step 2, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #60, step 3, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #60, step 4, discriminator loss=0.688 , generator loss=0.656\n",
      "Training progress in epoch #60, step 5, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #60, step 6, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #60, step 7, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #60, step 8, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #60, step 9, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #60, step 10, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #60, step 11, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #60, step 12, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #60, step 13, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #60, step 14, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #60, step 15, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #60, step 16, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #60, step 17, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #60, step 18, discriminator loss=0.683 , generator loss=0.699\n",
      "Training progress in epoch #60, step 19, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #60, step 20, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #60, step 21, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #60, step 22, discriminator loss=0.686 , generator loss=0.736\n",
      "Training progress in epoch #60, step 23, discriminator loss=0.693 , generator loss=0.729\n",
      "Training progress in epoch #60, step 24, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #60, step 25, discriminator loss=0.693 , generator loss=0.676\n",
      "Training progress in epoch #60, step 26, discriminator loss=0.695 , generator loss=0.682\n",
      "Training progress in epoch #60, step 27, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #60, step 28, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #60, step 29, discriminator loss=0.695 , generator loss=0.734\n",
      "Training progress in epoch #60, step 30, discriminator loss=0.687 , generator loss=0.733\n",
      "Training progress in epoch #60, step 31, discriminator loss=0.695 , generator loss=0.716\n",
      "Training progress in epoch #60, step 32, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #60, step 33, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #60, step 34, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #60, step 35, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #60, step 36, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #60, step 37, discriminator loss=0.690 , generator loss=0.707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #60, step 38, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #60, step 39, discriminator loss=0.689 , generator loss=0.678\n",
      "Training progress in epoch #60, step 40, discriminator loss=0.697 , generator loss=0.695\n",
      "Training progress in epoch #60, step 41, discriminator loss=0.698 , generator loss=0.706\n",
      "Training progress in epoch #60, step 42, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #60, step 43, discriminator loss=0.682 , generator loss=0.712\n",
      "Training progress in epoch #60, step 44, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #60, step 45, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #60, step 46, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #60, step 47, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #60, step 48, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #60, step 49, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #60, step 50, discriminator loss=0.696 , generator loss=0.683\n",
      "Training progress in epoch #60, step 51, discriminator loss=0.689 , generator loss=0.677\n",
      "Training progress in epoch #60, step 52, discriminator loss=0.685 , generator loss=0.687\n",
      "Training progress in epoch #60, step 53, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #60, step 54, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #60, step 55, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #60, step 56, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #60, step 57, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #60, step 58, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #60, step 59, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #60, step 60, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #60, step 61, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #60, step 62, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #60, step 63, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #60, step 64, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #60, step 65, discriminator loss=0.695 , generator loss=0.716\n",
      "Training progress in epoch #60, step 66, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #60, step 67, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #60, step 68, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #60, step 69, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #60, step 70, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #60, step 71, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #60, step 72, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #60, step 73, discriminator loss=0.683 , generator loss=0.698\n",
      "Training progress in epoch #60, step 74, discriminator loss=0.686 , generator loss=0.717\n",
      "Training progress in epoch #60, step 75, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #60, step 76, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #60, step 77, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #60, step 78, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #60, step 79, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #60, step 80, discriminator loss=0.698 , generator loss=0.710\n",
      "Training progress in epoch #60, step 81, discriminator loss=0.696 , generator loss=0.695\n",
      "Training progress in epoch #60, step 82, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #60, step 83, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #60, step 84, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #60, step 85, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #60, step 86, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #60, step 87, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #60, step 88, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #60, step 89, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #60, step 90, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #60, step 91, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #60, step 92, discriminator loss=0.687 , generator loss=0.720\n",
      "Training progress in epoch #60, step 93, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #60, step 94, discriminator loss=0.682 , generator loss=0.724\n",
      "Training progress in epoch #60, step 95, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #60, step 96, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #60, step 97, discriminator loss=0.700 , generator loss=0.678\n",
      "Training progress in epoch #60, step 98, discriminator loss=0.692 , generator loss=0.668\n",
      "Training progress in epoch #60, step 99, discriminator loss=0.697 , generator loss=0.690\n",
      "Training progress in epoch #60, step 100, discriminator loss=0.684 , generator loss=0.722\n",
      "Training progress in epoch #60, step 101, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #60, step 102, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #60, step 103, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #60, step 104, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #60, step 105, discriminator loss=0.690 , generator loss=0.727\n",
      "Training progress in epoch #60, step 106, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #60, step 107, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #60, step 108, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #60, step 109, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #60, step 110, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #60, step 111, discriminator loss=0.682 , generator loss=0.719\n",
      "Training progress in epoch #60, step 112, discriminator loss=0.691 , generator loss=0.739\n",
      "Training progress in epoch #60, step 113, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #60, step 114, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #60, step 115, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #60, step 116, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #60, step 117, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #60, step 118, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #60, step 119, discriminator loss=0.690 , generator loss=0.677\n",
      "Training progress in epoch #60, step 120, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #60, step 121, discriminator loss=0.702 , generator loss=0.730\n",
      "Training progress in epoch #60, step 122, discriminator loss=0.694 , generator loss=0.733\n",
      "Training progress in epoch #60, step 123, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #60, step 124, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #60, step 125, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #60, step 126, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #60, step 127, discriminator loss=0.697 , generator loss=0.703\n",
      "Training progress in epoch #60, step 128, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #60, step 129, discriminator loss=0.700 , generator loss=0.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #60, step 130, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #60, step 131, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #60, step 132, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #60, step 133, discriminator loss=0.699 , generator loss=0.708\n",
      "Training progress in epoch #60, step 134, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #60, step 135, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #60, step 136, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #60, step 137, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #60, step 138, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #60, step 139, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #60, step 140, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #60, step 141, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #60, step 142, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #60, step 143, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #60, step 144, discriminator loss=0.699 , generator loss=0.733\n",
      "Training progress in epoch #60, step 145, discriminator loss=0.691 , generator loss=0.741\n",
      "Training progress in epoch #60, step 146, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #60, step 147, discriminator loss=0.690 , generator loss=0.676\n",
      "Training progress in epoch #60, step 148, discriminator loss=0.687 , generator loss=0.673\n",
      "Training progress in epoch #60, step 149, discriminator loss=0.697 , generator loss=0.679\n",
      "Training progress in epoch #60, step 150, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #60, step 151, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #60, step 152, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #60, step 153, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #60, step 154, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #60, step 155, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #60, step 156, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #60, step 157, discriminator loss=0.697 , generator loss=0.721\n",
      "Training progress in epoch #60, step 158, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #60, step 159, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #60, step 160, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #60, step 161, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #60, step 162, discriminator loss=0.688 , generator loss=0.676\n",
      "Training progress in epoch #60, step 163, discriminator loss=0.686 , generator loss=0.678\n",
      "Training progress in epoch #60, step 164, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #60, step 165, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #60, step 166, discriminator loss=0.697 , generator loss=0.719\n",
      "Training progress in epoch #60, step 167, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #60, step 168, discriminator loss=0.686 , generator loss=0.726\n",
      "Training progress in epoch #60, step 169, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #60, step 170, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #60, step 171, discriminator loss=0.687 , generator loss=0.679\n",
      "Training progress in epoch #60, step 172, discriminator loss=0.690 , generator loss=0.679\n",
      "Training progress in epoch #60, step 173, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #60, step 174, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #60, step 175, discriminator loss=0.691 , generator loss=0.738\n",
      "Training progress in epoch #60, step 176, discriminator loss=0.694 , generator loss=0.728\n",
      "Training progress in epoch #60, step 177, discriminator loss=0.695 , generator loss=0.715\n",
      "Training progress in epoch #60, step 178, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #60, step 179, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #60, step 180, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #60, step 181, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #60, step 182, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #60, step 183, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #60, step 184, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #60, step 185, discriminator loss=0.686 , generator loss=0.724\n",
      "Training progress in epoch #60, step 186, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #60, step 187, discriminator loss=0.697 , generator loss=0.706\n",
      "Training progress in epoch #60, step 188, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #60, step 189, discriminator loss=0.697 , generator loss=0.701\n",
      "Training progress in epoch #60, step 190, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #60, step 191, discriminator loss=0.685 , generator loss=0.716\n",
      "Training progress in epoch #60, step 192, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #60, step 193, discriminator loss=0.697 , generator loss=0.691\n",
      "Training progress in epoch #60, step 194, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #60, step 195, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #60, step 196, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #60, step 197, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #60, step 198, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #60, step 199, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #60, step 200, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #60, step 201, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #60, step 202, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #60, step 203, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #60, step 204, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #60, step 205, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #60, step 206, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #60, step 207, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #60, step 208, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #60, step 209, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #60, step 210, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #60, step 211, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #60, step 212, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #60, step 213, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #60, step 214, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #60, step 215, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #60, step 216, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #60, step 217, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #60, step 218, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #60, step 219, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #60, step 220, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #60, step 221, discriminator loss=0.691 , generator loss=0.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #60, step 222, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #60, step 223, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #60, step 224, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #60, step 225, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #60, step 226, discriminator loss=0.684 , generator loss=0.705\n",
      "Training progress in epoch #60, step 227, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #60, step 228, discriminator loss=0.693 , generator loss=0.737\n",
      "Training progress in epoch #60, step 229, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #60, step 230, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #60, step 231, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #60, step 232, discriminator loss=0.691 , generator loss=0.676\n",
      "Training progress in epoch #60, step 233, discriminator loss=0.689 , generator loss=0.699\n",
      "Disciminator Accuracy on real images: 61%, on fake images: 51%\n",
      "Training progress in epoch #61, step 0, discriminator loss=0.698 , generator loss=0.709\n",
      "Training progress in epoch #61, step 1, discriminator loss=0.697 , generator loss=0.693\n",
      "Training progress in epoch #61, step 2, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #61, step 3, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #61, step 4, discriminator loss=0.704 , generator loss=0.720\n",
      "Training progress in epoch #61, step 5, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #61, step 6, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #61, step 7, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #61, step 8, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #61, step 9, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #61, step 10, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #61, step 11, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #61, step 12, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #61, step 13, discriminator loss=0.689 , generator loss=0.728\n",
      "Training progress in epoch #61, step 14, discriminator loss=0.694 , generator loss=0.731\n",
      "Training progress in epoch #61, step 15, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #61, step 16, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #61, step 17, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #61, step 18, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #61, step 19, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #61, step 20, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #61, step 21, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #61, step 22, discriminator loss=0.684 , generator loss=0.725\n",
      "Training progress in epoch #61, step 23, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #61, step 24, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #61, step 25, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #61, step 26, discriminator loss=0.697 , generator loss=0.699\n",
      "Training progress in epoch #61, step 27, discriminator loss=0.698 , generator loss=0.705\n",
      "Training progress in epoch #61, step 28, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #61, step 29, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #61, step 30, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #61, step 31, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #61, step 32, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #61, step 33, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #61, step 34, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #61, step 35, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #61, step 36, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #61, step 37, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #61, step 38, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #61, step 39, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #61, step 40, discriminator loss=0.684 , generator loss=0.678\n",
      "Training progress in epoch #61, step 41, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #61, step 42, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #61, step 43, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #61, step 44, discriminator loss=0.690 , generator loss=0.738\n",
      "Training progress in epoch #61, step 45, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #61, step 46, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #61, step 47, discriminator loss=0.698 , generator loss=0.680\n",
      "Training progress in epoch #61, step 48, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #61, step 49, discriminator loss=0.683 , generator loss=0.726\n",
      "Training progress in epoch #61, step 50, discriminator loss=0.696 , generator loss=0.719\n",
      "Training progress in epoch #61, step 51, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #61, step 52, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #61, step 53, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #61, step 54, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #61, step 55, discriminator loss=0.700 , generator loss=0.713\n",
      "Training progress in epoch #61, step 56, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #61, step 57, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #61, step 58, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #61, step 59, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #61, step 60, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #61, step 61, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #61, step 62, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #61, step 63, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #61, step 64, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #61, step 65, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #61, step 66, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #61, step 67, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #61, step 68, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #61, step 69, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #61, step 70, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #61, step 71, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #61, step 72, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #61, step 73, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #61, step 74, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #61, step 75, discriminator loss=0.695 , generator loss=0.661\n",
      "Training progress in epoch #61, step 76, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #61, step 77, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #61, step 78, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #61, step 79, discriminator loss=0.687 , generator loss=0.689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #61, step 80, discriminator loss=0.698 , generator loss=0.685\n",
      "Training progress in epoch #61, step 81, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #61, step 82, discriminator loss=0.680 , generator loss=0.705\n",
      "Training progress in epoch #61, step 83, discriminator loss=0.695 , generator loss=0.715\n",
      "Training progress in epoch #61, step 84, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #61, step 85, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #61, step 86, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #61, step 87, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #61, step 88, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #61, step 89, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #61, step 90, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #61, step 91, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #61, step 92, discriminator loss=0.697 , generator loss=0.700\n",
      "Training progress in epoch #61, step 93, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #61, step 94, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #61, step 95, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #61, step 96, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #61, step 97, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #61, step 98, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #61, step 99, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #61, step 100, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #61, step 101, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #61, step 102, discriminator loss=0.698 , generator loss=0.711\n",
      "Training progress in epoch #61, step 103, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #61, step 104, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #61, step 105, discriminator loss=0.686 , generator loss=0.729\n",
      "Training progress in epoch #61, step 106, discriminator loss=0.686 , generator loss=0.722\n",
      "Training progress in epoch #61, step 107, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #61, step 108, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #61, step 109, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #61, step 110, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #61, step 111, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #61, step 112, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #61, step 113, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #61, step 114, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #61, step 115, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #61, step 116, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #61, step 117, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #61, step 118, discriminator loss=0.697 , generator loss=0.691\n",
      "Training progress in epoch #61, step 119, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #61, step 120, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #61, step 121, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #61, step 122, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #61, step 123, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #61, step 124, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #61, step 125, discriminator loss=0.696 , generator loss=0.690\n",
      "Training progress in epoch #61, step 126, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #61, step 127, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #61, step 128, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #61, step 129, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #61, step 130, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #61, step 131, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #61, step 132, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #61, step 133, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #61, step 134, discriminator loss=0.683 , generator loss=0.697\n",
      "Training progress in epoch #61, step 135, discriminator loss=0.693 , generator loss=0.676\n",
      "Training progress in epoch #61, step 136, discriminator loss=0.684 , generator loss=0.689\n",
      "Training progress in epoch #61, step 137, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #61, step 138, discriminator loss=0.690 , generator loss=0.731\n",
      "Training progress in epoch #61, step 139, discriminator loss=0.698 , generator loss=0.712\n",
      "Training progress in epoch #61, step 140, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #61, step 141, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #61, step 142, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #61, step 143, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #61, step 144, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #61, step 145, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #61, step 146, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #61, step 147, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #61, step 148, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #61, step 149, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #61, step 150, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #61, step 151, discriminator loss=0.683 , generator loss=0.705\n",
      "Training progress in epoch #61, step 152, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #61, step 153, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #61, step 154, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #61, step 155, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #61, step 156, discriminator loss=0.695 , generator loss=0.671\n",
      "Training progress in epoch #61, step 157, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #61, step 158, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #61, step 159, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #61, step 160, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #61, step 161, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #61, step 162, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #61, step 163, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #61, step 164, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #61, step 165, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #61, step 166, discriminator loss=0.696 , generator loss=0.680\n",
      "Training progress in epoch #61, step 167, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #61, step 168, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #61, step 169, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #61, step 170, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #61, step 171, discriminator loss=0.691 , generator loss=0.707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #61, step 172, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #61, step 173, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #61, step 174, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #61, step 175, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #61, step 176, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #61, step 177, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #61, step 178, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #61, step 179, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #61, step 180, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #61, step 181, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #61, step 182, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #61, step 183, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #61, step 184, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #61, step 185, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #61, step 186, discriminator loss=0.698 , generator loss=0.711\n",
      "Training progress in epoch #61, step 187, discriminator loss=0.690 , generator loss=0.727\n",
      "Training progress in epoch #61, step 188, discriminator loss=0.693 , generator loss=0.738\n",
      "Training progress in epoch #61, step 189, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #61, step 190, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #61, step 191, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #61, step 192, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #61, step 193, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #61, step 194, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #61, step 195, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #61, step 196, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #61, step 197, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #61, step 198, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #61, step 199, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #61, step 200, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #61, step 201, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #61, step 202, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #61, step 203, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #61, step 204, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #61, step 205, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #61, step 206, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #61, step 207, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #61, step 208, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #61, step 209, discriminator loss=0.698 , generator loss=0.703\n",
      "Training progress in epoch #61, step 210, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #61, step 211, discriminator loss=0.698 , generator loss=0.700\n",
      "Training progress in epoch #61, step 212, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #61, step 213, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #61, step 214, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #61, step 215, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #61, step 216, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #61, step 217, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #61, step 218, discriminator loss=0.694 , generator loss=0.679\n",
      "Training progress in epoch #61, step 219, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #61, step 220, discriminator loss=0.696 , generator loss=0.706\n",
      "Training progress in epoch #61, step 221, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #61, step 222, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #61, step 223, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #61, step 224, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #61, step 225, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #61, step 226, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #61, step 227, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #61, step 228, discriminator loss=0.684 , generator loss=0.708\n",
      "Training progress in epoch #61, step 229, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #61, step 230, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #61, step 231, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #61, step 232, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #61, step 233, discriminator loss=0.690 , generator loss=0.695\n",
      "Disciminator Accuracy on real images: 55%, on fake images: 59%\n",
      "Training progress in epoch #62, step 0, discriminator loss=0.698 , generator loss=0.694\n",
      "Training progress in epoch #62, step 1, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #62, step 2, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #62, step 3, discriminator loss=0.698 , generator loss=0.710\n",
      "Training progress in epoch #62, step 4, discriminator loss=0.699 , generator loss=0.711\n",
      "Training progress in epoch #62, step 5, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #62, step 6, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #62, step 7, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #62, step 8, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #62, step 9, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #62, step 10, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #62, step 11, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #62, step 12, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #62, step 13, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #62, step 14, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #62, step 15, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #62, step 16, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #62, step 17, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #62, step 18, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #62, step 19, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #62, step 20, discriminator loss=0.698 , generator loss=0.716\n",
      "Training progress in epoch #62, step 21, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #62, step 22, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #62, step 23, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #62, step 24, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #62, step 25, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #62, step 26, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #62, step 27, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #62, step 28, discriminator loss=0.693 , generator loss=0.697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #62, step 29, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #62, step 30, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #62, step 31, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #62, step 32, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #62, step 33, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #62, step 34, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #62, step 35, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #62, step 36, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #62, step 37, discriminator loss=0.690 , generator loss=0.727\n",
      "Training progress in epoch #62, step 38, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #62, step 39, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #62, step 40, discriminator loss=0.686 , generator loss=0.683\n",
      "Training progress in epoch #62, step 41, discriminator loss=0.688 , generator loss=0.682\n",
      "Training progress in epoch #62, step 42, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #62, step 43, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #62, step 44, discriminator loss=0.684 , generator loss=0.717\n",
      "Training progress in epoch #62, step 45, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #62, step 46, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #62, step 47, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #62, step 48, discriminator loss=0.695 , generator loss=0.719\n",
      "Training progress in epoch #62, step 49, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #62, step 50, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #62, step 51, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #62, step 52, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #62, step 53, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #62, step 54, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #62, step 55, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #62, step 56, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #62, step 57, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #62, step 58, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #62, step 59, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #62, step 60, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #62, step 61, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #62, step 62, discriminator loss=0.684 , generator loss=0.720\n",
      "Training progress in epoch #62, step 63, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #62, step 64, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #62, step 65, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #62, step 66, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #62, step 67, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #62, step 68, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #62, step 69, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #62, step 70, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #62, step 71, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #62, step 72, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #62, step 73, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #62, step 74, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #62, step 75, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #62, step 76, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #62, step 77, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #62, step 78, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #62, step 79, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #62, step 80, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #62, step 81, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #62, step 82, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #62, step 83, discriminator loss=0.697 , generator loss=0.687\n",
      "Training progress in epoch #62, step 84, discriminator loss=0.697 , generator loss=0.682\n",
      "Training progress in epoch #62, step 85, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #62, step 86, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #62, step 87, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #62, step 88, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #62, step 89, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #62, step 90, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #62, step 91, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #62, step 92, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #62, step 93, discriminator loss=0.689 , generator loss=0.673\n",
      "Training progress in epoch #62, step 94, discriminator loss=0.693 , generator loss=0.676\n",
      "Training progress in epoch #62, step 95, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #62, step 96, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #62, step 97, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #62, step 98, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #62, step 99, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #62, step 100, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #62, step 101, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #62, step 102, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #62, step 103, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #62, step 104, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #62, step 105, discriminator loss=0.683 , generator loss=0.691\n",
      "Training progress in epoch #62, step 106, discriminator loss=0.682 , generator loss=0.685\n",
      "Training progress in epoch #62, step 107, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #62, step 108, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #62, step 109, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #62, step 110, discriminator loss=0.693 , generator loss=0.732\n",
      "Training progress in epoch #62, step 111, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #62, step 112, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #62, step 113, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #62, step 114, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #62, step 115, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #62, step 116, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #62, step 117, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #62, step 118, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #62, step 119, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #62, step 120, discriminator loss=0.691 , generator loss=0.712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #62, step 121, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #62, step 122, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #62, step 123, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #62, step 124, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #62, step 125, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #62, step 126, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #62, step 127, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #62, step 128, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #62, step 129, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #62, step 130, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #62, step 131, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #62, step 132, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #62, step 133, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #62, step 134, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #62, step 135, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #62, step 136, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #62, step 137, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #62, step 138, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #62, step 139, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #62, step 140, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #62, step 141, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #62, step 142, discriminator loss=0.697 , generator loss=0.735\n",
      "Training progress in epoch #62, step 143, discriminator loss=0.688 , generator loss=0.740\n",
      "Training progress in epoch #62, step 144, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #62, step 145, discriminator loss=0.697 , generator loss=0.693\n",
      "Training progress in epoch #62, step 146, discriminator loss=0.687 , generator loss=0.668\n",
      "Training progress in epoch #62, step 147, discriminator loss=0.686 , generator loss=0.669\n",
      "Training progress in epoch #62, step 148, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #62, step 149, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #62, step 150, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #62, step 151, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #62, step 152, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #62, step 153, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #62, step 154, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #62, step 155, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #62, step 156, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #62, step 157, discriminator loss=0.696 , generator loss=0.689\n",
      "Training progress in epoch #62, step 158, discriminator loss=0.686 , generator loss=0.685\n",
      "Training progress in epoch #62, step 159, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #62, step 160, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #62, step 161, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #62, step 162, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #62, step 163, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #62, step 164, discriminator loss=0.696 , generator loss=0.729\n",
      "Training progress in epoch #62, step 165, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #62, step 166, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #62, step 167, discriminator loss=0.689 , generator loss=0.675\n",
      "Training progress in epoch #62, step 168, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #62, step 169, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #62, step 170, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #62, step 171, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #62, step 172, discriminator loss=0.699 , generator loss=0.706\n",
      "Training progress in epoch #62, step 173, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #62, step 174, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #62, step 175, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #62, step 176, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #62, step 177, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #62, step 178, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #62, step 179, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #62, step 180, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #62, step 181, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #62, step 182, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #62, step 183, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #62, step 184, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #62, step 185, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #62, step 186, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #62, step 187, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #62, step 188, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #62, step 189, discriminator loss=0.684 , generator loss=0.686\n",
      "Training progress in epoch #62, step 190, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #62, step 191, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #62, step 192, discriminator loss=0.698 , generator loss=0.707\n",
      "Training progress in epoch #62, step 193, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #62, step 194, discriminator loss=0.697 , generator loss=0.693\n",
      "Training progress in epoch #62, step 195, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #62, step 196, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #62, step 197, discriminator loss=0.683 , generator loss=0.713\n",
      "Training progress in epoch #62, step 198, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #62, step 199, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #62, step 200, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #62, step 201, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #62, step 202, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #62, step 203, discriminator loss=0.683 , generator loss=0.693\n",
      "Training progress in epoch #62, step 204, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #62, step 205, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #62, step 206, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #62, step 207, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #62, step 208, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #62, step 209, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #62, step 210, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #62, step 211, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #62, step 212, discriminator loss=0.690 , generator loss=0.704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #62, step 213, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #62, step 214, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #62, step 215, discriminator loss=0.697 , generator loss=0.699\n",
      "Training progress in epoch #62, step 216, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #62, step 217, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #62, step 218, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #62, step 219, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #62, step 220, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #62, step 221, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #62, step 222, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #62, step 223, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #62, step 224, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #62, step 225, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #62, step 226, discriminator loss=0.695 , generator loss=0.726\n",
      "Training progress in epoch #62, step 227, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #62, step 228, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #62, step 229, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #62, step 230, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #62, step 231, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #62, step 232, discriminator loss=0.691 , generator loss=0.675\n",
      "Training progress in epoch #62, step 233, discriminator loss=0.690 , generator loss=0.687\n",
      "Disciminator Accuracy on real images: 93%, on fake images: 25%\n",
      "Training progress in epoch #63, step 0, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #63, step 1, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #63, step 2, discriminator loss=0.695 , generator loss=0.724\n",
      "Training progress in epoch #63, step 3, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #63, step 4, discriminator loss=0.682 , generator loss=0.703\n",
      "Training progress in epoch #63, step 5, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #63, step 6, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #63, step 7, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #63, step 8, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #63, step 9, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #63, step 10, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #63, step 11, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #63, step 12, discriminator loss=0.697 , generator loss=0.703\n",
      "Training progress in epoch #63, step 13, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #63, step 14, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #63, step 15, discriminator loss=0.687 , generator loss=0.720\n",
      "Training progress in epoch #63, step 16, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #63, step 17, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #63, step 18, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #63, step 19, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #63, step 20, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #63, step 21, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #63, step 22, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #63, step 23, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #63, step 24, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #63, step 25, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #63, step 26, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #63, step 27, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #63, step 28, discriminator loss=0.700 , generator loss=0.724\n",
      "Training progress in epoch #63, step 29, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #63, step 30, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #63, step 31, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #63, step 32, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #63, step 33, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #63, step 34, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #63, step 35, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #63, step 36, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #63, step 37, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #63, step 38, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #63, step 39, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #63, step 40, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #63, step 41, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #63, step 42, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #63, step 43, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #63, step 44, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #63, step 45, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #63, step 46, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #63, step 47, discriminator loss=0.691 , generator loss=0.738\n",
      "Training progress in epoch #63, step 48, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #63, step 49, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #63, step 50, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #63, step 51, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #63, step 52, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #63, step 53, discriminator loss=0.693 , generator loss=0.737\n",
      "Training progress in epoch #63, step 54, discriminator loss=0.694 , generator loss=0.746\n",
      "Training progress in epoch #63, step 55, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #63, step 56, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #63, step 57, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #63, step 58, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #63, step 59, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #63, step 60, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #63, step 61, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #63, step 62, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #63, step 63, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #63, step 64, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #63, step 65, discriminator loss=0.684 , generator loss=0.721\n",
      "Training progress in epoch #63, step 66, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #63, step 67, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #63, step 68, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #63, step 69, discriminator loss=0.696 , generator loss=0.669\n",
      "Training progress in epoch #63, step 70, discriminator loss=0.689 , generator loss=0.678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #63, step 71, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #63, step 72, discriminator loss=0.698 , generator loss=0.745\n",
      "Training progress in epoch #63, step 73, discriminator loss=0.685 , generator loss=0.718\n",
      "Training progress in epoch #63, step 74, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #63, step 75, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #63, step 76, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #63, step 77, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #63, step 78, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #63, step 79, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #63, step 80, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #63, step 81, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #63, step 82, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #63, step 83, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #63, step 84, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #63, step 85, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #63, step 86, discriminator loss=0.691 , generator loss=0.671\n",
      "Training progress in epoch #63, step 87, discriminator loss=0.688 , generator loss=0.676\n",
      "Training progress in epoch #63, step 88, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #63, step 89, discriminator loss=0.698 , generator loss=0.714\n",
      "Training progress in epoch #63, step 90, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #63, step 91, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #63, step 92, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #63, step 93, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #63, step 94, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #63, step 95, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #63, step 96, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #63, step 97, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #63, step 98, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #63, step 99, discriminator loss=0.696 , generator loss=0.736\n",
      "Training progress in epoch #63, step 100, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #63, step 101, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #63, step 102, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #63, step 103, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #63, step 104, discriminator loss=0.702 , generator loss=0.708\n",
      "Training progress in epoch #63, step 105, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #63, step 106, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #63, step 107, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #63, step 108, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #63, step 109, discriminator loss=0.692 , generator loss=0.736\n",
      "Training progress in epoch #63, step 110, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #63, step 111, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #63, step 112, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #63, step 113, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #63, step 114, discriminator loss=0.697 , generator loss=0.708\n",
      "Training progress in epoch #63, step 115, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #63, step 116, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #63, step 117, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #63, step 118, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #63, step 119, discriminator loss=0.689 , generator loss=0.676\n",
      "Training progress in epoch #63, step 120, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #63, step 121, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #63, step 122, discriminator loss=0.684 , generator loss=0.699\n",
      "Training progress in epoch #63, step 123, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #63, step 124, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #63, step 125, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #63, step 126, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #63, step 127, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #63, step 128, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #63, step 129, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #63, step 130, discriminator loss=0.690 , generator loss=0.677\n",
      "Training progress in epoch #63, step 131, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #63, step 132, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #63, step 133, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #63, step 134, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #63, step 135, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #63, step 136, discriminator loss=0.689 , generator loss=0.727\n",
      "Training progress in epoch #63, step 137, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #63, step 138, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #63, step 139, discriminator loss=0.697 , generator loss=0.674\n",
      "Training progress in epoch #63, step 140, discriminator loss=0.683 , generator loss=0.691\n",
      "Training progress in epoch #63, step 141, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #63, step 142, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #63, step 143, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #63, step 144, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #63, step 145, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #63, step 146, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #63, step 147, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #63, step 148, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #63, step 149, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #63, step 150, discriminator loss=0.695 , generator loss=0.683\n",
      "Training progress in epoch #63, step 151, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #63, step 152, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #63, step 153, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #63, step 154, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #63, step 155, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #63, step 156, discriminator loss=0.683 , generator loss=0.721\n",
      "Training progress in epoch #63, step 157, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #63, step 158, discriminator loss=0.686 , generator loss=0.679\n",
      "Training progress in epoch #63, step 159, discriminator loss=0.689 , generator loss=0.662\n",
      "Training progress in epoch #63, step 160, discriminator loss=0.690 , generator loss=0.670\n",
      "Training progress in epoch #63, step 161, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #63, step 162, discriminator loss=0.684 , generator loss=0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #63, step 163, discriminator loss=0.700 , generator loss=0.733\n",
      "Training progress in epoch #63, step 164, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #63, step 165, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #63, step 166, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #63, step 167, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #63, step 168, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #63, step 169, discriminator loss=0.694 , generator loss=0.676\n",
      "Training progress in epoch #63, step 170, discriminator loss=0.695 , generator loss=0.682\n",
      "Training progress in epoch #63, step 171, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #63, step 172, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #63, step 173, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #63, step 174, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #63, step 175, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #63, step 176, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #63, step 177, discriminator loss=0.681 , generator loss=0.696\n",
      "Training progress in epoch #63, step 178, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #63, step 179, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #63, step 180, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #63, step 181, discriminator loss=0.684 , generator loss=0.716\n",
      "Training progress in epoch #63, step 182, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #63, step 183, discriminator loss=0.683 , generator loss=0.703\n",
      "Training progress in epoch #63, step 184, discriminator loss=0.697 , generator loss=0.689\n",
      "Training progress in epoch #63, step 185, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #63, step 186, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #63, step 187, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #63, step 188, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #63, step 189, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #63, step 190, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #63, step 191, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #63, step 192, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #63, step 193, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #63, step 194, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #63, step 195, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #63, step 196, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #63, step 197, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #63, step 198, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #63, step 199, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #63, step 200, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #63, step 201, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #63, step 202, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #63, step 203, discriminator loss=0.685 , generator loss=0.680\n",
      "Training progress in epoch #63, step 204, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #63, step 205, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #63, step 206, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #63, step 207, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #63, step 208, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #63, step 209, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #63, step 210, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #63, step 211, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #63, step 212, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #63, step 213, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #63, step 214, discriminator loss=0.697 , generator loss=0.683\n",
      "Training progress in epoch #63, step 215, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #63, step 216, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #63, step 217, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #63, step 218, discriminator loss=0.697 , generator loss=0.698\n",
      "Training progress in epoch #63, step 219, discriminator loss=0.697 , generator loss=0.708\n",
      "Training progress in epoch #63, step 220, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #63, step 221, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #63, step 222, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #63, step 223, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #63, step 224, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #63, step 225, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #63, step 226, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #63, step 227, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #63, step 228, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #63, step 229, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #63, step 230, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #63, step 231, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #63, step 232, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #63, step 233, discriminator loss=0.691 , generator loss=0.694\n",
      "Disciminator Accuracy on real images: 51%, on fake images: 64%\n",
      "Training progress in epoch #64, step 0, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #64, step 1, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #64, step 2, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #64, step 3, discriminator loss=0.693 , generator loss=0.734\n",
      "Training progress in epoch #64, step 4, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #64, step 5, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #64, step 6, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #64, step 7, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #64, step 8, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #64, step 9, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #64, step 10, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #64, step 11, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #64, step 12, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #64, step 13, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #64, step 14, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #64, step 15, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #64, step 16, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #64, step 17, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #64, step 18, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #64, step 19, discriminator loss=0.689 , generator loss=0.689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #64, step 20, discriminator loss=0.696 , generator loss=0.707\n",
      "Training progress in epoch #64, step 21, discriminator loss=0.692 , generator loss=0.733\n",
      "Training progress in epoch #64, step 22, discriminator loss=0.695 , generator loss=0.750\n",
      "Training progress in epoch #64, step 23, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #64, step 24, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #64, step 25, discriminator loss=0.696 , generator loss=0.679\n",
      "Training progress in epoch #64, step 26, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #64, step 27, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #64, step 28, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #64, step 29, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #64, step 30, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #64, step 31, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #64, step 32, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #64, step 33, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #64, step 34, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #64, step 35, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #64, step 36, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #64, step 37, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #64, step 38, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #64, step 39, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #64, step 40, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #64, step 41, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #64, step 42, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #64, step 43, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #64, step 44, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #64, step 45, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #64, step 46, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #64, step 47, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #64, step 48, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #64, step 49, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #64, step 50, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #64, step 51, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #64, step 52, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #64, step 53, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #64, step 54, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #64, step 55, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #64, step 56, discriminator loss=0.699 , generator loss=0.703\n",
      "Training progress in epoch #64, step 57, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #64, step 58, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #64, step 59, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #64, step 60, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #64, step 61, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #64, step 62, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #64, step 63, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #64, step 64, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #64, step 65, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #64, step 66, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #64, step 67, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #64, step 68, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #64, step 69, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #64, step 70, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #64, step 71, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #64, step 72, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #64, step 73, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #64, step 74, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #64, step 75, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #64, step 76, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #64, step 77, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #64, step 78, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #64, step 79, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #64, step 80, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #64, step 81, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #64, step 82, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #64, step 83, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #64, step 84, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #64, step 85, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #64, step 86, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #64, step 87, discriminator loss=0.700 , generator loss=0.682\n",
      "Training progress in epoch #64, step 88, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #64, step 89, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #64, step 90, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #64, step 91, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #64, step 92, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #64, step 93, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #64, step 94, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #64, step 95, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #64, step 96, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #64, step 97, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #64, step 98, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #64, step 99, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #64, step 100, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #64, step 101, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #64, step 102, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #64, step 103, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #64, step 104, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #64, step 105, discriminator loss=0.694 , generator loss=0.670\n",
      "Training progress in epoch #64, step 106, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #64, step 107, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #64, step 108, discriminator loss=0.696 , generator loss=0.739\n",
      "Training progress in epoch #64, step 109, discriminator loss=0.683 , generator loss=0.726\n",
      "Training progress in epoch #64, step 110, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #64, step 111, discriminator loss=0.694 , generator loss=0.683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #64, step 112, discriminator loss=0.697 , generator loss=0.700\n",
      "Training progress in epoch #64, step 113, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #64, step 114, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #64, step 115, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #64, step 116, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #64, step 117, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #64, step 118, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #64, step 119, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #64, step 120, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #64, step 121, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #64, step 122, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #64, step 123, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #64, step 124, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #64, step 125, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #64, step 126, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #64, step 127, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #64, step 128, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #64, step 129, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #64, step 130, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #64, step 131, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #64, step 132, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #64, step 133, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #64, step 134, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #64, step 135, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #64, step 136, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #64, step 137, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #64, step 138, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #64, step 139, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #64, step 140, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #64, step 141, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #64, step 142, discriminator loss=0.684 , generator loss=0.710\n",
      "Training progress in epoch #64, step 143, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #64, step 144, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #64, step 145, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #64, step 146, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #64, step 147, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #64, step 148, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #64, step 149, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #64, step 150, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #64, step 151, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #64, step 152, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #64, step 153, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #64, step 154, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #64, step 155, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #64, step 156, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #64, step 157, discriminator loss=0.695 , generator loss=0.715\n",
      "Training progress in epoch #64, step 158, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #64, step 159, discriminator loss=0.688 , generator loss=0.675\n",
      "Training progress in epoch #64, step 160, discriminator loss=0.688 , generator loss=0.669\n",
      "Training progress in epoch #64, step 161, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #64, step 162, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #64, step 163, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #64, step 164, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #64, step 165, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #64, step 166, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #64, step 167, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #64, step 168, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #64, step 169, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #64, step 170, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #64, step 171, discriminator loss=0.697 , generator loss=0.690\n",
      "Training progress in epoch #64, step 172, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #64, step 173, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #64, step 174, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #64, step 175, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #64, step 176, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #64, step 177, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #64, step 178, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #64, step 179, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #64, step 180, discriminator loss=0.699 , generator loss=0.699\n",
      "Training progress in epoch #64, step 181, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #64, step 182, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #64, step 183, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #64, step 184, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #64, step 185, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #64, step 186, discriminator loss=0.694 , generator loss=0.731\n",
      "Training progress in epoch #64, step 187, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #64, step 188, discriminator loss=0.697 , generator loss=0.690\n",
      "Training progress in epoch #64, step 189, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #64, step 190, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #64, step 191, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #64, step 192, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #64, step 193, discriminator loss=0.683 , generator loss=0.727\n",
      "Training progress in epoch #64, step 194, discriminator loss=0.690 , generator loss=0.729\n",
      "Training progress in epoch #64, step 195, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #64, step 196, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #64, step 197, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #64, step 198, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #64, step 199, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #64, step 200, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #64, step 201, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #64, step 202, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #64, step 203, discriminator loss=0.688 , generator loss=0.717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #64, step 204, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #64, step 205, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #64, step 206, discriminator loss=0.697 , generator loss=0.717\n",
      "Training progress in epoch #64, step 207, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #64, step 208, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #64, step 209, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #64, step 210, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #64, step 211, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #64, step 212, discriminator loss=0.697 , generator loss=0.700\n",
      "Training progress in epoch #64, step 213, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #64, step 214, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #64, step 215, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #64, step 216, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #64, step 217, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #64, step 218, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #64, step 219, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #64, step 220, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #64, step 221, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #64, step 222, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #64, step 223, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #64, step 224, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #64, step 225, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #64, step 226, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #64, step 227, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #64, step 228, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #64, step 229, discriminator loss=0.698 , generator loss=0.701\n",
      "Training progress in epoch #64, step 230, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #64, step 231, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #64, step 232, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #64, step 233, discriminator loss=0.691 , generator loss=0.689\n",
      "Disciminator Accuracy on real images: 51%, on fake images: 71%\n",
      "Training progress in epoch #65, step 0, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #65, step 1, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #65, step 2, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #65, step 3, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #65, step 4, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #65, step 5, discriminator loss=0.685 , generator loss=0.713\n",
      "Training progress in epoch #65, step 6, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #65, step 7, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #65, step 8, discriminator loss=0.698 , generator loss=0.687\n",
      "Training progress in epoch #65, step 9, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #65, step 10, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #65, step 11, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #65, step 12, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #65, step 13, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #65, step 14, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #65, step 15, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #65, step 16, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #65, step 17, discriminator loss=0.696 , generator loss=0.724\n",
      "Training progress in epoch #65, step 18, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #65, step 19, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #65, step 20, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #65, step 21, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #65, step 22, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #65, step 23, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #65, step 24, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #65, step 25, discriminator loss=0.695 , generator loss=0.719\n",
      "Training progress in epoch #65, step 26, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #65, step 27, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #65, step 28, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #65, step 29, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #65, step 30, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #65, step 31, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #65, step 32, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #65, step 33, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #65, step 34, discriminator loss=0.685 , generator loss=0.718\n",
      "Training progress in epoch #65, step 35, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #65, step 36, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #65, step 37, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #65, step 38, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #65, step 39, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #65, step 40, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #65, step 41, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #65, step 42, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #65, step 43, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #65, step 44, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #65, step 45, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #65, step 46, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #65, step 47, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #65, step 48, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #65, step 49, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #65, step 50, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #65, step 51, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #65, step 52, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #65, step 53, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #65, step 54, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #65, step 55, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #65, step 56, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #65, step 57, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #65, step 58, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #65, step 59, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #65, step 60, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #65, step 61, discriminator loss=0.690 , generator loss=0.692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #65, step 62, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #65, step 63, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #65, step 64, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #65, step 65, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #65, step 66, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #65, step 67, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #65, step 68, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #65, step 69, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #65, step 70, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #65, step 71, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #65, step 72, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #65, step 73, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #65, step 74, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #65, step 75, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #65, step 76, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #65, step 77, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #65, step 78, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #65, step 79, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #65, step 80, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #65, step 81, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #65, step 82, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #65, step 83, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #65, step 84, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #65, step 85, discriminator loss=0.701 , generator loss=0.704\n",
      "Training progress in epoch #65, step 86, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #65, step 87, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #65, step 88, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #65, step 89, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #65, step 90, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #65, step 91, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #65, step 92, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #65, step 93, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #65, step 94, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #65, step 95, discriminator loss=0.683 , generator loss=0.684\n",
      "Training progress in epoch #65, step 96, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #65, step 97, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #65, step 98, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #65, step 99, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #65, step 100, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #65, step 101, discriminator loss=0.701 , generator loss=0.705\n",
      "Training progress in epoch #65, step 102, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #65, step 103, discriminator loss=0.697 , generator loss=0.694\n",
      "Training progress in epoch #65, step 104, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #65, step 105, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #65, step 106, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #65, step 107, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #65, step 108, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #65, step 109, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #65, step 110, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #65, step 111, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #65, step 112, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #65, step 113, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #65, step 114, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #65, step 115, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #65, step 116, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #65, step 117, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #65, step 118, discriminator loss=0.699 , generator loss=0.722\n",
      "Training progress in epoch #65, step 119, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #65, step 120, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #65, step 121, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #65, step 122, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #65, step 123, discriminator loss=0.700 , generator loss=0.711\n",
      "Training progress in epoch #65, step 124, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #65, step 125, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #65, step 126, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #65, step 127, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #65, step 128, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #65, step 129, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #65, step 130, discriminator loss=0.685 , generator loss=0.720\n",
      "Training progress in epoch #65, step 131, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #65, step 132, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #65, step 133, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #65, step 134, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #65, step 135, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #65, step 136, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #65, step 137, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #65, step 138, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #65, step 139, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #65, step 140, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #65, step 141, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #65, step 142, discriminator loss=0.699 , generator loss=0.705\n",
      "Training progress in epoch #65, step 143, discriminator loss=0.684 , generator loss=0.685\n",
      "Training progress in epoch #65, step 144, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #65, step 145, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #65, step 146, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #65, step 147, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #65, step 148, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #65, step 149, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #65, step 150, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #65, step 151, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #65, step 152, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #65, step 153, discriminator loss=0.691 , generator loss=0.699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #65, step 154, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #65, step 155, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #65, step 156, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #65, step 157, discriminator loss=0.696 , generator loss=0.707\n",
      "Training progress in epoch #65, step 158, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #65, step 159, discriminator loss=0.691 , generator loss=0.740\n",
      "Training progress in epoch #65, step 160, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #65, step 161, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #65, step 162, discriminator loss=0.690 , generator loss=0.679\n",
      "Training progress in epoch #65, step 163, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #65, step 164, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #65, step 165, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #65, step 166, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #65, step 167, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #65, step 168, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #65, step 169, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #65, step 170, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #65, step 171, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #65, step 172, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #65, step 173, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #65, step 174, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #65, step 175, discriminator loss=0.686 , generator loss=0.684\n",
      "Training progress in epoch #65, step 176, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #65, step 177, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #65, step 178, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #65, step 179, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #65, step 180, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #65, step 181, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #65, step 182, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #65, step 183, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #65, step 184, discriminator loss=0.697 , generator loss=0.727\n",
      "Training progress in epoch #65, step 185, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #65, step 186, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #65, step 187, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #65, step 188, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #65, step 189, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #65, step 190, discriminator loss=0.699 , generator loss=0.718\n",
      "Training progress in epoch #65, step 191, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #65, step 192, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #65, step 193, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #65, step 194, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #65, step 195, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #65, step 196, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #65, step 197, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #65, step 198, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #65, step 199, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #65, step 200, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #65, step 201, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #65, step 202, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #65, step 203, discriminator loss=0.692 , generator loss=0.674\n",
      "Training progress in epoch #65, step 204, discriminator loss=0.684 , generator loss=0.692\n",
      "Training progress in epoch #65, step 205, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #65, step 206, discriminator loss=0.685 , generator loss=0.728\n",
      "Training progress in epoch #65, step 207, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #65, step 208, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #65, step 209, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #65, step 210, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #65, step 211, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #65, step 212, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #65, step 213, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #65, step 214, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #65, step 215, discriminator loss=0.685 , generator loss=0.709\n",
      "Training progress in epoch #65, step 216, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #65, step 217, discriminator loss=0.694 , generator loss=0.735\n",
      "Training progress in epoch #65, step 218, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #65, step 219, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #65, step 220, discriminator loss=0.697 , generator loss=0.689\n",
      "Training progress in epoch #65, step 221, discriminator loss=0.698 , generator loss=0.697\n",
      "Training progress in epoch #65, step 222, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #65, step 223, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #65, step 224, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #65, step 225, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #65, step 226, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #65, step 227, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #65, step 228, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #65, step 229, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #65, step 230, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #65, step 231, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #65, step 232, discriminator loss=0.684 , generator loss=0.686\n",
      "Training progress in epoch #65, step 233, discriminator loss=0.687 , generator loss=0.693\n",
      "Disciminator Accuracy on real images: 75%, on fake images: 45%\n",
      "Training progress in epoch #66, step 0, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #66, step 1, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #66, step 2, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #66, step 3, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #66, step 4, discriminator loss=0.699 , generator loss=0.712\n",
      "Training progress in epoch #66, step 5, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #66, step 6, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #66, step 7, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #66, step 8, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #66, step 9, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #66, step 10, discriminator loss=0.689 , generator loss=0.691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #66, step 11, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #66, step 12, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #66, step 13, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #66, step 14, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #66, step 15, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #66, step 16, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #66, step 17, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #66, step 18, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #66, step 19, discriminator loss=0.689 , generator loss=0.674\n",
      "Training progress in epoch #66, step 20, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #66, step 21, discriminator loss=0.687 , generator loss=0.720\n",
      "Training progress in epoch #66, step 22, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #66, step 23, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #66, step 24, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #66, step 25, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #66, step 26, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #66, step 27, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #66, step 28, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #66, step 29, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #66, step 30, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #66, step 31, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #66, step 32, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #66, step 33, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #66, step 34, discriminator loss=0.685 , generator loss=0.710\n",
      "Training progress in epoch #66, step 35, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #66, step 36, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #66, step 37, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #66, step 38, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #66, step 39, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #66, step 40, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #66, step 41, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #66, step 42, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #66, step 43, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #66, step 44, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #66, step 45, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #66, step 46, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #66, step 47, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #66, step 48, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #66, step 49, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #66, step 50, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #66, step 51, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #66, step 52, discriminator loss=0.690 , generator loss=0.669\n",
      "Training progress in epoch #66, step 53, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #66, step 54, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #66, step 55, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #66, step 56, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #66, step 57, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #66, step 58, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #66, step 59, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #66, step 60, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #66, step 61, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #66, step 62, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #66, step 63, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #66, step 64, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #66, step 65, discriminator loss=0.700 , generator loss=0.697\n",
      "Training progress in epoch #66, step 66, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #66, step 67, discriminator loss=0.686 , generator loss=0.681\n",
      "Training progress in epoch #66, step 68, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #66, step 69, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #66, step 70, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #66, step 71, discriminator loss=0.694 , generator loss=0.731\n",
      "Training progress in epoch #66, step 72, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #66, step 73, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #66, step 74, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #66, step 75, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #66, step 76, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #66, step 77, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #66, step 78, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #66, step 79, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #66, step 80, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #66, step 81, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #66, step 82, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #66, step 83, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #66, step 84, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #66, step 85, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #66, step 86, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #66, step 87, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #66, step 88, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #66, step 89, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #66, step 90, discriminator loss=0.697 , generator loss=0.700\n",
      "Training progress in epoch #66, step 91, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #66, step 92, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #66, step 93, discriminator loss=0.693 , generator loss=0.664\n",
      "Training progress in epoch #66, step 94, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #66, step 95, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #66, step 96, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #66, step 97, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #66, step 98, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #66, step 99, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #66, step 100, discriminator loss=0.688 , generator loss=0.725\n",
      "Training progress in epoch #66, step 101, discriminator loss=0.690 , generator loss=0.727\n",
      "Training progress in epoch #66, step 102, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #66, step 103, discriminator loss=0.691 , generator loss=0.679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #66, step 104, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #66, step 105, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #66, step 106, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #66, step 107, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #66, step 108, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #66, step 109, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #66, step 110, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #66, step 111, discriminator loss=0.685 , generator loss=0.691\n",
      "Training progress in epoch #66, step 112, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #66, step 113, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #66, step 114, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #66, step 115, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #66, step 116, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #66, step 117, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #66, step 118, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #66, step 119, discriminator loss=0.684 , generator loss=0.692\n",
      "Training progress in epoch #66, step 120, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #66, step 121, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #66, step 122, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #66, step 123, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #66, step 124, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #66, step 125, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #66, step 126, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #66, step 127, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #66, step 128, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #66, step 129, discriminator loss=0.699 , generator loss=0.709\n",
      "Training progress in epoch #66, step 130, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #66, step 131, discriminator loss=0.696 , generator loss=0.684\n",
      "Training progress in epoch #66, step 132, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #66, step 133, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #66, step 134, discriminator loss=0.697 , generator loss=0.729\n",
      "Training progress in epoch #66, step 135, discriminator loss=0.689 , generator loss=0.742\n",
      "Training progress in epoch #66, step 136, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #66, step 137, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #66, step 138, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #66, step 139, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #66, step 140, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #66, step 141, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #66, step 142, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #66, step 143, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #66, step 144, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #66, step 145, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #66, step 146, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #66, step 147, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #66, step 148, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #66, step 149, discriminator loss=0.696 , generator loss=0.670\n",
      "Training progress in epoch #66, step 150, discriminator loss=0.699 , generator loss=0.674\n",
      "Training progress in epoch #66, step 151, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #66, step 152, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #66, step 153, discriminator loss=0.691 , generator loss=0.740\n",
      "Training progress in epoch #66, step 154, discriminator loss=0.686 , generator loss=0.739\n",
      "Training progress in epoch #66, step 155, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #66, step 156, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #66, step 157, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #66, step 158, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #66, step 159, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #66, step 160, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #66, step 161, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #66, step 162, discriminator loss=0.691 , generator loss=0.727\n",
      "Training progress in epoch #66, step 163, discriminator loss=0.693 , generator loss=0.731\n",
      "Training progress in epoch #66, step 164, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #66, step 165, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #66, step 166, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #66, step 167, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #66, step 168, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #66, step 169, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #66, step 170, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #66, step 171, discriminator loss=0.689 , generator loss=0.678\n",
      "Training progress in epoch #66, step 172, discriminator loss=0.690 , generator loss=0.679\n",
      "Training progress in epoch #66, step 173, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #66, step 174, discriminator loss=0.696 , generator loss=0.725\n",
      "Training progress in epoch #66, step 175, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #66, step 176, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #66, step 177, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #66, step 178, discriminator loss=0.682 , generator loss=0.708\n",
      "Training progress in epoch #66, step 179, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #66, step 180, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #66, step 181, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #66, step 182, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #66, step 183, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #66, step 184, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #66, step 185, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #66, step 186, discriminator loss=0.700 , generator loss=0.713\n",
      "Training progress in epoch #66, step 187, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #66, step 188, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #66, step 189, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #66, step 190, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #66, step 191, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #66, step 192, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #66, step 193, discriminator loss=0.691 , generator loss=0.733\n",
      "Training progress in epoch #66, step 194, discriminator loss=0.684 , generator loss=0.732\n",
      "Training progress in epoch #66, step 195, discriminator loss=0.691 , generator loss=0.718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #66, step 196, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #66, step 197, discriminator loss=0.695 , generator loss=0.671\n",
      "Training progress in epoch #66, step 198, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #66, step 199, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #66, step 200, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #66, step 201, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #66, step 202, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #66, step 203, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #66, step 204, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #66, step 205, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #66, step 206, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #66, step 207, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #66, step 208, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #66, step 209, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #66, step 210, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #66, step 211, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #66, step 212, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #66, step 213, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #66, step 214, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #66, step 215, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #66, step 216, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #66, step 217, discriminator loss=0.691 , generator loss=0.677\n",
      "Training progress in epoch #66, step 218, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #66, step 219, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #66, step 220, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #66, step 221, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #66, step 222, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #66, step 223, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #66, step 224, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #66, step 225, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #66, step 226, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #66, step 227, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #66, step 228, discriminator loss=0.697 , generator loss=0.672\n",
      "Training progress in epoch #66, step 229, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #66, step 230, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #66, step 231, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #66, step 232, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #66, step 233, discriminator loss=0.686 , generator loss=0.707\n",
      "Disciminator Accuracy on real images: 43%, on fake images: 82%\n",
      "Training progress in epoch #67, step 0, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #67, step 1, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #67, step 2, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #67, step 3, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #67, step 4, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #67, step 5, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #67, step 6, discriminator loss=0.695 , generator loss=0.675\n",
      "Training progress in epoch #67, step 7, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #67, step 8, discriminator loss=0.694 , generator loss=0.736\n",
      "Training progress in epoch #67, step 9, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #67, step 10, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #67, step 11, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #67, step 12, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #67, step 13, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #67, step 14, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #67, step 15, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #67, step 16, discriminator loss=0.694 , generator loss=0.675\n",
      "Training progress in epoch #67, step 17, discriminator loss=0.694 , generator loss=0.680\n",
      "Training progress in epoch #67, step 18, discriminator loss=0.697 , generator loss=0.707\n",
      "Training progress in epoch #67, step 19, discriminator loss=0.691 , generator loss=0.731\n",
      "Training progress in epoch #67, step 20, discriminator loss=0.686 , generator loss=0.724\n",
      "Training progress in epoch #67, step 21, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #67, step 22, discriminator loss=0.686 , generator loss=0.721\n",
      "Training progress in epoch #67, step 23, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #67, step 24, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #67, step 25, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #67, step 26, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #67, step 27, discriminator loss=0.680 , generator loss=0.705\n",
      "Training progress in epoch #67, step 28, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #67, step 29, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #67, step 30, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #67, step 31, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #67, step 32, discriminator loss=0.697 , generator loss=0.668\n",
      "Training progress in epoch #67, step 33, discriminator loss=0.696 , generator loss=0.677\n",
      "Training progress in epoch #67, step 34, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #67, step 35, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #67, step 36, discriminator loss=0.700 , generator loss=0.723\n",
      "Training progress in epoch #67, step 37, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #67, step 38, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #67, step 39, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #67, step 40, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #67, step 41, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #67, step 42, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #67, step 43, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #67, step 44, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #67, step 45, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #67, step 46, discriminator loss=0.697 , generator loss=0.699\n",
      "Training progress in epoch #67, step 47, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #67, step 48, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #67, step 49, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #67, step 50, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #67, step 51, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #67, step 52, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #67, step 53, discriminator loss=0.694 , generator loss=0.702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #67, step 54, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #67, step 55, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #67, step 56, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #67, step 57, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #67, step 58, discriminator loss=0.694 , generator loss=0.674\n",
      "Training progress in epoch #67, step 59, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #67, step 60, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #67, step 61, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #67, step 62, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #67, step 63, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #67, step 64, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #67, step 65, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #67, step 66, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #67, step 67, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #67, step 68, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #67, step 69, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #67, step 70, discriminator loss=0.696 , generator loss=0.695\n",
      "Training progress in epoch #67, step 71, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #67, step 72, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #67, step 73, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #67, step 74, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #67, step 75, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #67, step 76, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #67, step 77, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #67, step 78, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #67, step 79, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #67, step 80, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #67, step 81, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #67, step 82, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #67, step 83, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #67, step 84, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #67, step 85, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #67, step 86, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #67, step 87, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #67, step 88, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #67, step 89, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #67, step 90, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #67, step 91, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #67, step 92, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #67, step 93, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #67, step 94, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #67, step 95, discriminator loss=0.698 , generator loss=0.702\n",
      "Training progress in epoch #67, step 96, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #67, step 97, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #67, step 98, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #67, step 99, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #67, step 100, discriminator loss=0.685 , generator loss=0.725\n",
      "Training progress in epoch #67, step 101, discriminator loss=0.686 , generator loss=0.725\n",
      "Training progress in epoch #67, step 102, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #67, step 103, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #67, step 104, discriminator loss=0.687 , generator loss=0.668\n",
      "Training progress in epoch #67, step 105, discriminator loss=0.688 , generator loss=0.680\n",
      "Training progress in epoch #67, step 106, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #67, step 107, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #67, step 108, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #67, step 109, discriminator loss=0.698 , generator loss=0.713\n",
      "Training progress in epoch #67, step 110, discriminator loss=0.696 , generator loss=0.730\n",
      "Training progress in epoch #67, step 111, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #67, step 112, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #67, step 113, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #67, step 114, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #67, step 115, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #67, step 116, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #67, step 117, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #67, step 118, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #67, step 119, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #67, step 120, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #67, step 121, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #67, step 122, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #67, step 123, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #67, step 124, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #67, step 125, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #67, step 126, discriminator loss=0.699 , generator loss=0.681\n",
      "Training progress in epoch #67, step 127, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #67, step 128, discriminator loss=0.699 , generator loss=0.702\n",
      "Training progress in epoch #67, step 129, discriminator loss=0.700 , generator loss=0.711\n",
      "Training progress in epoch #67, step 130, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #67, step 131, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #67, step 132, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #67, step 133, discriminator loss=0.698 , generator loss=0.705\n",
      "Training progress in epoch #67, step 134, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #67, step 135, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #67, step 136, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #67, step 137, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #67, step 138, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #67, step 139, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #67, step 140, discriminator loss=0.696 , generator loss=0.726\n",
      "Training progress in epoch #67, step 141, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #67, step 142, discriminator loss=0.684 , generator loss=0.724\n",
      "Training progress in epoch #67, step 143, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #67, step 144, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #67, step 145, discriminator loss=0.696 , generator loss=0.693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #67, step 146, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #67, step 147, discriminator loss=0.690 , generator loss=0.674\n",
      "Training progress in epoch #67, step 148, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #67, step 149, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #67, step 150, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #67, step 151, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #67, step 152, discriminator loss=0.700 , generator loss=0.730\n",
      "Training progress in epoch #67, step 153, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #67, step 154, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #67, step 155, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #67, step 156, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #67, step 157, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #67, step 158, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #67, step 159, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #67, step 160, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #67, step 161, discriminator loss=0.684 , generator loss=0.715\n",
      "Training progress in epoch #67, step 162, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #67, step 163, discriminator loss=0.686 , generator loss=0.714\n",
      "Training progress in epoch #67, step 164, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #67, step 165, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #67, step 166, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #67, step 167, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #67, step 168, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #67, step 169, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #67, step 170, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #67, step 171, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #67, step 172, discriminator loss=0.699 , generator loss=0.699\n",
      "Training progress in epoch #67, step 173, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #67, step 174, discriminator loss=0.697 , generator loss=0.690\n",
      "Training progress in epoch #67, step 175, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #67, step 176, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #67, step 177, discriminator loss=0.697 , generator loss=0.693\n",
      "Training progress in epoch #67, step 178, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #67, step 179, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #67, step 180, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #67, step 181, discriminator loss=0.684 , generator loss=0.691\n",
      "Training progress in epoch #67, step 182, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #67, step 183, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #67, step 184, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #67, step 185, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #67, step 186, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #67, step 187, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #67, step 188, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #67, step 189, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #67, step 190, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #67, step 191, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #67, step 192, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #67, step 193, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #67, step 194, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #67, step 195, discriminator loss=0.694 , generator loss=0.721\n",
      "Training progress in epoch #67, step 196, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #67, step 197, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #67, step 198, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #67, step 199, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #67, step 200, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #67, step 201, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #67, step 202, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #67, step 203, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #67, step 204, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #67, step 205, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #67, step 206, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #67, step 207, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #67, step 208, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #67, step 209, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #67, step 210, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #67, step 211, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #67, step 212, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #67, step 213, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #67, step 214, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #67, step 215, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #67, step 216, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #67, step 217, discriminator loss=0.702 , generator loss=0.708\n",
      "Training progress in epoch #67, step 218, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #67, step 219, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #67, step 220, discriminator loss=0.695 , generator loss=0.689\n",
      "Training progress in epoch #67, step 221, discriminator loss=0.689 , generator loss=0.675\n",
      "Training progress in epoch #67, step 222, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #67, step 223, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #67, step 224, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #67, step 225, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #67, step 226, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #67, step 227, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #67, step 228, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #67, step 229, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #67, step 230, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #67, step 231, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #67, step 232, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #67, step 233, discriminator loss=0.694 , generator loss=0.713\n",
      "Disciminator Accuracy on real images: 31%, on fake images: 81%\n",
      "Training progress in epoch #68, step 0, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #68, step 1, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #68, step 2, discriminator loss=0.692 , generator loss=0.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #68, step 3, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #68, step 4, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #68, step 5, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #68, step 6, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #68, step 7, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #68, step 8, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #68, step 9, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #68, step 10, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #68, step 11, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #68, step 12, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #68, step 13, discriminator loss=0.694 , generator loss=0.671\n",
      "Training progress in epoch #68, step 14, discriminator loss=0.691 , generator loss=0.675\n",
      "Training progress in epoch #68, step 15, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #68, step 16, discriminator loss=0.698 , generator loss=0.715\n",
      "Training progress in epoch #68, step 17, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #68, step 18, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #68, step 19, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #68, step 20, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #68, step 21, discriminator loss=0.690 , generator loss=0.672\n",
      "Training progress in epoch #68, step 22, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #68, step 23, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #68, step 24, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #68, step 25, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #68, step 26, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #68, step 27, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #68, step 28, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #68, step 29, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #68, step 30, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #68, step 31, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #68, step 32, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #68, step 33, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #68, step 34, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #68, step 35, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #68, step 36, discriminator loss=0.688 , generator loss=0.666\n",
      "Training progress in epoch #68, step 37, discriminator loss=0.692 , generator loss=0.673\n",
      "Training progress in epoch #68, step 38, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #68, step 39, discriminator loss=0.685 , generator loss=0.718\n",
      "Training progress in epoch #68, step 40, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #68, step 41, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #68, step 42, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #68, step 43, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #68, step 44, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #68, step 45, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #68, step 46, discriminator loss=0.700 , generator loss=0.702\n",
      "Training progress in epoch #68, step 47, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #68, step 48, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #68, step 49, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #68, step 50, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #68, step 51, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #68, step 52, discriminator loss=0.696 , generator loss=0.728\n",
      "Training progress in epoch #68, step 53, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #68, step 54, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #68, step 55, discriminator loss=0.684 , generator loss=0.707\n",
      "Training progress in epoch #68, step 56, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #68, step 57, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #68, step 58, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #68, step 59, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #68, step 60, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #68, step 61, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #68, step 62, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #68, step 63, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #68, step 64, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #68, step 65, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #68, step 66, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #68, step 67, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #68, step 68, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #68, step 69, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #68, step 70, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #68, step 71, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #68, step 72, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #68, step 73, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #68, step 74, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #68, step 75, discriminator loss=0.699 , generator loss=0.705\n",
      "Training progress in epoch #68, step 76, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #68, step 77, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #68, step 78, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #68, step 79, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #68, step 80, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #68, step 81, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #68, step 82, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #68, step 83, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #68, step 84, discriminator loss=0.694 , generator loss=0.732\n",
      "Training progress in epoch #68, step 85, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #68, step 86, discriminator loss=0.697 , generator loss=0.717\n",
      "Training progress in epoch #68, step 87, discriminator loss=0.682 , generator loss=0.695\n",
      "Training progress in epoch #68, step 88, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #68, step 89, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #68, step 90, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #68, step 91, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #68, step 92, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #68, step 93, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #68, step 94, discriminator loss=0.696 , generator loss=0.728\n",
      "Training progress in epoch #68, step 95, discriminator loss=0.697 , generator loss=0.715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #68, step 96, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #68, step 97, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #68, step 98, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #68, step 99, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #68, step 100, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #68, step 101, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #68, step 102, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #68, step 103, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #68, step 104, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #68, step 105, discriminator loss=0.695 , generator loss=0.727\n",
      "Training progress in epoch #68, step 106, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #68, step 107, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #68, step 108, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #68, step 109, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #68, step 110, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #68, step 111, discriminator loss=0.691 , generator loss=0.667\n",
      "Training progress in epoch #68, step 112, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #68, step 113, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #68, step 114, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #68, step 115, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #68, step 116, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #68, step 117, discriminator loss=0.690 , generator loss=0.729\n",
      "Training progress in epoch #68, step 118, discriminator loss=0.699 , generator loss=0.709\n",
      "Training progress in epoch #68, step 119, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #68, step 120, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #68, step 121, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #68, step 122, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #68, step 123, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #68, step 124, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #68, step 125, discriminator loss=0.683 , generator loss=0.692\n",
      "Training progress in epoch #68, step 126, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #68, step 127, discriminator loss=0.692 , generator loss=0.733\n",
      "Training progress in epoch #68, step 128, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #68, step 129, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #68, step 130, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #68, step 131, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #68, step 132, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #68, step 133, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #68, step 134, discriminator loss=0.691 , generator loss=0.665\n",
      "Training progress in epoch #68, step 135, discriminator loss=0.695 , generator loss=0.676\n",
      "Training progress in epoch #68, step 136, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #68, step 137, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #68, step 138, discriminator loss=0.690 , generator loss=0.746\n",
      "Training progress in epoch #68, step 139, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #68, step 140, discriminator loss=0.697 , generator loss=0.702\n",
      "Training progress in epoch #68, step 141, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #68, step 142, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #68, step 143, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #68, step 144, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #68, step 145, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #68, step 146, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #68, step 147, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #68, step 148, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #68, step 149, discriminator loss=0.689 , generator loss=0.742\n",
      "Training progress in epoch #68, step 150, discriminator loss=0.693 , generator loss=0.731\n",
      "Training progress in epoch #68, step 151, discriminator loss=0.698 , generator loss=0.697\n",
      "Training progress in epoch #68, step 152, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #68, step 153, discriminator loss=0.690 , generator loss=0.670\n",
      "Training progress in epoch #68, step 154, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #68, step 155, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #68, step 156, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #68, step 157, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #68, step 158, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #68, step 159, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #68, step 160, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #68, step 161, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #68, step 162, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #68, step 163, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #68, step 164, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #68, step 165, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #68, step 166, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #68, step 167, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #68, step 168, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #68, step 169, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #68, step 170, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #68, step 171, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #68, step 172, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #68, step 173, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #68, step 174, discriminator loss=0.688 , generator loss=0.677\n",
      "Training progress in epoch #68, step 175, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #68, step 176, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #68, step 177, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #68, step 178, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #68, step 179, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #68, step 180, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #68, step 181, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #68, step 182, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #68, step 183, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #68, step 184, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #68, step 185, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #68, step 186, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #68, step 187, discriminator loss=0.686 , generator loss=0.702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #68, step 188, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #68, step 189, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #68, step 190, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #68, step 191, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #68, step 192, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #68, step 193, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #68, step 194, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #68, step 195, discriminator loss=0.700 , generator loss=0.707\n",
      "Training progress in epoch #68, step 196, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #68, step 197, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #68, step 198, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #68, step 199, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #68, step 200, discriminator loss=0.698 , generator loss=0.686\n",
      "Training progress in epoch #68, step 201, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #68, step 202, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #68, step 203, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #68, step 204, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #68, step 205, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #68, step 206, discriminator loss=0.697 , generator loss=0.732\n",
      "Training progress in epoch #68, step 207, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #68, step 208, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #68, step 209, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #68, step 210, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #68, step 211, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #68, step 212, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #68, step 213, discriminator loss=0.696 , generator loss=0.689\n",
      "Training progress in epoch #68, step 214, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #68, step 215, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #68, step 216, discriminator loss=0.695 , generator loss=0.731\n",
      "Training progress in epoch #68, step 217, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #68, step 218, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #68, step 219, discriminator loss=0.698 , generator loss=0.689\n",
      "Training progress in epoch #68, step 220, discriminator loss=0.698 , generator loss=0.683\n",
      "Training progress in epoch #68, step 221, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #68, step 222, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #68, step 223, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #68, step 224, discriminator loss=0.696 , generator loss=0.723\n",
      "Training progress in epoch #68, step 225, discriminator loss=0.692 , generator loss=0.732\n",
      "Training progress in epoch #68, step 226, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #68, step 227, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #68, step 228, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #68, step 229, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #68, step 230, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #68, step 231, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #68, step 232, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #68, step 233, discriminator loss=0.690 , generator loss=0.700\n",
      "Disciminator Accuracy on real images: 56%, on fake images: 67%\n",
      "Training progress in epoch #69, step 0, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #69, step 1, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #69, step 2, discriminator loss=0.687 , generator loss=0.725\n",
      "Training progress in epoch #69, step 3, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #69, step 4, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #69, step 5, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #69, step 6, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #69, step 7, discriminator loss=0.696 , generator loss=0.680\n",
      "Training progress in epoch #69, step 8, discriminator loss=0.698 , generator loss=0.691\n",
      "Training progress in epoch #69, step 9, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #69, step 10, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #69, step 11, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #69, step 12, discriminator loss=0.701 , generator loss=0.705\n",
      "Training progress in epoch #69, step 13, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #69, step 14, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #69, step 15, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #69, step 16, discriminator loss=0.696 , generator loss=0.684\n",
      "Training progress in epoch #69, step 17, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #69, step 18, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #69, step 19, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #69, step 20, discriminator loss=0.696 , generator loss=0.721\n",
      "Training progress in epoch #69, step 21, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #69, step 22, discriminator loss=0.684 , generator loss=0.695\n",
      "Training progress in epoch #69, step 23, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #69, step 24, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #69, step 25, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #69, step 26, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #69, step 27, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #69, step 28, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #69, step 29, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #69, step 30, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #69, step 31, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #69, step 32, discriminator loss=0.697 , generator loss=0.698\n",
      "Training progress in epoch #69, step 33, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #69, step 34, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #69, step 35, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #69, step 36, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #69, step 37, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #69, step 38, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #69, step 39, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #69, step 40, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #69, step 41, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #69, step 42, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #69, step 43, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #69, step 44, discriminator loss=0.688 , generator loss=0.694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #69, step 45, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #69, step 46, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #69, step 47, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #69, step 48, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #69, step 49, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #69, step 50, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #69, step 51, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #69, step 52, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #69, step 53, discriminator loss=0.686 , generator loss=0.683\n",
      "Training progress in epoch #69, step 54, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #69, step 55, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #69, step 56, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #69, step 57, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #69, step 58, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #69, step 59, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #69, step 60, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #69, step 61, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #69, step 62, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #69, step 63, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #69, step 64, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #69, step 65, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #69, step 66, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #69, step 67, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #69, step 68, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #69, step 69, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #69, step 70, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #69, step 71, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #69, step 72, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #69, step 73, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #69, step 74, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #69, step 75, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #69, step 76, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #69, step 77, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #69, step 78, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #69, step 79, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #69, step 80, discriminator loss=0.688 , generator loss=0.678\n",
      "Training progress in epoch #69, step 81, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #69, step 82, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #69, step 83, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #69, step 84, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #69, step 85, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #69, step 86, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #69, step 87, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #69, step 88, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #69, step 89, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #69, step 90, discriminator loss=0.697 , generator loss=0.692\n",
      "Training progress in epoch #69, step 91, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #69, step 92, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #69, step 93, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #69, step 94, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #69, step 95, discriminator loss=0.688 , generator loss=0.670\n",
      "Training progress in epoch #69, step 96, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #69, step 97, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #69, step 98, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #69, step 99, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #69, step 100, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #69, step 101, discriminator loss=0.696 , generator loss=0.686\n",
      "Training progress in epoch #69, step 102, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #69, step 103, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #69, step 104, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #69, step 105, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #69, step 106, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #69, step 107, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #69, step 108, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #69, step 109, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #69, step 110, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #69, step 111, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #69, step 112, discriminator loss=0.685 , generator loss=0.700\n",
      "Training progress in epoch #69, step 113, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #69, step 114, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #69, step 115, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #69, step 116, discriminator loss=0.698 , generator loss=0.714\n",
      "Training progress in epoch #69, step 117, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #69, step 118, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #69, step 119, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #69, step 120, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #69, step 121, discriminator loss=0.697 , generator loss=0.696\n",
      "Training progress in epoch #69, step 122, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #69, step 123, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #69, step 124, discriminator loss=0.683 , generator loss=0.692\n",
      "Training progress in epoch #69, step 125, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #69, step 126, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #69, step 127, discriminator loss=0.683 , generator loss=0.684\n",
      "Training progress in epoch #69, step 128, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #69, step 129, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #69, step 130, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #69, step 131, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #69, step 132, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #69, step 133, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #69, step 134, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #69, step 135, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #69, step 136, discriminator loss=0.692 , generator loss=0.699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #69, step 137, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #69, step 138, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #69, step 139, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #69, step 140, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #69, step 141, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #69, step 142, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #69, step 143, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #69, step 144, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #69, step 145, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #69, step 146, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #69, step 147, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #69, step 148, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #69, step 149, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #69, step 150, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #69, step 151, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #69, step 152, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #69, step 153, discriminator loss=0.697 , generator loss=0.707\n",
      "Training progress in epoch #69, step 154, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #69, step 155, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #69, step 156, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #69, step 157, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #69, step 158, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #69, step 159, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #69, step 160, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #69, step 161, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #69, step 162, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #69, step 163, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #69, step 164, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #69, step 165, discriminator loss=0.700 , generator loss=0.715\n",
      "Training progress in epoch #69, step 166, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #69, step 167, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #69, step 168, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #69, step 169, discriminator loss=0.683 , generator loss=0.706\n",
      "Training progress in epoch #69, step 170, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #69, step 171, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #69, step 172, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #69, step 173, discriminator loss=0.685 , generator loss=0.681\n",
      "Training progress in epoch #69, step 174, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #69, step 175, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #69, step 176, discriminator loss=0.696 , generator loss=0.722\n",
      "Training progress in epoch #69, step 177, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #69, step 178, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #69, step 179, discriminator loss=0.687 , generator loss=0.725\n",
      "Training progress in epoch #69, step 180, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #69, step 181, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #69, step 182, discriminator loss=0.693 , generator loss=0.672\n",
      "Training progress in epoch #69, step 183, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #69, step 184, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #69, step 185, discriminator loss=0.684 , generator loss=0.716\n",
      "Training progress in epoch #69, step 186, discriminator loss=0.693 , generator loss=0.732\n",
      "Training progress in epoch #69, step 187, discriminator loss=0.695 , generator loss=0.730\n",
      "Training progress in epoch #69, step 188, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #69, step 189, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #69, step 190, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #69, step 191, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #69, step 192, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #69, step 193, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #69, step 194, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #69, step 195, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #69, step 196, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #69, step 197, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #69, step 198, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #69, step 199, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #69, step 200, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #69, step 201, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #69, step 202, discriminator loss=0.683 , generator loss=0.696\n",
      "Training progress in epoch #69, step 203, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #69, step 204, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #69, step 205, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #69, step 206, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #69, step 207, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #69, step 208, discriminator loss=0.698 , generator loss=0.704\n",
      "Training progress in epoch #69, step 209, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #69, step 210, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #69, step 211, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #69, step 212, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #69, step 213, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #69, step 214, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #69, step 215, discriminator loss=0.688 , generator loss=0.725\n",
      "Training progress in epoch #69, step 216, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #69, step 217, discriminator loss=0.696 , generator loss=0.674\n",
      "Training progress in epoch #69, step 218, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #69, step 219, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #69, step 220, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #69, step 221, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #69, step 222, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #69, step 223, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #69, step 224, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #69, step 225, discriminator loss=0.692 , generator loss=0.743\n",
      "Training progress in epoch #69, step 226, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #69, step 227, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #69, step 228, discriminator loss=0.692 , generator loss=0.681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #69, step 229, discriminator loss=0.696 , generator loss=0.680\n",
      "Training progress in epoch #69, step 230, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #69, step 231, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #69, step 232, discriminator loss=0.694 , generator loss=0.729\n",
      "Training progress in epoch #69, step 233, discriminator loss=0.690 , generator loss=0.729\n",
      "Disciminator Accuracy on real images: 27%, on fake images: 97%\n",
      "Training progress in epoch #70, step 0, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #70, step 1, discriminator loss=0.699 , generator loss=0.683\n",
      "Training progress in epoch #70, step 2, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #70, step 3, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #70, step 4, discriminator loss=0.685 , generator loss=0.691\n",
      "Training progress in epoch #70, step 5, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #70, step 6, discriminator loss=0.683 , generator loss=0.716\n",
      "Training progress in epoch #70, step 7, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #70, step 8, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #70, step 9, discriminator loss=0.697 , generator loss=0.696\n",
      "Training progress in epoch #70, step 10, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #70, step 11, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #70, step 12, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #70, step 13, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #70, step 14, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #70, step 15, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #70, step 16, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #70, step 17, discriminator loss=0.685 , generator loss=0.709\n",
      "Training progress in epoch #70, step 18, discriminator loss=0.698 , generator loss=0.695\n",
      "Training progress in epoch #70, step 19, discriminator loss=0.687 , generator loss=0.682\n",
      "Training progress in epoch #70, step 20, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #70, step 21, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #70, step 22, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #70, step 23, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #70, step 24, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #70, step 25, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #70, step 26, discriminator loss=0.695 , generator loss=0.726\n",
      "Training progress in epoch #70, step 27, discriminator loss=0.696 , generator loss=0.727\n",
      "Training progress in epoch #70, step 28, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #70, step 29, discriminator loss=0.698 , generator loss=0.705\n",
      "Training progress in epoch #70, step 30, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #70, step 31, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #70, step 32, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #70, step 33, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #70, step 34, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #70, step 35, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #70, step 36, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #70, step 37, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #70, step 38, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #70, step 39, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #70, step 40, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #70, step 41, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #70, step 42, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #70, step 43, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #70, step 44, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #70, step 45, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #70, step 46, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #70, step 47, discriminator loss=0.696 , generator loss=0.694\n",
      "Training progress in epoch #70, step 48, discriminator loss=0.697 , generator loss=0.699\n",
      "Training progress in epoch #70, step 49, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #70, step 50, discriminator loss=0.686 , generator loss=0.685\n",
      "Training progress in epoch #70, step 51, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #70, step 52, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #70, step 53, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #70, step 54, discriminator loss=0.682 , generator loss=0.714\n",
      "Training progress in epoch #70, step 55, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #70, step 56, discriminator loss=0.684 , generator loss=0.713\n",
      "Training progress in epoch #70, step 57, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #70, step 58, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #70, step 59, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #70, step 60, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #70, step 61, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #70, step 62, discriminator loss=0.695 , generator loss=0.677\n",
      "Training progress in epoch #70, step 63, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #70, step 64, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #70, step 65, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #70, step 66, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #70, step 67, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #70, step 68, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #70, step 69, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #70, step 70, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #70, step 71, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #70, step 72, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #70, step 73, discriminator loss=0.695 , generator loss=0.726\n",
      "Training progress in epoch #70, step 74, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #70, step 75, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #70, step 76, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #70, step 77, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #70, step 78, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #70, step 79, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #70, step 80, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #70, step 81, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #70, step 82, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #70, step 83, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #70, step 84, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #70, step 85, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #70, step 86, discriminator loss=0.687 , generator loss=0.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #70, step 87, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #70, step 88, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #70, step 89, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #70, step 90, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #70, step 91, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #70, step 92, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #70, step 93, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #70, step 94, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #70, step 95, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #70, step 96, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #70, step 97, discriminator loss=0.697 , generator loss=0.692\n",
      "Training progress in epoch #70, step 98, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #70, step 99, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #70, step 100, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #70, step 101, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #70, step 102, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #70, step 103, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #70, step 104, discriminator loss=0.689 , generator loss=0.728\n",
      "Training progress in epoch #70, step 105, discriminator loss=0.692 , generator loss=0.728\n",
      "Training progress in epoch #70, step 106, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #70, step 107, discriminator loss=0.685 , generator loss=0.691\n",
      "Training progress in epoch #70, step 108, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #70, step 109, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #70, step 110, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #70, step 111, discriminator loss=0.685 , generator loss=0.694\n",
      "Training progress in epoch #70, step 112, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #70, step 113, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #70, step 114, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #70, step 115, discriminator loss=0.695 , generator loss=0.725\n",
      "Training progress in epoch #70, step 116, discriminator loss=0.686 , generator loss=0.709\n",
      "Training progress in epoch #70, step 117, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #70, step 118, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #70, step 119, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #70, step 120, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #70, step 121, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #70, step 122, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #70, step 123, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #70, step 124, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #70, step 125, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #70, step 126, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #70, step 127, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #70, step 128, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #70, step 129, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #70, step 130, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #70, step 131, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #70, step 132, discriminator loss=0.697 , generator loss=0.710\n",
      "Training progress in epoch #70, step 133, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #70, step 134, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #70, step 135, discriminator loss=0.692 , generator loss=0.676\n",
      "Training progress in epoch #70, step 136, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #70, step 137, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #70, step 138, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #70, step 139, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #70, step 140, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #70, step 141, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #70, step 142, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #70, step 143, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #70, step 144, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #70, step 145, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #70, step 146, discriminator loss=0.697 , generator loss=0.699\n",
      "Training progress in epoch #70, step 147, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #70, step 148, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #70, step 149, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #70, step 150, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #70, step 151, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #70, step 152, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #70, step 153, discriminator loss=0.690 , generator loss=0.673\n",
      "Training progress in epoch #70, step 154, discriminator loss=0.688 , generator loss=0.672\n",
      "Training progress in epoch #70, step 155, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #70, step 156, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #70, step 157, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #70, step 158, discriminator loss=0.689 , generator loss=0.735\n",
      "Training progress in epoch #70, step 159, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #70, step 160, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #70, step 161, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #70, step 162, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #70, step 163, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #70, step 164, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #70, step 165, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #70, step 166, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #70, step 167, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #70, step 168, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #70, step 169, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #70, step 170, discriminator loss=0.683 , generator loss=0.718\n",
      "Training progress in epoch #70, step 171, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #70, step 172, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #70, step 173, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #70, step 174, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #70, step 175, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #70, step 176, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #70, step 177, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #70, step 178, discriminator loss=0.690 , generator loss=0.699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #70, step 179, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #70, step 180, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #70, step 181, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #70, step 182, discriminator loss=0.697 , generator loss=0.708\n",
      "Training progress in epoch #70, step 183, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #70, step 184, discriminator loss=0.687 , generator loss=0.674\n",
      "Training progress in epoch #70, step 185, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #70, step 186, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #70, step 187, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #70, step 188, discriminator loss=0.698 , generator loss=0.706\n",
      "Training progress in epoch #70, step 189, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #70, step 190, discriminator loss=0.697 , generator loss=0.702\n",
      "Training progress in epoch #70, step 191, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #70, step 192, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #70, step 193, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #70, step 194, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #70, step 195, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #70, step 196, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #70, step 197, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #70, step 198, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #70, step 199, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #70, step 200, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #70, step 201, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #70, step 202, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #70, step 203, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #70, step 204, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #70, step 205, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #70, step 206, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #70, step 207, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #70, step 208, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #70, step 209, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #70, step 210, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #70, step 211, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #70, step 212, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #70, step 213, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #70, step 214, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #70, step 215, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #70, step 216, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #70, step 217, discriminator loss=0.692 , generator loss=0.733\n",
      "Training progress in epoch #70, step 218, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #70, step 219, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #70, step 220, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #70, step 221, discriminator loss=0.685 , generator loss=0.676\n",
      "Training progress in epoch #70, step 222, discriminator loss=0.686 , generator loss=0.679\n",
      "Training progress in epoch #70, step 223, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #70, step 224, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #70, step 225, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #70, step 226, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #70, step 227, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #70, step 228, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #70, step 229, discriminator loss=0.699 , generator loss=0.686\n",
      "Training progress in epoch #70, step 230, discriminator loss=0.684 , generator loss=0.696\n",
      "Training progress in epoch #70, step 231, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #70, step 232, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #70, step 233, discriminator loss=0.691 , generator loss=0.702\n",
      "Disciminator Accuracy on real images: 48%, on fake images: 66%\n",
      "Training progress in epoch #71, step 0, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #71, step 1, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #71, step 2, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #71, step 3, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #71, step 4, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #71, step 5, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #71, step 6, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #71, step 7, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #71, step 8, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #71, step 9, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #71, step 10, discriminator loss=0.694 , generator loss=0.677\n",
      "Training progress in epoch #71, step 11, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #71, step 12, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #71, step 13, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #71, step 14, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #71, step 15, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #71, step 16, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #71, step 17, discriminator loss=0.685 , generator loss=0.680\n",
      "Training progress in epoch #71, step 18, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #71, step 19, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #71, step 20, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #71, step 21, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #71, step 22, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #71, step 23, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #71, step 24, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #71, step 25, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #71, step 26, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #71, step 27, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #71, step 28, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #71, step 29, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #71, step 30, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #71, step 31, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #71, step 32, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #71, step 33, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #71, step 34, discriminator loss=0.694 , generator loss=0.721\n",
      "Training progress in epoch #71, step 35, discriminator loss=0.687 , generator loss=0.697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #71, step 36, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #71, step 37, discriminator loss=0.691 , generator loss=0.675\n",
      "Training progress in epoch #71, step 38, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #71, step 39, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #71, step 40, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #71, step 41, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #71, step 42, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #71, step 43, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #71, step 44, discriminator loss=0.697 , generator loss=0.724\n",
      "Training progress in epoch #71, step 45, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #71, step 46, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #71, step 47, discriminator loss=0.686 , generator loss=0.676\n",
      "Training progress in epoch #71, step 48, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #71, step 49, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #71, step 50, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #71, step 51, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #71, step 52, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #71, step 53, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #71, step 54, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #71, step 55, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #71, step 56, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #71, step 57, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #71, step 58, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #71, step 59, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #71, step 60, discriminator loss=0.683 , generator loss=0.691\n",
      "Training progress in epoch #71, step 61, discriminator loss=0.688 , generator loss=0.670\n",
      "Training progress in epoch #71, step 62, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #71, step 63, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #71, step 64, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #71, step 65, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #71, step 66, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #71, step 67, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #71, step 68, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #71, step 69, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #71, step 70, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #71, step 71, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #71, step 72, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #71, step 73, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #71, step 74, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #71, step 75, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #71, step 76, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #71, step 77, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #71, step 78, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #71, step 79, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #71, step 80, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #71, step 81, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #71, step 82, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #71, step 83, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #71, step 84, discriminator loss=0.683 , generator loss=0.698\n",
      "Training progress in epoch #71, step 85, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #71, step 86, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #71, step 87, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #71, step 88, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #71, step 89, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #71, step 90, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #71, step 91, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #71, step 92, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #71, step 93, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #71, step 94, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #71, step 95, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #71, step 96, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #71, step 97, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #71, step 98, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #71, step 99, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #71, step 100, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #71, step 101, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #71, step 102, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #71, step 103, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #71, step 104, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #71, step 105, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #71, step 106, discriminator loss=0.699 , generator loss=0.687\n",
      "Training progress in epoch #71, step 107, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #71, step 108, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #71, step 109, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #71, step 110, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #71, step 111, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #71, step 112, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #71, step 113, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #71, step 114, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #71, step 115, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #71, step 116, discriminator loss=0.684 , generator loss=0.700\n",
      "Training progress in epoch #71, step 117, discriminator loss=0.698 , generator loss=0.710\n",
      "Training progress in epoch #71, step 118, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #71, step 119, discriminator loss=0.697 , generator loss=0.690\n",
      "Training progress in epoch #71, step 120, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #71, step 121, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #71, step 122, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #71, step 123, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #71, step 124, discriminator loss=0.694 , generator loss=0.726\n",
      "Training progress in epoch #71, step 125, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #71, step 126, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #71, step 127, discriminator loss=0.691 , generator loss=0.696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #71, step 128, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #71, step 129, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #71, step 130, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #71, step 131, discriminator loss=0.696 , generator loss=0.677\n",
      "Training progress in epoch #71, step 132, discriminator loss=0.697 , generator loss=0.710\n",
      "Training progress in epoch #71, step 133, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #71, step 134, discriminator loss=0.695 , generator loss=0.741\n",
      "Training progress in epoch #71, step 135, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #71, step 136, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #71, step 137, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #71, step 138, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #71, step 139, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #71, step 140, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #71, step 141, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #71, step 142, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #71, step 143, discriminator loss=0.684 , generator loss=0.703\n",
      "Training progress in epoch #71, step 144, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #71, step 145, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #71, step 146, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #71, step 147, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #71, step 148, discriminator loss=0.690 , generator loss=0.676\n",
      "Training progress in epoch #71, step 149, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #71, step 150, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #71, step 151, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #71, step 152, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #71, step 153, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #71, step 154, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #71, step 155, discriminator loss=0.698 , generator loss=0.723\n",
      "Training progress in epoch #71, step 156, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #71, step 157, discriminator loss=0.689 , generator loss=0.669\n",
      "Training progress in epoch #71, step 158, discriminator loss=0.690 , generator loss=0.672\n",
      "Training progress in epoch #71, step 159, discriminator loss=0.700 , generator loss=0.704\n",
      "Training progress in epoch #71, step 160, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #71, step 161, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #71, step 162, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #71, step 163, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #71, step 164, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #71, step 165, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #71, step 166, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #71, step 167, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #71, step 168, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #71, step 169, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #71, step 170, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #71, step 171, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #71, step 172, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #71, step 173, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #71, step 174, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #71, step 175, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #71, step 176, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #71, step 177, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #71, step 178, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #71, step 179, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #71, step 180, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #71, step 181, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #71, step 182, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #71, step 183, discriminator loss=0.695 , generator loss=0.676\n",
      "Training progress in epoch #71, step 184, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #71, step 185, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #71, step 186, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #71, step 187, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #71, step 188, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #71, step 189, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #71, step 190, discriminator loss=0.683 , generator loss=0.692\n",
      "Training progress in epoch #71, step 191, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #71, step 192, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #71, step 193, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #71, step 194, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #71, step 195, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #71, step 196, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #71, step 197, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #71, step 198, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #71, step 199, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #71, step 200, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #71, step 201, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #71, step 202, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #71, step 203, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #71, step 204, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #71, step 205, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #71, step 206, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #71, step 207, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #71, step 208, discriminator loss=0.681 , generator loss=0.700\n",
      "Training progress in epoch #71, step 209, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #71, step 210, discriminator loss=0.697 , generator loss=0.693\n",
      "Training progress in epoch #71, step 211, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #71, step 212, discriminator loss=0.696 , generator loss=0.716\n",
      "Training progress in epoch #71, step 213, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #71, step 214, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #71, step 215, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #71, step 216, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #71, step 217, discriminator loss=0.684 , generator loss=0.694\n",
      "Training progress in epoch #71, step 218, discriminator loss=0.696 , generator loss=0.686\n",
      "Training progress in epoch #71, step 219, discriminator loss=0.692 , generator loss=0.696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #71, step 220, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #71, step 221, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #71, step 222, discriminator loss=0.698 , generator loss=0.709\n",
      "Training progress in epoch #71, step 223, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #71, step 224, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #71, step 225, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #71, step 226, discriminator loss=0.699 , generator loss=0.696\n",
      "Training progress in epoch #71, step 227, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #71, step 228, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #71, step 229, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #71, step 230, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #71, step 231, discriminator loss=0.697 , generator loss=0.716\n",
      "Training progress in epoch #71, step 232, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #71, step 233, discriminator loss=0.697 , generator loss=0.698\n",
      "Disciminator Accuracy on real images: 50%, on fake images: 76%\n",
      "Training progress in epoch #72, step 0, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #72, step 1, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #72, step 2, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #72, step 3, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #72, step 4, discriminator loss=0.684 , generator loss=0.687\n",
      "Training progress in epoch #72, step 5, discriminator loss=0.690 , generator loss=0.670\n",
      "Training progress in epoch #72, step 6, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #72, step 7, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #72, step 8, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #72, step 9, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #72, step 10, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #72, step 11, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #72, step 12, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #72, step 13, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #72, step 14, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #72, step 15, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #72, step 16, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #72, step 17, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #72, step 18, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #72, step 19, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #72, step 20, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #72, step 21, discriminator loss=0.696 , generator loss=0.713\n",
      "Training progress in epoch #72, step 22, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #72, step 23, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #72, step 24, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #72, step 25, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #72, step 26, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #72, step 27, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #72, step 28, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #72, step 29, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #72, step 30, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #72, step 31, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #72, step 32, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #72, step 33, discriminator loss=0.697 , generator loss=0.707\n",
      "Training progress in epoch #72, step 34, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #72, step 35, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #72, step 36, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #72, step 37, discriminator loss=0.698 , generator loss=0.688\n",
      "Training progress in epoch #72, step 38, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #72, step 39, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #72, step 40, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #72, step 41, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #72, step 42, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #72, step 43, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #72, step 44, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #72, step 45, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #72, step 46, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #72, step 47, discriminator loss=0.694 , generator loss=0.677\n",
      "Training progress in epoch #72, step 48, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #72, step 49, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #72, step 50, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #72, step 51, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #72, step 52, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #72, step 53, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #72, step 54, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #72, step 55, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #72, step 56, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #72, step 57, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #72, step 58, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #72, step 59, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #72, step 60, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #72, step 61, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #72, step 62, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #72, step 63, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #72, step 64, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #72, step 65, discriminator loss=0.685 , generator loss=0.710\n",
      "Training progress in epoch #72, step 66, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #72, step 67, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #72, step 68, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #72, step 69, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #72, step 70, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #72, step 71, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #72, step 72, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #72, step 73, discriminator loss=0.698 , generator loss=0.701\n",
      "Training progress in epoch #72, step 74, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #72, step 75, discriminator loss=0.693 , generator loss=0.733\n",
      "Training progress in epoch #72, step 76, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #72, step 77, discriminator loss=0.698 , generator loss=0.709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #72, step 78, discriminator loss=0.697 , generator loss=0.711\n",
      "Training progress in epoch #72, step 79, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #72, step 80, discriminator loss=0.690 , generator loss=0.672\n",
      "Training progress in epoch #72, step 81, discriminator loss=0.700 , generator loss=0.686\n",
      "Training progress in epoch #72, step 82, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #72, step 83, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #72, step 84, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #72, step 85, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #72, step 86, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #72, step 87, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #72, step 88, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #72, step 89, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #72, step 90, discriminator loss=0.684 , generator loss=0.725\n",
      "Training progress in epoch #72, step 91, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #72, step 92, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #72, step 93, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #72, step 94, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #72, step 95, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #72, step 96, discriminator loss=0.698 , generator loss=0.707\n",
      "Training progress in epoch #72, step 97, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #72, step 98, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #72, step 99, discriminator loss=0.699 , generator loss=0.693\n",
      "Training progress in epoch #72, step 100, discriminator loss=0.702 , generator loss=0.711\n",
      "Training progress in epoch #72, step 101, discriminator loss=0.696 , generator loss=0.728\n",
      "Training progress in epoch #72, step 102, discriminator loss=0.695 , generator loss=0.716\n",
      "Training progress in epoch #72, step 103, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #72, step 104, discriminator loss=0.685 , generator loss=0.687\n",
      "Training progress in epoch #72, step 105, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #72, step 106, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #72, step 107, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #72, step 108, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #72, step 109, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #72, step 110, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #72, step 111, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #72, step 112, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #72, step 113, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #72, step 114, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #72, step 115, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #72, step 116, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #72, step 117, discriminator loss=0.697 , generator loss=0.707\n",
      "Training progress in epoch #72, step 118, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #72, step 119, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #72, step 120, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #72, step 121, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #72, step 122, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #72, step 123, discriminator loss=0.698 , generator loss=0.701\n",
      "Training progress in epoch #72, step 124, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #72, step 125, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #72, step 126, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #72, step 127, discriminator loss=0.696 , generator loss=0.718\n",
      "Training progress in epoch #72, step 128, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #72, step 129, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #72, step 130, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #72, step 131, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #72, step 132, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #72, step 133, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #72, step 134, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #72, step 135, discriminator loss=0.697 , generator loss=0.723\n",
      "Training progress in epoch #72, step 136, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #72, step 137, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #72, step 138, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #72, step 139, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #72, step 140, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #72, step 141, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #72, step 142, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #72, step 143, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #72, step 144, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #72, step 145, discriminator loss=0.694 , generator loss=0.721\n",
      "Training progress in epoch #72, step 146, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #72, step 147, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #72, step 148, discriminator loss=0.694 , generator loss=0.676\n",
      "Training progress in epoch #72, step 149, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #72, step 150, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #72, step 151, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #72, step 152, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #72, step 153, discriminator loss=0.685 , generator loss=0.686\n",
      "Training progress in epoch #72, step 154, discriminator loss=0.693 , generator loss=0.678\n",
      "Training progress in epoch #72, step 155, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #72, step 156, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #72, step 157, discriminator loss=0.696 , generator loss=0.716\n",
      "Training progress in epoch #72, step 158, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #72, step 159, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #72, step 160, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #72, step 161, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #72, step 162, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #72, step 163, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #72, step 164, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #72, step 165, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #72, step 166, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #72, step 167, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #72, step 168, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #72, step 169, discriminator loss=0.691 , generator loss=0.696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #72, step 170, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #72, step 171, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #72, step 172, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #72, step 173, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #72, step 174, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #72, step 175, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #72, step 176, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #72, step 177, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #72, step 178, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #72, step 179, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #72, step 180, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #72, step 181, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #72, step 182, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #72, step 183, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #72, step 184, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #72, step 185, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #72, step 186, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #72, step 187, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #72, step 188, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #72, step 189, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #72, step 190, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #72, step 191, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #72, step 192, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #72, step 193, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #72, step 194, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #72, step 195, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #72, step 196, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #72, step 197, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #72, step 198, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #72, step 199, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #72, step 200, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #72, step 201, discriminator loss=0.684 , generator loss=0.719\n",
      "Training progress in epoch #72, step 202, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #72, step 203, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #72, step 204, discriminator loss=0.696 , generator loss=0.690\n",
      "Training progress in epoch #72, step 205, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #72, step 206, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #72, step 207, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #72, step 208, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #72, step 209, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #72, step 210, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #72, step 211, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #72, step 212, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #72, step 213, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #72, step 214, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #72, step 215, discriminator loss=0.688 , generator loss=0.666\n",
      "Training progress in epoch #72, step 216, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #72, step 217, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #72, step 218, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #72, step 219, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #72, step 220, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #72, step 221, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #72, step 222, discriminator loss=0.688 , generator loss=0.725\n",
      "Training progress in epoch #72, step 223, discriminator loss=0.692 , generator loss=0.728\n",
      "Training progress in epoch #72, step 224, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #72, step 225, discriminator loss=0.694 , generator loss=0.668\n",
      "Training progress in epoch #72, step 226, discriminator loss=0.693 , generator loss=0.669\n",
      "Training progress in epoch #72, step 227, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #72, step 228, discriminator loss=0.687 , generator loss=0.730\n",
      "Training progress in epoch #72, step 229, discriminator loss=0.687 , generator loss=0.732\n",
      "Training progress in epoch #72, step 230, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #72, step 231, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #72, step 232, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #72, step 233, discriminator loss=0.692 , generator loss=0.710\n",
      "Disciminator Accuracy on real images: 40%, on fake images: 78%\n",
      "Training progress in epoch #73, step 0, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #73, step 1, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #73, step 2, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #73, step 3, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #73, step 4, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #73, step 5, discriminator loss=0.697 , generator loss=0.713\n",
      "Training progress in epoch #73, step 6, discriminator loss=0.696 , generator loss=0.732\n",
      "Training progress in epoch #73, step 7, discriminator loss=0.687 , generator loss=0.743\n",
      "Training progress in epoch #73, step 8, discriminator loss=0.694 , generator loss=0.730\n",
      "Training progress in epoch #73, step 9, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #73, step 10, discriminator loss=0.699 , generator loss=0.680\n",
      "Training progress in epoch #73, step 11, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #73, step 12, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #73, step 13, discriminator loss=0.696 , generator loss=0.706\n",
      "Training progress in epoch #73, step 14, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #73, step 15, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #73, step 16, discriminator loss=0.688 , generator loss=0.734\n",
      "Training progress in epoch #73, step 17, discriminator loss=0.693 , generator loss=0.735\n",
      "Training progress in epoch #73, step 18, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #73, step 19, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #73, step 20, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #73, step 21, discriminator loss=0.694 , generator loss=0.673\n",
      "Training progress in epoch #73, step 22, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #73, step 23, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #73, step 24, discriminator loss=0.700 , generator loss=0.721\n",
      "Training progress in epoch #73, step 25, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #73, step 26, discriminator loss=0.692 , generator loss=0.707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #73, step 27, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #73, step 28, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #73, step 29, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #73, step 30, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #73, step 31, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #73, step 32, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #73, step 33, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #73, step 34, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #73, step 35, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #73, step 36, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #73, step 37, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #73, step 38, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #73, step 39, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #73, step 40, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #73, step 41, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #73, step 42, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #73, step 43, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #73, step 44, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #73, step 45, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #73, step 46, discriminator loss=0.698 , generator loss=0.687\n",
      "Training progress in epoch #73, step 47, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #73, step 48, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #73, step 49, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #73, step 50, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #73, step 51, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #73, step 52, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #73, step 53, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #73, step 54, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #73, step 55, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #73, step 56, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #73, step 57, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #73, step 58, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #73, step 59, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #73, step 60, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #73, step 61, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #73, step 62, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #73, step 63, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #73, step 64, discriminator loss=0.696 , generator loss=0.676\n",
      "Training progress in epoch #73, step 65, discriminator loss=0.696 , generator loss=0.680\n",
      "Training progress in epoch #73, step 66, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #73, step 67, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #73, step 68, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #73, step 69, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #73, step 70, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #73, step 71, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #73, step 72, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #73, step 73, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #73, step 74, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #73, step 75, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #73, step 76, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #73, step 77, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #73, step 78, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #73, step 79, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #73, step 80, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #73, step 81, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #73, step 82, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #73, step 83, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #73, step 84, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #73, step 85, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #73, step 86, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #73, step 87, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #73, step 88, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #73, step 89, discriminator loss=0.693 , generator loss=0.678\n",
      "Training progress in epoch #73, step 90, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #73, step 91, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #73, step 92, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #73, step 93, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #73, step 94, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #73, step 95, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #73, step 96, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #73, step 97, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #73, step 98, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #73, step 99, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #73, step 100, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #73, step 101, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #73, step 102, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #73, step 103, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #73, step 104, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #73, step 105, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #73, step 106, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #73, step 107, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #73, step 108, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #73, step 109, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #73, step 110, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #73, step 111, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #73, step 112, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #73, step 113, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #73, step 114, discriminator loss=0.696 , generator loss=0.683\n",
      "Training progress in epoch #73, step 115, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #73, step 116, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #73, step 117, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #73, step 118, discriminator loss=0.691 , generator loss=0.710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #73, step 119, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #73, step 120, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #73, step 121, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #73, step 122, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #73, step 123, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #73, step 124, discriminator loss=0.684 , generator loss=0.722\n",
      "Training progress in epoch #73, step 125, discriminator loss=0.693 , generator loss=0.729\n",
      "Training progress in epoch #73, step 126, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #73, step 127, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #73, step 128, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #73, step 129, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #73, step 130, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #73, step 131, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #73, step 132, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #73, step 133, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #73, step 134, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #73, step 135, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #73, step 136, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #73, step 137, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #73, step 138, discriminator loss=0.695 , generator loss=0.726\n",
      "Training progress in epoch #73, step 139, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #73, step 140, discriminator loss=0.698 , generator loss=0.700\n",
      "Training progress in epoch #73, step 141, discriminator loss=0.689 , generator loss=0.671\n",
      "Training progress in epoch #73, step 142, discriminator loss=0.686 , generator loss=0.679\n",
      "Training progress in epoch #73, step 143, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #73, step 144, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #73, step 145, discriminator loss=0.681 , generator loss=0.711\n",
      "Training progress in epoch #73, step 146, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #73, step 147, discriminator loss=0.685 , generator loss=0.712\n",
      "Training progress in epoch #73, step 148, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #73, step 149, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #73, step 150, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #73, step 151, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #73, step 152, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #73, step 153, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #73, step 154, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #73, step 155, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #73, step 156, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #73, step 157, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #73, step 158, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #73, step 159, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #73, step 160, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #73, step 161, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #73, step 162, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #73, step 163, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #73, step 164, discriminator loss=0.684 , generator loss=0.718\n",
      "Training progress in epoch #73, step 165, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #73, step 166, discriminator loss=0.696 , generator loss=0.683\n",
      "Training progress in epoch #73, step 167, discriminator loss=0.693 , generator loss=0.678\n",
      "Training progress in epoch #73, step 168, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #73, step 169, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #73, step 170, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #73, step 171, discriminator loss=0.696 , generator loss=0.690\n",
      "Training progress in epoch #73, step 172, discriminator loss=0.694 , generator loss=0.673\n",
      "Training progress in epoch #73, step 173, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #73, step 174, discriminator loss=0.697 , generator loss=0.719\n",
      "Training progress in epoch #73, step 175, discriminator loss=0.694 , generator loss=0.728\n",
      "Training progress in epoch #73, step 176, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #73, step 177, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #73, step 178, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #73, step 179, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #73, step 180, discriminator loss=0.696 , generator loss=0.685\n",
      "Training progress in epoch #73, step 181, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #73, step 182, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #73, step 183, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #73, step 184, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #73, step 185, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #73, step 186, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #73, step 187, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #73, step 188, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #73, step 189, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #73, step 190, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #73, step 191, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #73, step 192, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #73, step 193, discriminator loss=0.698 , generator loss=0.705\n",
      "Training progress in epoch #73, step 194, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #73, step 195, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #73, step 196, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #73, step 197, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #73, step 198, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #73, step 199, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #73, step 200, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #73, step 201, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #73, step 202, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #73, step 203, discriminator loss=0.697 , generator loss=0.709\n",
      "Training progress in epoch #73, step 204, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #73, step 205, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #73, step 206, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #73, step 207, discriminator loss=0.690 , generator loss=0.676\n",
      "Training progress in epoch #73, step 208, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #73, step 209, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #73, step 210, discriminator loss=0.690 , generator loss=0.702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #73, step 211, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #73, step 212, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #73, step 213, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #73, step 214, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #73, step 215, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #73, step 216, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #73, step 217, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #73, step 218, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #73, step 219, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #73, step 220, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #73, step 221, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #73, step 222, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #73, step 223, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #73, step 224, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #73, step 225, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #73, step 226, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #73, step 227, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #73, step 228, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #73, step 229, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #73, step 230, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #73, step 231, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #73, step 232, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #73, step 233, discriminator loss=0.693 , generator loss=0.696\n",
      "Disciminator Accuracy on real images: 52%, on fake images: 64%\n",
      "Training progress in epoch #74, step 0, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #74, step 1, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #74, step 2, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #74, step 3, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #74, step 4, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #74, step 5, discriminator loss=0.700 , generator loss=0.689\n",
      "Training progress in epoch #74, step 6, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #74, step 7, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #74, step 8, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #74, step 9, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #74, step 10, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #74, step 11, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #74, step 12, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #74, step 13, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #74, step 14, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #74, step 15, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #74, step 16, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #74, step 17, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #74, step 18, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #74, step 19, discriminator loss=0.685 , generator loss=0.723\n",
      "Training progress in epoch #74, step 20, discriminator loss=0.697 , generator loss=0.721\n",
      "Training progress in epoch #74, step 21, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #74, step 22, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #74, step 23, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #74, step 24, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #74, step 25, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #74, step 26, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #74, step 27, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #74, step 28, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #74, step 29, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #74, step 30, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #74, step 31, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #74, step 32, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #74, step 33, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #74, step 34, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #74, step 35, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #74, step 36, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #74, step 37, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #74, step 38, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #74, step 39, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #74, step 40, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #74, step 41, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #74, step 42, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #74, step 43, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #74, step 44, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #74, step 45, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #74, step 46, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #74, step 47, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #74, step 48, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #74, step 49, discriminator loss=0.689 , generator loss=0.677\n",
      "Training progress in epoch #74, step 50, discriminator loss=0.690 , generator loss=0.678\n",
      "Training progress in epoch #74, step 51, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #74, step 52, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #74, step 53, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #74, step 54, discriminator loss=0.697 , generator loss=0.694\n",
      "Training progress in epoch #74, step 55, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #74, step 56, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #74, step 57, discriminator loss=0.687 , generator loss=0.741\n",
      "Training progress in epoch #74, step 58, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #74, step 59, discriminator loss=0.694 , generator loss=0.673\n",
      "Training progress in epoch #74, step 60, discriminator loss=0.692 , generator loss=0.659\n",
      "Training progress in epoch #74, step 61, discriminator loss=0.687 , generator loss=0.675\n",
      "Training progress in epoch #74, step 62, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #74, step 63, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #74, step 64, discriminator loss=0.693 , generator loss=0.733\n",
      "Training progress in epoch #74, step 65, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #74, step 66, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #74, step 67, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #74, step 68, discriminator loss=0.692 , generator loss=0.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #74, step 69, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #74, step 70, discriminator loss=0.696 , generator loss=0.671\n",
      "Training progress in epoch #74, step 71, discriminator loss=0.690 , generator loss=0.672\n",
      "Training progress in epoch #74, step 72, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #74, step 73, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #74, step 74, discriminator loss=0.696 , generator loss=0.736\n",
      "Training progress in epoch #74, step 75, discriminator loss=0.694 , generator loss=0.746\n",
      "Training progress in epoch #74, step 76, discriminator loss=0.691 , generator loss=0.737\n",
      "Training progress in epoch #74, step 77, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #74, step 78, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #74, step 79, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #74, step 80, discriminator loss=0.697 , generator loss=0.678\n",
      "Training progress in epoch #74, step 81, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #74, step 82, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #74, step 83, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #74, step 84, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #74, step 85, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #74, step 86, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #74, step 87, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #74, step 88, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #74, step 89, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #74, step 90, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #74, step 91, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #74, step 92, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #74, step 93, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #74, step 94, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #74, step 95, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #74, step 96, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #74, step 97, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #74, step 98, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #74, step 99, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #74, step 100, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #74, step 101, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #74, step 102, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #74, step 103, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #74, step 104, discriminator loss=0.696 , generator loss=0.683\n",
      "Training progress in epoch #74, step 105, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #74, step 106, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #74, step 107, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #74, step 108, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #74, step 109, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #74, step 110, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #74, step 111, discriminator loss=0.685 , generator loss=0.687\n",
      "Training progress in epoch #74, step 112, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #74, step 113, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #74, step 114, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #74, step 115, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #74, step 116, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #74, step 117, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #74, step 118, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #74, step 119, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #74, step 120, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #74, step 121, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #74, step 122, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #74, step 123, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #74, step 124, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #74, step 125, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #74, step 126, discriminator loss=0.686 , generator loss=0.680\n",
      "Training progress in epoch #74, step 127, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #74, step 128, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #74, step 129, discriminator loss=0.684 , generator loss=0.716\n",
      "Training progress in epoch #74, step 130, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #74, step 131, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #74, step 132, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #74, step 133, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #74, step 134, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #74, step 135, discriminator loss=0.696 , generator loss=0.682\n",
      "Training progress in epoch #74, step 136, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #74, step 137, discriminator loss=0.697 , generator loss=0.704\n",
      "Training progress in epoch #74, step 138, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #74, step 139, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #74, step 140, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #74, step 141, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #74, step 142, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #74, step 143, discriminator loss=0.697 , generator loss=0.696\n",
      "Training progress in epoch #74, step 144, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #74, step 145, discriminator loss=0.684 , generator loss=0.697\n",
      "Training progress in epoch #74, step 146, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #74, step 147, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #74, step 148, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #74, step 149, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #74, step 150, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #74, step 151, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #74, step 152, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #74, step 153, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #74, step 154, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #74, step 155, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #74, step 156, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #74, step 157, discriminator loss=0.689 , generator loss=0.733\n",
      "Training progress in epoch #74, step 158, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #74, step 159, discriminator loss=0.695 , generator loss=0.676\n",
      "Training progress in epoch #74, step 160, discriminator loss=0.694 , generator loss=0.672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #74, step 161, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #74, step 162, discriminator loss=0.697 , generator loss=0.717\n",
      "Training progress in epoch #74, step 163, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #74, step 164, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #74, step 165, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #74, step 166, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #74, step 167, discriminator loss=0.685 , generator loss=0.718\n",
      "Training progress in epoch #74, step 168, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #74, step 169, discriminator loss=0.689 , generator loss=0.730\n",
      "Training progress in epoch #74, step 170, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #74, step 171, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #74, step 172, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #74, step 173, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #74, step 174, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #74, step 175, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #74, step 176, discriminator loss=0.694 , generator loss=0.740\n",
      "Training progress in epoch #74, step 177, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #74, step 178, discriminator loss=0.694 , generator loss=0.678\n",
      "Training progress in epoch #74, step 179, discriminator loss=0.687 , generator loss=0.677\n",
      "Training progress in epoch #74, step 180, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #74, step 181, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #74, step 182, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #74, step 183, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #74, step 184, discriminator loss=0.696 , generator loss=0.713\n",
      "Training progress in epoch #74, step 185, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #74, step 186, discriminator loss=0.698 , generator loss=0.678\n",
      "Training progress in epoch #74, step 187, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #74, step 188, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #74, step 189, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #74, step 190, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #74, step 191, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #74, step 192, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #74, step 193, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #74, step 194, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #74, step 195, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #74, step 196, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #74, step 197, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #74, step 198, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #74, step 199, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #74, step 200, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #74, step 201, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #74, step 202, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #74, step 203, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #74, step 204, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #74, step 205, discriminator loss=0.684 , generator loss=0.686\n",
      "Training progress in epoch #74, step 206, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #74, step 207, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #74, step 208, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #74, step 209, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #74, step 210, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #74, step 211, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #74, step 212, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #74, step 213, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #74, step 214, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #74, step 215, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #74, step 216, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #74, step 217, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #74, step 218, discriminator loss=0.699 , generator loss=0.695\n",
      "Training progress in epoch #74, step 219, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #74, step 220, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #74, step 221, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #74, step 222, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #74, step 223, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #74, step 224, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #74, step 225, discriminator loss=0.696 , generator loss=0.678\n",
      "Training progress in epoch #74, step 226, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #74, step 227, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #74, step 228, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #74, step 229, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #74, step 230, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #74, step 231, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #74, step 232, discriminator loss=0.699 , generator loss=0.712\n",
      "Training progress in epoch #74, step 233, discriminator loss=0.689 , generator loss=0.720\n",
      "Disciminator Accuracy on real images: 23%, on fake images: 94%\n",
      "Training progress in epoch #75, step 0, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #75, step 1, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #75, step 2, discriminator loss=0.698 , generator loss=0.709\n",
      "Training progress in epoch #75, step 3, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #75, step 4, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #75, step 5, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #75, step 6, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #75, step 7, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #75, step 8, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #75, step 9, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #75, step 10, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #75, step 11, discriminator loss=0.699 , generator loss=0.699\n",
      "Training progress in epoch #75, step 12, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #75, step 13, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #75, step 14, discriminator loss=0.698 , generator loss=0.698\n",
      "Training progress in epoch #75, step 15, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #75, step 16, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #75, step 17, discriminator loss=0.691 , generator loss=0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #75, step 18, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #75, step 19, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #75, step 20, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #75, step 21, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #75, step 22, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #75, step 23, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #75, step 24, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #75, step 25, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #75, step 26, discriminator loss=0.698 , generator loss=0.702\n",
      "Training progress in epoch #75, step 27, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #75, step 28, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #75, step 29, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #75, step 30, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #75, step 31, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #75, step 32, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #75, step 33, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #75, step 34, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #75, step 35, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #75, step 36, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #75, step 37, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #75, step 38, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #75, step 39, discriminator loss=0.683 , generator loss=0.703\n",
      "Training progress in epoch #75, step 40, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #75, step 41, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #75, step 42, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #75, step 43, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #75, step 44, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #75, step 45, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #75, step 46, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #75, step 47, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #75, step 48, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #75, step 49, discriminator loss=0.698 , generator loss=0.708\n",
      "Training progress in epoch #75, step 50, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #75, step 51, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #75, step 52, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #75, step 53, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #75, step 54, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #75, step 55, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #75, step 56, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #75, step 57, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #75, step 58, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #75, step 59, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #75, step 60, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #75, step 61, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #75, step 62, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #75, step 63, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #75, step 64, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #75, step 65, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #75, step 66, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #75, step 67, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #75, step 68, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #75, step 69, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #75, step 70, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #75, step 71, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #75, step 72, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #75, step 73, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #75, step 74, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #75, step 75, discriminator loss=0.692 , generator loss=0.674\n",
      "Training progress in epoch #75, step 76, discriminator loss=0.689 , generator loss=0.672\n",
      "Training progress in epoch #75, step 77, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #75, step 78, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #75, step 79, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #75, step 80, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #75, step 81, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #75, step 82, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #75, step 83, discriminator loss=0.692 , generator loss=0.666\n",
      "Training progress in epoch #75, step 84, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #75, step 85, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #75, step 86, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #75, step 87, discriminator loss=0.693 , generator loss=0.737\n",
      "Training progress in epoch #75, step 88, discriminator loss=0.689 , generator loss=0.727\n",
      "Training progress in epoch #75, step 89, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #75, step 90, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #75, step 91, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #75, step 92, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #75, step 93, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #75, step 94, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #75, step 95, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #75, step 96, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #75, step 97, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #75, step 98, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #75, step 99, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #75, step 100, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #75, step 101, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #75, step 102, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #75, step 103, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #75, step 104, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #75, step 105, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #75, step 106, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #75, step 107, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #75, step 108, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #75, step 109, discriminator loss=0.688 , generator loss=0.686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #75, step 110, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #75, step 111, discriminator loss=0.690 , generator loss=0.735\n",
      "Training progress in epoch #75, step 112, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #75, step 113, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #75, step 114, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #75, step 115, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #75, step 116, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #75, step 117, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #75, step 118, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #75, step 119, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #75, step 120, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #75, step 121, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #75, step 122, discriminator loss=0.691 , generator loss=0.674\n",
      "Training progress in epoch #75, step 123, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #75, step 124, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #75, step 125, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #75, step 126, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #75, step 127, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #75, step 128, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #75, step 129, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #75, step 130, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #75, step 131, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #75, step 132, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #75, step 133, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #75, step 134, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #75, step 135, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #75, step 136, discriminator loss=0.689 , generator loss=0.724\n",
      "Training progress in epoch #75, step 137, discriminator loss=0.697 , generator loss=0.717\n",
      "Training progress in epoch #75, step 138, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #75, step 139, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #75, step 140, discriminator loss=0.695 , generator loss=0.681\n",
      "Training progress in epoch #75, step 141, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #75, step 142, discriminator loss=0.685 , generator loss=0.720\n",
      "Training progress in epoch #75, step 143, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #75, step 144, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #75, step 145, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #75, step 146, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #75, step 147, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #75, step 148, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #75, step 149, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #75, step 150, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #75, step 151, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #75, step 152, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #75, step 153, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #75, step 154, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #75, step 155, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #75, step 156, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #75, step 157, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #75, step 158, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #75, step 159, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #75, step 160, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #75, step 161, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #75, step 162, discriminator loss=0.696 , generator loss=0.719\n",
      "Training progress in epoch #75, step 163, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #75, step 164, discriminator loss=0.697 , generator loss=0.669\n",
      "Training progress in epoch #75, step 165, discriminator loss=0.691 , generator loss=0.663\n",
      "Training progress in epoch #75, step 166, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #75, step 167, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #75, step 168, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #75, step 169, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #75, step 170, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #75, step 171, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #75, step 172, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #75, step 173, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #75, step 174, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #75, step 175, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #75, step 176, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #75, step 177, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #75, step 178, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #75, step 179, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #75, step 180, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #75, step 181, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #75, step 182, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #75, step 183, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #75, step 184, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #75, step 185, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #75, step 186, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #75, step 187, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #75, step 188, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #75, step 189, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #75, step 190, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #75, step 191, discriminator loss=0.685 , generator loss=0.712\n",
      "Training progress in epoch #75, step 192, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #75, step 193, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #75, step 194, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #75, step 195, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #75, step 196, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #75, step 197, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #75, step 198, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #75, step 199, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #75, step 200, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #75, step 201, discriminator loss=0.690 , generator loss=0.710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #75, step 202, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #75, step 203, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #75, step 204, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #75, step 205, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #75, step 206, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #75, step 207, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #75, step 208, discriminator loss=0.688 , generator loss=0.677\n",
      "Training progress in epoch #75, step 209, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #75, step 210, discriminator loss=0.687 , generator loss=0.679\n",
      "Training progress in epoch #75, step 211, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #75, step 212, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #75, step 213, discriminator loss=0.694 , generator loss=0.732\n",
      "Training progress in epoch #75, step 214, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #75, step 215, discriminator loss=0.697 , generator loss=0.714\n",
      "Training progress in epoch #75, step 216, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #75, step 217, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #75, step 218, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #75, step 219, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #75, step 220, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #75, step 221, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #75, step 222, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #75, step 223, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #75, step 224, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #75, step 225, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #75, step 226, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #75, step 227, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #75, step 228, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #75, step 229, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #75, step 230, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #75, step 231, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #75, step 232, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #75, step 233, discriminator loss=0.691 , generator loss=0.708\n",
      "Disciminator Accuracy on real images: 30%, on fake images: 72%\n",
      "Training progress in epoch #76, step 0, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #76, step 1, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #76, step 2, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #76, step 3, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #76, step 4, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #76, step 5, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #76, step 6, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #76, step 7, discriminator loss=0.684 , generator loss=0.710\n",
      "Training progress in epoch #76, step 8, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #76, step 9, discriminator loss=0.686 , generator loss=0.687\n",
      "Training progress in epoch #76, step 10, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #76, step 11, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #76, step 12, discriminator loss=0.685 , generator loss=0.691\n",
      "Training progress in epoch #76, step 13, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #76, step 14, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #76, step 15, discriminator loss=0.696 , generator loss=0.722\n",
      "Training progress in epoch #76, step 16, discriminator loss=0.688 , generator loss=0.738\n",
      "Training progress in epoch #76, step 17, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #76, step 18, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #76, step 19, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #76, step 20, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #76, step 21, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #76, step 22, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #76, step 23, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #76, step 24, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #76, step 25, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #76, step 26, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #76, step 27, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #76, step 28, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #76, step 29, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #76, step 30, discriminator loss=0.686 , generator loss=0.710\n",
      "Training progress in epoch #76, step 31, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #76, step 32, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #76, step 33, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #76, step 34, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #76, step 35, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #76, step 36, discriminator loss=0.693 , generator loss=0.678\n",
      "Training progress in epoch #76, step 37, discriminator loss=0.691 , generator loss=0.671\n",
      "Training progress in epoch #76, step 38, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #76, step 39, discriminator loss=0.695 , generator loss=0.740\n",
      "Training progress in epoch #76, step 40, discriminator loss=0.689 , generator loss=0.740\n",
      "Training progress in epoch #76, step 41, discriminator loss=0.698 , generator loss=0.723\n",
      "Training progress in epoch #76, step 42, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #76, step 43, discriminator loss=0.692 , generator loss=0.670\n",
      "Training progress in epoch #76, step 44, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #76, step 45, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #76, step 46, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #76, step 47, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #76, step 48, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #76, step 49, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #76, step 50, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #76, step 51, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #76, step 52, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #76, step 53, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #76, step 54, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #76, step 55, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #76, step 56, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #76, step 57, discriminator loss=0.696 , generator loss=0.733\n",
      "Training progress in epoch #76, step 58, discriminator loss=0.691 , generator loss=0.723\n",
      "Training progress in epoch #76, step 59, discriminator loss=0.692 , generator loss=0.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #76, step 60, discriminator loss=0.697 , generator loss=0.686\n",
      "Training progress in epoch #76, step 61, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #76, step 62, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #76, step 63, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #76, step 64, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #76, step 65, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #76, step 66, discriminator loss=0.700 , generator loss=0.703\n",
      "Training progress in epoch #76, step 67, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #76, step 68, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #76, step 69, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #76, step 70, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #76, step 71, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #76, step 72, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #76, step 73, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #76, step 74, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #76, step 75, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #76, step 76, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #76, step 77, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #76, step 78, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #76, step 79, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #76, step 80, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #76, step 81, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #76, step 82, discriminator loss=0.697 , generator loss=0.696\n",
      "Training progress in epoch #76, step 83, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #76, step 84, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #76, step 85, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #76, step 86, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #76, step 87, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #76, step 88, discriminator loss=0.696 , generator loss=0.689\n",
      "Training progress in epoch #76, step 89, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #76, step 90, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #76, step 91, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #76, step 92, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #76, step 93, discriminator loss=0.686 , generator loss=0.726\n",
      "Training progress in epoch #76, step 94, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #76, step 95, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #76, step 96, discriminator loss=0.697 , generator loss=0.684\n",
      "Training progress in epoch #76, step 97, discriminator loss=0.697 , generator loss=0.682\n",
      "Training progress in epoch #76, step 98, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #76, step 99, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #76, step 100, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #76, step 101, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #76, step 102, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #76, step 103, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #76, step 104, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #76, step 105, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #76, step 106, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #76, step 107, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #76, step 108, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #76, step 109, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #76, step 110, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #76, step 111, discriminator loss=0.697 , generator loss=0.698\n",
      "Training progress in epoch #76, step 112, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #76, step 113, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #76, step 114, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #76, step 115, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #76, step 116, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #76, step 117, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #76, step 118, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #76, step 119, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #76, step 120, discriminator loss=0.685 , generator loss=0.730\n",
      "Training progress in epoch #76, step 121, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #76, step 122, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #76, step 123, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #76, step 124, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #76, step 125, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #76, step 126, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #76, step 127, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #76, step 128, discriminator loss=0.697 , generator loss=0.716\n",
      "Training progress in epoch #76, step 129, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #76, step 130, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #76, step 131, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #76, step 132, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #76, step 133, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #76, step 134, discriminator loss=0.696 , generator loss=0.695\n",
      "Training progress in epoch #76, step 135, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #76, step 136, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #76, step 137, discriminator loss=0.697 , generator loss=0.715\n",
      "Training progress in epoch #76, step 138, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #76, step 139, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #76, step 140, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #76, step 141, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #76, step 142, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #76, step 143, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #76, step 144, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #76, step 145, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #76, step 146, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #76, step 147, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #76, step 148, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #76, step 149, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #76, step 150, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #76, step 151, discriminator loss=0.692 , generator loss=0.687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #76, step 152, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #76, step 153, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #76, step 154, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #76, step 155, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #76, step 156, discriminator loss=0.697 , generator loss=0.702\n",
      "Training progress in epoch #76, step 157, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #76, step 158, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #76, step 159, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #76, step 160, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #76, step 161, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #76, step 162, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #76, step 163, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #76, step 164, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #76, step 165, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #76, step 166, discriminator loss=0.697 , generator loss=0.699\n",
      "Training progress in epoch #76, step 167, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #76, step 168, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #76, step 169, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #76, step 170, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #76, step 171, discriminator loss=0.684 , generator loss=0.699\n",
      "Training progress in epoch #76, step 172, discriminator loss=0.685 , generator loss=0.710\n",
      "Training progress in epoch #76, step 173, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #76, step 174, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #76, step 175, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #76, step 176, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #76, step 177, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #76, step 178, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #76, step 179, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #76, step 180, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #76, step 181, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #76, step 182, discriminator loss=0.695 , generator loss=0.728\n",
      "Training progress in epoch #76, step 183, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #76, step 184, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #76, step 185, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #76, step 186, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #76, step 187, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #76, step 188, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #76, step 189, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #76, step 190, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #76, step 191, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #76, step 192, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #76, step 193, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #76, step 194, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #76, step 195, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #76, step 196, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #76, step 197, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #76, step 198, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #76, step 199, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #76, step 200, discriminator loss=0.696 , generator loss=0.685\n",
      "Training progress in epoch #76, step 201, discriminator loss=0.694 , generator loss=0.672\n",
      "Training progress in epoch #76, step 202, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #76, step 203, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #76, step 204, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #76, step 205, discriminator loss=0.696 , generator loss=0.729\n",
      "Training progress in epoch #76, step 206, discriminator loss=0.697 , generator loss=0.734\n",
      "Training progress in epoch #76, step 207, discriminator loss=0.691 , generator loss=0.732\n",
      "Training progress in epoch #76, step 208, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #76, step 209, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #76, step 210, discriminator loss=0.687 , generator loss=0.679\n",
      "Training progress in epoch #76, step 211, discriminator loss=0.692 , generator loss=0.663\n",
      "Training progress in epoch #76, step 212, discriminator loss=0.684 , generator loss=0.689\n",
      "Training progress in epoch #76, step 213, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #76, step 214, discriminator loss=0.685 , generator loss=0.730\n",
      "Training progress in epoch #76, step 215, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #76, step 216, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #76, step 217, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #76, step 218, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #76, step 219, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #76, step 220, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #76, step 221, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #76, step 222, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #76, step 223, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #76, step 224, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #76, step 225, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #76, step 226, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #76, step 227, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #76, step 228, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #76, step 229, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #76, step 230, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #76, step 231, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #76, step 232, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #76, step 233, discriminator loss=0.693 , generator loss=0.693\n",
      "Disciminator Accuracy on real images: 54%, on fake images: 67%\n",
      "Training progress in epoch #77, step 0, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #77, step 1, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #77, step 2, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #77, step 3, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #77, step 4, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #77, step 5, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #77, step 6, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #77, step 7, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #77, step 8, discriminator loss=0.690 , generator loss=0.696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #77, step 9, discriminator loss=0.695 , generator loss=0.677\n",
      "Training progress in epoch #77, step 10, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #77, step 11, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #77, step 12, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #77, step 13, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #77, step 14, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #77, step 15, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #77, step 16, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #77, step 17, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #77, step 18, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #77, step 19, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #77, step 20, discriminator loss=0.698 , generator loss=0.703\n",
      "Training progress in epoch #77, step 21, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #77, step 22, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #77, step 23, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #77, step 24, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #77, step 25, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #77, step 26, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #77, step 27, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #77, step 28, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #77, step 29, discriminator loss=0.697 , generator loss=0.711\n",
      "Training progress in epoch #77, step 30, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #77, step 31, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #77, step 32, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #77, step 33, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #77, step 34, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #77, step 35, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #77, step 36, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #77, step 37, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #77, step 38, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #77, step 39, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #77, step 40, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #77, step 41, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #77, step 42, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #77, step 43, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #77, step 44, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #77, step 45, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #77, step 46, discriminator loss=0.696 , generator loss=0.719\n",
      "Training progress in epoch #77, step 47, discriminator loss=0.693 , generator loss=0.731\n",
      "Training progress in epoch #77, step 48, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #77, step 49, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #77, step 50, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #77, step 51, discriminator loss=0.690 , generator loss=0.675\n",
      "Training progress in epoch #77, step 52, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #77, step 53, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #77, step 54, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #77, step 55, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #77, step 56, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #77, step 57, discriminator loss=0.694 , generator loss=0.730\n",
      "Training progress in epoch #77, step 58, discriminator loss=0.686 , generator loss=0.730\n",
      "Training progress in epoch #77, step 59, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #77, step 60, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #77, step 61, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #77, step 62, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #77, step 63, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #77, step 64, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #77, step 65, discriminator loss=0.697 , generator loss=0.698\n",
      "Training progress in epoch #77, step 66, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #77, step 67, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #77, step 68, discriminator loss=0.699 , generator loss=0.700\n",
      "Training progress in epoch #77, step 69, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #77, step 70, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #77, step 71, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #77, step 72, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #77, step 73, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #77, step 74, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #77, step 75, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #77, step 76, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #77, step 77, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #77, step 78, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #77, step 79, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #77, step 80, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #77, step 81, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #77, step 82, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #77, step 83, discriminator loss=0.697 , generator loss=0.719\n",
      "Training progress in epoch #77, step 84, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #77, step 85, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #77, step 86, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #77, step 87, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #77, step 88, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #77, step 89, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #77, step 90, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #77, step 91, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #77, step 92, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #77, step 93, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #77, step 94, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #77, step 95, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #77, step 96, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #77, step 97, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #77, step 98, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #77, step 99, discriminator loss=0.688 , generator loss=0.722\n",
      "Training progress in epoch #77, step 100, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #77, step 101, discriminator loss=0.692 , generator loss=0.696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #77, step 102, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #77, step 103, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #77, step 104, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #77, step 105, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #77, step 106, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #77, step 107, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #77, step 108, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #77, step 109, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #77, step 110, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #77, step 111, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #77, step 112, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #77, step 113, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #77, step 114, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #77, step 115, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #77, step 116, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #77, step 117, discriminator loss=0.694 , generator loss=0.675\n",
      "Training progress in epoch #77, step 118, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #77, step 119, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #77, step 120, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #77, step 121, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #77, step 122, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #77, step 123, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #77, step 124, discriminator loss=0.685 , generator loss=0.681\n",
      "Training progress in epoch #77, step 125, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #77, step 126, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #77, step 127, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #77, step 128, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #77, step 129, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #77, step 130, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #77, step 131, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #77, step 132, discriminator loss=0.698 , generator loss=0.702\n",
      "Training progress in epoch #77, step 133, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #77, step 134, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #77, step 135, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #77, step 136, discriminator loss=0.699 , generator loss=0.710\n",
      "Training progress in epoch #77, step 137, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #77, step 138, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #77, step 139, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #77, step 140, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #77, step 141, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #77, step 142, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #77, step 143, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #77, step 144, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #77, step 145, discriminator loss=0.698 , generator loss=0.695\n",
      "Training progress in epoch #77, step 146, discriminator loss=0.692 , generator loss=0.670\n",
      "Training progress in epoch #77, step 147, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #77, step 148, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #77, step 149, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #77, step 150, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #77, step 151, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #77, step 152, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #77, step 153, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #77, step 154, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #77, step 155, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #77, step 156, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #77, step 157, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #77, step 158, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #77, step 159, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #77, step 160, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #77, step 161, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #77, step 162, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #77, step 163, discriminator loss=0.692 , generator loss=0.738\n",
      "Training progress in epoch #77, step 164, discriminator loss=0.682 , generator loss=0.733\n",
      "Training progress in epoch #77, step 165, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #77, step 166, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #77, step 167, discriminator loss=0.695 , generator loss=0.670\n",
      "Training progress in epoch #77, step 168, discriminator loss=0.690 , generator loss=0.666\n",
      "Training progress in epoch #77, step 169, discriminator loss=0.697 , generator loss=0.677\n",
      "Training progress in epoch #77, step 170, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #77, step 171, discriminator loss=0.694 , generator loss=0.729\n",
      "Training progress in epoch #77, step 172, discriminator loss=0.693 , generator loss=0.733\n",
      "Training progress in epoch #77, step 173, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #77, step 174, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #77, step 175, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #77, step 176, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #77, step 177, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #77, step 178, discriminator loss=0.690 , generator loss=0.677\n",
      "Training progress in epoch #77, step 179, discriminator loss=0.687 , generator loss=0.676\n",
      "Training progress in epoch #77, step 180, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #77, step 181, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #77, step 182, discriminator loss=0.689 , generator loss=0.734\n",
      "Training progress in epoch #77, step 183, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #77, step 184, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #77, step 185, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #77, step 186, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #77, step 187, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #77, step 188, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #77, step 189, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #77, step 190, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #77, step 191, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #77, step 192, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #77, step 193, discriminator loss=0.693 , generator loss=0.689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #77, step 194, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #77, step 195, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #77, step 196, discriminator loss=0.696 , generator loss=0.723\n",
      "Training progress in epoch #77, step 197, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #77, step 198, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #77, step 199, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #77, step 200, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #77, step 201, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #77, step 202, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #77, step 203, discriminator loss=0.698 , generator loss=0.695\n",
      "Training progress in epoch #77, step 204, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #77, step 205, discriminator loss=0.698 , generator loss=0.685\n",
      "Training progress in epoch #77, step 206, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #77, step 207, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #77, step 208, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #77, step 209, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #77, step 210, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #77, step 211, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #77, step 212, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #77, step 213, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #77, step 214, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #77, step 215, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #77, step 216, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #77, step 217, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #77, step 218, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #77, step 219, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #77, step 220, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #77, step 221, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #77, step 222, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #77, step 223, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #77, step 224, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #77, step 225, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #77, step 226, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #77, step 227, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #77, step 228, discriminator loss=0.698 , generator loss=0.699\n",
      "Training progress in epoch #77, step 229, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #77, step 230, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #77, step 231, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #77, step 232, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #77, step 233, discriminator loss=0.691 , generator loss=0.702\n",
      "Disciminator Accuracy on real images: 34%, on fake images: 82%\n",
      "Training progress in epoch #78, step 0, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #78, step 1, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #78, step 2, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #78, step 3, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #78, step 4, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #78, step 5, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #78, step 6, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #78, step 7, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #78, step 8, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #78, step 9, discriminator loss=0.697 , generator loss=0.707\n",
      "Training progress in epoch #78, step 10, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #78, step 11, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #78, step 12, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #78, step 13, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #78, step 14, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #78, step 15, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #78, step 16, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #78, step 17, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #78, step 18, discriminator loss=0.686 , generator loss=0.717\n",
      "Training progress in epoch #78, step 19, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #78, step 20, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #78, step 21, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #78, step 22, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #78, step 23, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #78, step 24, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #78, step 25, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #78, step 26, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #78, step 27, discriminator loss=0.688 , generator loss=0.739\n",
      "Training progress in epoch #78, step 28, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #78, step 29, discriminator loss=0.686 , generator loss=0.711\n",
      "Training progress in epoch #78, step 30, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #78, step 31, discriminator loss=0.699 , generator loss=0.672\n",
      "Training progress in epoch #78, step 32, discriminator loss=0.694 , generator loss=0.673\n",
      "Training progress in epoch #78, step 33, discriminator loss=0.687 , generator loss=0.682\n",
      "Training progress in epoch #78, step 34, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #78, step 35, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #78, step 36, discriminator loss=0.685 , generator loss=0.735\n",
      "Training progress in epoch #78, step 37, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #78, step 38, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #78, step 39, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #78, step 40, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #78, step 41, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #78, step 42, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #78, step 43, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #78, step 44, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #78, step 45, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #78, step 46, discriminator loss=0.700 , generator loss=0.710\n",
      "Training progress in epoch #78, step 47, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #78, step 48, discriminator loss=0.694 , generator loss=0.674\n",
      "Training progress in epoch #78, step 49, discriminator loss=0.694 , generator loss=0.669\n",
      "Training progress in epoch #78, step 50, discriminator loss=0.685 , generator loss=0.691\n",
      "Training progress in epoch #78, step 51, discriminator loss=0.692 , generator loss=0.710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #78, step 52, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #78, step 53, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #78, step 54, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #78, step 55, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #78, step 56, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #78, step 57, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #78, step 58, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #78, step 59, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #78, step 60, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #78, step 61, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #78, step 62, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #78, step 63, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #78, step 64, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #78, step 65, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #78, step 66, discriminator loss=0.697 , generator loss=0.698\n",
      "Training progress in epoch #78, step 67, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #78, step 68, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #78, step 69, discriminator loss=0.697 , generator loss=0.693\n",
      "Training progress in epoch #78, step 70, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #78, step 71, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #78, step 72, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #78, step 73, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #78, step 74, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #78, step 75, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #78, step 76, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #78, step 77, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #78, step 78, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #78, step 79, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #78, step 80, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #78, step 81, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #78, step 82, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #78, step 83, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #78, step 84, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #78, step 85, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #78, step 86, discriminator loss=0.696 , generator loss=0.678\n",
      "Training progress in epoch #78, step 87, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #78, step 88, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #78, step 89, discriminator loss=0.696 , generator loss=0.724\n",
      "Training progress in epoch #78, step 90, discriminator loss=0.694 , generator loss=0.733\n",
      "Training progress in epoch #78, step 91, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #78, step 92, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #78, step 93, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #78, step 94, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #78, step 95, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #78, step 96, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #78, step 97, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #78, step 98, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #78, step 99, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #78, step 100, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #78, step 101, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #78, step 102, discriminator loss=0.691 , generator loss=0.736\n",
      "Training progress in epoch #78, step 103, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #78, step 104, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #78, step 105, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #78, step 106, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #78, step 107, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #78, step 108, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #78, step 109, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #78, step 110, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #78, step 111, discriminator loss=0.689 , generator loss=0.731\n",
      "Training progress in epoch #78, step 112, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #78, step 113, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #78, step 114, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #78, step 115, discriminator loss=0.695 , generator loss=0.680\n",
      "Training progress in epoch #78, step 116, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #78, step 117, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #78, step 118, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #78, step 119, discriminator loss=0.691 , generator loss=0.731\n",
      "Training progress in epoch #78, step 120, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #78, step 121, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #78, step 122, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #78, step 123, discriminator loss=0.695 , generator loss=0.682\n",
      "Training progress in epoch #78, step 124, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #78, step 125, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #78, step 126, discriminator loss=0.698 , generator loss=0.702\n",
      "Training progress in epoch #78, step 127, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #78, step 128, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #78, step 129, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #78, step 130, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #78, step 131, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #78, step 132, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #78, step 133, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #78, step 134, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #78, step 135, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #78, step 136, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #78, step 137, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #78, step 138, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #78, step 139, discriminator loss=0.687 , generator loss=0.667\n",
      "Training progress in epoch #78, step 140, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #78, step 141, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #78, step 142, discriminator loss=0.690 , generator loss=0.740\n",
      "Training progress in epoch #78, step 143, discriminator loss=0.695 , generator loss=0.732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #78, step 144, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #78, step 145, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #78, step 146, discriminator loss=0.691 , generator loss=0.669\n",
      "Training progress in epoch #78, step 147, discriminator loss=0.691 , generator loss=0.663\n",
      "Training progress in epoch #78, step 148, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #78, step 149, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #78, step 150, discriminator loss=0.693 , generator loss=0.731\n",
      "Training progress in epoch #78, step 151, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #78, step 152, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #78, step 153, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #78, step 154, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #78, step 155, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #78, step 156, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #78, step 157, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #78, step 158, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #78, step 159, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #78, step 160, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #78, step 161, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #78, step 162, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #78, step 163, discriminator loss=0.697 , generator loss=0.704\n",
      "Training progress in epoch #78, step 164, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #78, step 165, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #78, step 166, discriminator loss=0.698 , generator loss=0.710\n",
      "Training progress in epoch #78, step 167, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #78, step 168, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #78, step 169, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #78, step 170, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #78, step 171, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #78, step 172, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #78, step 173, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #78, step 174, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #78, step 175, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #78, step 176, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #78, step 177, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #78, step 178, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #78, step 179, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #78, step 180, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #78, step 181, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #78, step 182, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #78, step 183, discriminator loss=0.693 , generator loss=0.731\n",
      "Training progress in epoch #78, step 184, discriminator loss=0.693 , generator loss=0.729\n",
      "Training progress in epoch #78, step 185, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #78, step 186, discriminator loss=0.696 , generator loss=0.683\n",
      "Training progress in epoch #78, step 187, discriminator loss=0.690 , generator loss=0.657\n",
      "Training progress in epoch #78, step 188, discriminator loss=0.688 , generator loss=0.675\n",
      "Training progress in epoch #78, step 189, discriminator loss=0.696 , generator loss=0.682\n",
      "Training progress in epoch #78, step 190, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #78, step 191, discriminator loss=0.691 , generator loss=0.732\n",
      "Training progress in epoch #78, step 192, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #78, step 193, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #78, step 194, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #78, step 195, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #78, step 196, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #78, step 197, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #78, step 198, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #78, step 199, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #78, step 200, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #78, step 201, discriminator loss=0.691 , generator loss=0.676\n",
      "Training progress in epoch #78, step 202, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #78, step 203, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #78, step 204, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #78, step 205, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #78, step 206, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #78, step 207, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #78, step 208, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #78, step 209, discriminator loss=0.685 , generator loss=0.692\n",
      "Training progress in epoch #78, step 210, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #78, step 211, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #78, step 212, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #78, step 213, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #78, step 214, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #78, step 215, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #78, step 216, discriminator loss=0.685 , generator loss=0.707\n",
      "Training progress in epoch #78, step 217, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #78, step 218, discriminator loss=0.695 , generator loss=0.681\n",
      "Training progress in epoch #78, step 219, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #78, step 220, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #78, step 221, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #78, step 222, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #78, step 223, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #78, step 224, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #78, step 225, discriminator loss=0.691 , generator loss=0.723\n",
      "Training progress in epoch #78, step 226, discriminator loss=0.699 , generator loss=0.712\n",
      "Training progress in epoch #78, step 227, discriminator loss=0.696 , generator loss=0.706\n",
      "Training progress in epoch #78, step 228, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #78, step 229, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #78, step 230, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #78, step 231, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #78, step 232, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #78, step 233, discriminator loss=0.694 , generator loss=0.721\n",
      "Disciminator Accuracy on real images: 14%, on fake images: 98%\n",
      "Training progress in epoch #79, step 0, discriminator loss=0.686 , generator loss=0.714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #79, step 1, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #79, step 2, discriminator loss=0.685 , generator loss=0.711\n",
      "Training progress in epoch #79, step 3, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #79, step 4, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #79, step 5, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #79, step 6, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #79, step 7, discriminator loss=0.687 , generator loss=0.678\n",
      "Training progress in epoch #79, step 8, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #79, step 9, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #79, step 10, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #79, step 11, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #79, step 12, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #79, step 13, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #79, step 14, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #79, step 15, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #79, step 16, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #79, step 17, discriminator loss=0.696 , generator loss=0.683\n",
      "Training progress in epoch #79, step 18, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #79, step 19, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #79, step 20, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #79, step 21, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #79, step 22, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #79, step 23, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #79, step 24, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #79, step 25, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #79, step 26, discriminator loss=0.685 , generator loss=0.704\n",
      "Training progress in epoch #79, step 27, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #79, step 28, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #79, step 29, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #79, step 30, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #79, step 31, discriminator loss=0.693 , generator loss=0.728\n",
      "Training progress in epoch #79, step 32, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #79, step 33, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #79, step 34, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #79, step 35, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #79, step 36, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #79, step 37, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #79, step 38, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #79, step 39, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #79, step 40, discriminator loss=0.691 , generator loss=0.724\n",
      "Training progress in epoch #79, step 41, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #79, step 42, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #79, step 43, discriminator loss=0.686 , generator loss=0.679\n",
      "Training progress in epoch #79, step 44, discriminator loss=0.686 , generator loss=0.682\n",
      "Training progress in epoch #79, step 45, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #79, step 46, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #79, step 47, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #79, step 48, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #79, step 49, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #79, step 50, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #79, step 51, discriminator loss=0.698 , generator loss=0.702\n",
      "Training progress in epoch #79, step 52, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #79, step 53, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #79, step 54, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #79, step 55, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #79, step 56, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #79, step 57, discriminator loss=0.685 , generator loss=0.683\n",
      "Training progress in epoch #79, step 58, discriminator loss=0.683 , generator loss=0.688\n",
      "Training progress in epoch #79, step 59, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #79, step 60, discriminator loss=0.697 , generator loss=0.723\n",
      "Training progress in epoch #79, step 61, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #79, step 62, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #79, step 63, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #79, step 64, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #79, step 65, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #79, step 66, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #79, step 67, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #79, step 68, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #79, step 69, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #79, step 70, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #79, step 71, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #79, step 72, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #79, step 73, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #79, step 74, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #79, step 75, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #79, step 76, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #79, step 77, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #79, step 78, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #79, step 79, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #79, step 80, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #79, step 81, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #79, step 82, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #79, step 83, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #79, step 84, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #79, step 85, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #79, step 86, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #79, step 87, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #79, step 88, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #79, step 89, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #79, step 90, discriminator loss=0.685 , generator loss=0.693\n",
      "Training progress in epoch #79, step 91, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #79, step 92, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #79, step 93, discriminator loss=0.694 , generator loss=0.704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #79, step 94, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #79, step 95, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #79, step 96, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #79, step 97, discriminator loss=0.698 , generator loss=0.686\n",
      "Training progress in epoch #79, step 98, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #79, step 99, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #79, step 100, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #79, step 101, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #79, step 102, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #79, step 103, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #79, step 104, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #79, step 105, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #79, step 106, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #79, step 107, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #79, step 108, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #79, step 109, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #79, step 110, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #79, step 111, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #79, step 112, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #79, step 113, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #79, step 114, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #79, step 115, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #79, step 116, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #79, step 117, discriminator loss=0.694 , generator loss=0.730\n",
      "Training progress in epoch #79, step 118, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #79, step 119, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #79, step 120, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #79, step 121, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #79, step 122, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #79, step 123, discriminator loss=0.684 , generator loss=0.705\n",
      "Training progress in epoch #79, step 124, discriminator loss=0.696 , generator loss=0.695\n",
      "Training progress in epoch #79, step 125, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #79, step 126, discriminator loss=0.697 , generator loss=0.718\n",
      "Training progress in epoch #79, step 127, discriminator loss=0.694 , generator loss=0.726\n",
      "Training progress in epoch #79, step 128, discriminator loss=0.694 , generator loss=0.730\n",
      "Training progress in epoch #79, step 129, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #79, step 130, discriminator loss=0.697 , generator loss=0.678\n",
      "Training progress in epoch #79, step 131, discriminator loss=0.685 , generator loss=0.661\n",
      "Training progress in epoch #79, step 132, discriminator loss=0.697 , generator loss=0.671\n",
      "Training progress in epoch #79, step 133, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #79, step 134, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #79, step 135, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #79, step 136, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #79, step 137, discriminator loss=0.683 , generator loss=0.715\n",
      "Training progress in epoch #79, step 138, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #79, step 139, discriminator loss=0.688 , generator loss=0.727\n",
      "Training progress in epoch #79, step 140, discriminator loss=0.690 , generator loss=0.733\n",
      "Training progress in epoch #79, step 141, discriminator loss=0.697 , generator loss=0.700\n",
      "Training progress in epoch #79, step 142, discriminator loss=0.687 , generator loss=0.682\n",
      "Training progress in epoch #79, step 143, discriminator loss=0.695 , generator loss=0.674\n",
      "Training progress in epoch #79, step 144, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #79, step 145, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #79, step 146, discriminator loss=0.694 , generator loss=0.728\n",
      "Training progress in epoch #79, step 147, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #79, step 148, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #79, step 149, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #79, step 150, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #79, step 151, discriminator loss=0.701 , generator loss=0.703\n",
      "Training progress in epoch #79, step 152, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #79, step 153, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #79, step 154, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #79, step 155, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #79, step 156, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #79, step 157, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #79, step 158, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #79, step 159, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #79, step 160, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #79, step 161, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #79, step 162, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #79, step 163, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #79, step 164, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #79, step 165, discriminator loss=0.699 , generator loss=0.714\n",
      "Training progress in epoch #79, step 166, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #79, step 167, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #79, step 168, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #79, step 169, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #79, step 170, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #79, step 171, discriminator loss=0.693 , generator loss=0.727\n",
      "Training progress in epoch #79, step 172, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #79, step 173, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #79, step 174, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #79, step 175, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #79, step 176, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #79, step 177, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #79, step 178, discriminator loss=0.697 , generator loss=0.707\n",
      "Training progress in epoch #79, step 179, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #79, step 180, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #79, step 181, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #79, step 182, discriminator loss=0.693 , generator loss=0.726\n",
      "Training progress in epoch #79, step 183, discriminator loss=0.698 , generator loss=0.729\n",
      "Training progress in epoch #79, step 184, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #79, step 185, discriminator loss=0.693 , generator loss=0.697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #79, step 186, discriminator loss=0.698 , generator loss=0.692\n",
      "Training progress in epoch #79, step 187, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #79, step 188, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #79, step 189, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #79, step 190, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #79, step 191, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #79, step 192, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #79, step 193, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #79, step 194, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #79, step 195, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #79, step 196, discriminator loss=0.695 , generator loss=0.719\n",
      "Training progress in epoch #79, step 197, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #79, step 198, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #79, step 199, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #79, step 200, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #79, step 201, discriminator loss=0.696 , generator loss=0.677\n",
      "Training progress in epoch #79, step 202, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #79, step 203, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #79, step 204, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #79, step 205, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #79, step 206, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #79, step 207, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #79, step 208, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #79, step 209, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #79, step 210, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #79, step 211, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #79, step 212, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #79, step 213, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #79, step 214, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #79, step 215, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #79, step 216, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #79, step 217, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #79, step 218, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #79, step 219, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #79, step 220, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #79, step 221, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #79, step 222, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #79, step 223, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #79, step 224, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #79, step 225, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #79, step 226, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #79, step 227, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #79, step 228, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #79, step 229, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #79, step 230, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #79, step 231, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #79, step 232, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #79, step 233, discriminator loss=0.692 , generator loss=0.698\n",
      "Disciminator Accuracy on real images: 75%, on fake images: 50%\n",
      "Training progress in epoch #80, step 0, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #80, step 1, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #80, step 2, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #80, step 3, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #80, step 4, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #80, step 5, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #80, step 6, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #80, step 7, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #80, step 8, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #80, step 9, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #80, step 10, discriminator loss=0.697 , generator loss=0.692\n",
      "Training progress in epoch #80, step 11, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #80, step 12, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #80, step 13, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #80, step 14, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #80, step 15, discriminator loss=0.696 , generator loss=0.685\n",
      "Training progress in epoch #80, step 16, discriminator loss=0.692 , generator loss=0.675\n",
      "Training progress in epoch #80, step 17, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #80, step 18, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #80, step 19, discriminator loss=0.693 , generator loss=0.728\n",
      "Training progress in epoch #80, step 20, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #80, step 21, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #80, step 22, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #80, step 23, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #80, step 24, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #80, step 25, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #80, step 26, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #80, step 27, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #80, step 28, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #80, step 29, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #80, step 30, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #80, step 31, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #80, step 32, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #80, step 33, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #80, step 34, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #80, step 35, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #80, step 36, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #80, step 37, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #80, step 38, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #80, step 39, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #80, step 40, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #80, step 41, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #80, step 42, discriminator loss=0.696 , generator loss=0.689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #80, step 43, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #80, step 44, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #80, step 45, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #80, step 46, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #80, step 47, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #80, step 48, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #80, step 49, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #80, step 50, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #80, step 51, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #80, step 52, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #80, step 53, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #80, step 54, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #80, step 55, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #80, step 56, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #80, step 57, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #80, step 58, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #80, step 59, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #80, step 60, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #80, step 61, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #80, step 62, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #80, step 63, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #80, step 64, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #80, step 65, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #80, step 66, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #80, step 67, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #80, step 68, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #80, step 69, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #80, step 70, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #80, step 71, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #80, step 72, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #80, step 73, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #80, step 74, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #80, step 75, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #80, step 76, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #80, step 77, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #80, step 78, discriminator loss=0.699 , generator loss=0.702\n",
      "Training progress in epoch #80, step 79, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #80, step 80, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #80, step 81, discriminator loss=0.684 , generator loss=0.697\n",
      "Training progress in epoch #80, step 82, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #80, step 83, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #80, step 84, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #80, step 85, discriminator loss=0.696 , generator loss=0.724\n",
      "Training progress in epoch #80, step 86, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #80, step 87, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #80, step 88, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #80, step 89, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #80, step 90, discriminator loss=0.695 , generator loss=0.681\n",
      "Training progress in epoch #80, step 91, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #80, step 92, discriminator loss=0.692 , generator loss=0.728\n",
      "Training progress in epoch #80, step 93, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #80, step 94, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #80, step 95, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #80, step 96, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #80, step 97, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #80, step 98, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #80, step 99, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #80, step 100, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #80, step 101, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #80, step 102, discriminator loss=0.698 , generator loss=0.709\n",
      "Training progress in epoch #80, step 103, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #80, step 104, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #80, step 105, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #80, step 106, discriminator loss=0.698 , generator loss=0.707\n",
      "Training progress in epoch #80, step 107, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #80, step 108, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #80, step 109, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #80, step 110, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #80, step 111, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #80, step 112, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #80, step 113, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #80, step 114, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #80, step 115, discriminator loss=0.698 , generator loss=0.713\n",
      "Training progress in epoch #80, step 116, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #80, step 117, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #80, step 118, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #80, step 119, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #80, step 120, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #80, step 121, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #80, step 122, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #80, step 123, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #80, step 124, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #80, step 125, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #80, step 126, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #80, step 127, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #80, step 128, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #80, step 129, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #80, step 130, discriminator loss=0.690 , generator loss=0.724\n",
      "Training progress in epoch #80, step 131, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #80, step 132, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #80, step 133, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #80, step 134, discriminator loss=0.691 , generator loss=0.692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #80, step 135, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #80, step 136, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #80, step 137, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #80, step 138, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #80, step 139, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #80, step 140, discriminator loss=0.695 , generator loss=0.689\n",
      "Training progress in epoch #80, step 141, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #80, step 142, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #80, step 143, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #80, step 144, discriminator loss=0.696 , generator loss=0.695\n",
      "Training progress in epoch #80, step 145, discriminator loss=0.696 , generator loss=0.690\n",
      "Training progress in epoch #80, step 146, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #80, step 147, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #80, step 148, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #80, step 149, discriminator loss=0.697 , generator loss=0.694\n",
      "Training progress in epoch #80, step 150, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #80, step 151, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #80, step 152, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #80, step 153, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #80, step 154, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #80, step 155, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #80, step 156, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #80, step 157, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #80, step 158, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #80, step 159, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #80, step 160, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #80, step 161, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #80, step 162, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #80, step 163, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #80, step 164, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #80, step 165, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #80, step 166, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #80, step 167, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #80, step 168, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #80, step 169, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #80, step 170, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #80, step 171, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #80, step 172, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #80, step 173, discriminator loss=0.693 , generator loss=0.729\n",
      "Training progress in epoch #80, step 174, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #80, step 175, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #80, step 176, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #80, step 177, discriminator loss=0.688 , generator loss=0.680\n",
      "Training progress in epoch #80, step 178, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #80, step 179, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #80, step 180, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #80, step 181, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #80, step 182, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #80, step 183, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #80, step 184, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #80, step 185, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #80, step 186, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #80, step 187, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #80, step 188, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #80, step 189, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #80, step 190, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #80, step 191, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #80, step 192, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #80, step 193, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #80, step 194, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #80, step 195, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #80, step 196, discriminator loss=0.689 , generator loss=0.727\n",
      "Training progress in epoch #80, step 197, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #80, step 198, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #80, step 199, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #80, step 200, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #80, step 201, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #80, step 202, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #80, step 203, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #80, step 204, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #80, step 205, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #80, step 206, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #80, step 207, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #80, step 208, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #80, step 209, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #80, step 210, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #80, step 211, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #80, step 212, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #80, step 213, discriminator loss=0.699 , generator loss=0.703\n",
      "Training progress in epoch #80, step 214, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #80, step 215, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #80, step 216, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #80, step 217, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #80, step 218, discriminator loss=0.697 , generator loss=0.723\n",
      "Training progress in epoch #80, step 219, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #80, step 220, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #80, step 221, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #80, step 222, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #80, step 223, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #80, step 224, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #80, step 225, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #80, step 226, discriminator loss=0.692 , generator loss=0.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #80, step 227, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #80, step 228, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #80, step 229, discriminator loss=0.699 , generator loss=0.681\n",
      "Training progress in epoch #80, step 230, discriminator loss=0.686 , generator loss=0.680\n",
      "Training progress in epoch #80, step 231, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #80, step 232, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #80, step 233, discriminator loss=0.693 , generator loss=0.716\n",
      "Disciminator Accuracy on real images: 27%, on fake images: 93%\n",
      "Training progress in epoch #81, step 0, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #81, step 1, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #81, step 2, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #81, step 3, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #81, step 4, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #81, step 5, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #81, step 6, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #81, step 7, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #81, step 8, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #81, step 9, discriminator loss=0.697 , generator loss=0.718\n",
      "Training progress in epoch #81, step 10, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #81, step 11, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #81, step 12, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #81, step 13, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #81, step 14, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #81, step 15, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #81, step 16, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #81, step 17, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #81, step 18, discriminator loss=0.690 , generator loss=0.679\n",
      "Training progress in epoch #81, step 19, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #81, step 20, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #81, step 21, discriminator loss=0.696 , generator loss=0.720\n",
      "Training progress in epoch #81, step 22, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #81, step 23, discriminator loss=0.695 , generator loss=0.683\n",
      "Training progress in epoch #81, step 24, discriminator loss=0.691 , generator loss=0.667\n",
      "Training progress in epoch #81, step 25, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #81, step 26, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #81, step 27, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #81, step 28, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #81, step 29, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #81, step 30, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #81, step 31, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #81, step 32, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #81, step 33, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #81, step 34, discriminator loss=0.683 , generator loss=0.704\n",
      "Training progress in epoch #81, step 35, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #81, step 36, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #81, step 37, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #81, step 38, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #81, step 39, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #81, step 40, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #81, step 41, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #81, step 42, discriminator loss=0.697 , generator loss=0.703\n",
      "Training progress in epoch #81, step 43, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #81, step 44, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #81, step 45, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #81, step 46, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #81, step 47, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #81, step 48, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #81, step 49, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #81, step 50, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #81, step 51, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #81, step 52, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #81, step 53, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #81, step 54, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #81, step 55, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #81, step 56, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #81, step 57, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #81, step 58, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #81, step 59, discriminator loss=0.685 , generator loss=0.720\n",
      "Training progress in epoch #81, step 60, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #81, step 61, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #81, step 62, discriminator loss=0.686 , generator loss=0.688\n",
      "Training progress in epoch #81, step 63, discriminator loss=0.687 , generator loss=0.671\n",
      "Training progress in epoch #81, step 64, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #81, step 65, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #81, step 66, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #81, step 67, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #81, step 68, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #81, step 69, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #81, step 70, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #81, step 71, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #81, step 72, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #81, step 73, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #81, step 74, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #81, step 75, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #81, step 76, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #81, step 77, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #81, step 78, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #81, step 79, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #81, step 80, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #81, step 81, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #81, step 82, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #81, step 83, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #81, step 84, discriminator loss=0.691 , generator loss=0.689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #81, step 85, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #81, step 86, discriminator loss=0.689 , generator loss=0.732\n",
      "Training progress in epoch #81, step 87, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #81, step 88, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #81, step 89, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #81, step 90, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #81, step 91, discriminator loss=0.684 , generator loss=0.689\n",
      "Training progress in epoch #81, step 92, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #81, step 93, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #81, step 94, discriminator loss=0.697 , generator loss=0.721\n",
      "Training progress in epoch #81, step 95, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #81, step 96, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #81, step 97, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #81, step 98, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #81, step 99, discriminator loss=0.682 , generator loss=0.704\n",
      "Training progress in epoch #81, step 100, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #81, step 101, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #81, step 102, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #81, step 103, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #81, step 104, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #81, step 105, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #81, step 106, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #81, step 107, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #81, step 108, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #81, step 109, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #81, step 110, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #81, step 111, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #81, step 112, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #81, step 113, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #81, step 114, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #81, step 115, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #81, step 116, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #81, step 117, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #81, step 118, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #81, step 119, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #81, step 120, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #81, step 121, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #81, step 122, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #81, step 123, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #81, step 124, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #81, step 125, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #81, step 126, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #81, step 127, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #81, step 128, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #81, step 129, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #81, step 130, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #81, step 131, discriminator loss=0.699 , generator loss=0.707\n",
      "Training progress in epoch #81, step 132, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #81, step 133, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #81, step 134, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #81, step 135, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #81, step 136, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #81, step 137, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #81, step 138, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #81, step 139, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #81, step 140, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #81, step 141, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #81, step 142, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #81, step 143, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #81, step 144, discriminator loss=0.685 , generator loss=0.671\n",
      "Training progress in epoch #81, step 145, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #81, step 146, discriminator loss=0.686 , generator loss=0.696\n",
      "Training progress in epoch #81, step 147, discriminator loss=0.691 , generator loss=0.729\n",
      "Training progress in epoch #81, step 148, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #81, step 149, discriminator loss=0.690 , generator loss=0.738\n",
      "Training progress in epoch #81, step 150, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #81, step 151, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #81, step 152, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #81, step 153, discriminator loss=0.688 , generator loss=0.684\n",
      "Training progress in epoch #81, step 154, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #81, step 155, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #81, step 156, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #81, step 157, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #81, step 158, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #81, step 159, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #81, step 160, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #81, step 161, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #81, step 162, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #81, step 163, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #81, step 164, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #81, step 165, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #81, step 166, discriminator loss=0.683 , generator loss=0.708\n",
      "Training progress in epoch #81, step 167, discriminator loss=0.695 , generator loss=0.726\n",
      "Training progress in epoch #81, step 168, discriminator loss=0.688 , generator loss=0.738\n",
      "Training progress in epoch #81, step 169, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #81, step 170, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #81, step 171, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #81, step 172, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #81, step 173, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #81, step 174, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #81, step 175, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #81, step 176, discriminator loss=0.689 , generator loss=0.697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #81, step 177, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #81, step 178, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #81, step 179, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #81, step 180, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #81, step 181, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #81, step 182, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #81, step 183, discriminator loss=0.686 , generator loss=0.684\n",
      "Training progress in epoch #81, step 184, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #81, step 185, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #81, step 186, discriminator loss=0.685 , generator loss=0.715\n",
      "Training progress in epoch #81, step 187, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #81, step 188, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #81, step 189, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #81, step 190, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #81, step 191, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #81, step 192, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #81, step 193, discriminator loss=0.684 , generator loss=0.700\n",
      "Training progress in epoch #81, step 194, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #81, step 195, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #81, step 196, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #81, step 197, discriminator loss=0.691 , generator loss=0.722\n",
      "Training progress in epoch #81, step 198, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #81, step 199, discriminator loss=0.698 , generator loss=0.688\n",
      "Training progress in epoch #81, step 200, discriminator loss=0.690 , generator loss=0.668\n",
      "Training progress in epoch #81, step 201, discriminator loss=0.692 , generator loss=0.669\n",
      "Training progress in epoch #81, step 202, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #81, step 203, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #81, step 204, discriminator loss=0.690 , generator loss=0.729\n",
      "Training progress in epoch #81, step 205, discriminator loss=0.688 , generator loss=0.732\n",
      "Training progress in epoch #81, step 206, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #81, step 207, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #81, step 208, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #81, step 209, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #81, step 210, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #81, step 211, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #81, step 212, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #81, step 213, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #81, step 214, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #81, step 215, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #81, step 216, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #81, step 217, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #81, step 218, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #81, step 219, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #81, step 220, discriminator loss=0.697 , generator loss=0.706\n",
      "Training progress in epoch #81, step 221, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #81, step 222, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #81, step 223, discriminator loss=0.696 , generator loss=0.680\n",
      "Training progress in epoch #81, step 224, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #81, step 225, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #81, step 226, discriminator loss=0.687 , generator loss=0.715\n",
      "Training progress in epoch #81, step 227, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #81, step 228, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #81, step 229, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #81, step 230, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #81, step 231, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #81, step 232, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #81, step 233, discriminator loss=0.690 , generator loss=0.695\n",
      "Disciminator Accuracy on real images: 53%, on fake images: 65%\n",
      "Training progress in epoch #82, step 0, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #82, step 1, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #82, step 2, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #82, step 3, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #82, step 4, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #82, step 5, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #82, step 6, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #82, step 7, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #82, step 8, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #82, step 9, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #82, step 10, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #82, step 11, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #82, step 12, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #82, step 13, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #82, step 14, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #82, step 15, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #82, step 16, discriminator loss=0.685 , generator loss=0.720\n",
      "Training progress in epoch #82, step 17, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #82, step 18, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #82, step 19, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #82, step 20, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #82, step 21, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #82, step 22, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #82, step 23, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #82, step 24, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #82, step 25, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #82, step 26, discriminator loss=0.701 , generator loss=0.698\n",
      "Training progress in epoch #82, step 27, discriminator loss=0.689 , generator loss=0.677\n",
      "Training progress in epoch #82, step 28, discriminator loss=0.698 , generator loss=0.678\n",
      "Training progress in epoch #82, step 29, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #82, step 30, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #82, step 31, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #82, step 32, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #82, step 33, discriminator loss=0.692 , generator loss=0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #82, step 34, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #82, step 35, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #82, step 36, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #82, step 37, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #82, step 38, discriminator loss=0.697 , generator loss=0.700\n",
      "Training progress in epoch #82, step 39, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #82, step 40, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #82, step 41, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #82, step 42, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #82, step 43, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #82, step 44, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #82, step 45, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #82, step 46, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #82, step 47, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #82, step 48, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #82, step 49, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #82, step 50, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #82, step 51, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #82, step 52, discriminator loss=0.696 , generator loss=0.689\n",
      "Training progress in epoch #82, step 53, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #82, step 54, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #82, step 55, discriminator loss=0.695 , generator loss=0.716\n",
      "Training progress in epoch #82, step 56, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #82, step 57, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #82, step 58, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #82, step 59, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #82, step 60, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #82, step 61, discriminator loss=0.685 , generator loss=0.724\n",
      "Training progress in epoch #82, step 62, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #82, step 63, discriminator loss=0.684 , generator loss=0.693\n",
      "Training progress in epoch #82, step 64, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #82, step 65, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #82, step 66, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #82, step 67, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #82, step 68, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #82, step 69, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #82, step 70, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #82, step 71, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #82, step 72, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #82, step 73, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #82, step 74, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #82, step 75, discriminator loss=0.698 , generator loss=0.687\n",
      "Training progress in epoch #82, step 76, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #82, step 77, discriminator loss=0.688 , generator loss=0.678\n",
      "Training progress in epoch #82, step 78, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #82, step 79, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #82, step 80, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #82, step 81, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #82, step 82, discriminator loss=0.697 , generator loss=0.711\n",
      "Training progress in epoch #82, step 83, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #82, step 84, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #82, step 85, discriminator loss=0.682 , generator loss=0.706\n",
      "Training progress in epoch #82, step 86, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #82, step 87, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #82, step 88, discriminator loss=0.697 , generator loss=0.691\n",
      "Training progress in epoch #82, step 89, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #82, step 90, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #82, step 91, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #82, step 92, discriminator loss=0.699 , generator loss=0.723\n",
      "Training progress in epoch #82, step 93, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #82, step 94, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #82, step 95, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #82, step 96, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #82, step 97, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #82, step 98, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #82, step 99, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #82, step 100, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #82, step 101, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #82, step 102, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #82, step 103, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #82, step 104, discriminator loss=0.697 , generator loss=0.689\n",
      "Training progress in epoch #82, step 105, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #82, step 106, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #82, step 107, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #82, step 108, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #82, step 109, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #82, step 110, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #82, step 111, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #82, step 112, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #82, step 113, discriminator loss=0.686 , generator loss=0.719\n",
      "Training progress in epoch #82, step 114, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #82, step 115, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #82, step 116, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #82, step 117, discriminator loss=0.688 , generator loss=0.678\n",
      "Training progress in epoch #82, step 118, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #82, step 119, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #82, step 120, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #82, step 121, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #82, step 122, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #82, step 123, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #82, step 124, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #82, step 125, discriminator loss=0.690 , generator loss=0.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #82, step 126, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #82, step 127, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #82, step 128, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #82, step 129, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #82, step 130, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #82, step 131, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #82, step 132, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #82, step 133, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #82, step 134, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #82, step 135, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #82, step 136, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #82, step 137, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #82, step 138, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #82, step 139, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #82, step 140, discriminator loss=0.697 , generator loss=0.684\n",
      "Training progress in epoch #82, step 141, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #82, step 142, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #82, step 143, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #82, step 144, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #82, step 145, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #82, step 146, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #82, step 147, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #82, step 148, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #82, step 149, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #82, step 150, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #82, step 151, discriminator loss=0.694 , generator loss=0.737\n",
      "Training progress in epoch #82, step 152, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #82, step 153, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #82, step 154, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #82, step 155, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #82, step 156, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #82, step 157, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #82, step 158, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #82, step 159, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #82, step 160, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #82, step 161, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #82, step 162, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #82, step 163, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #82, step 164, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #82, step 165, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #82, step 166, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #82, step 167, discriminator loss=0.694 , generator loss=0.674\n",
      "Training progress in epoch #82, step 168, discriminator loss=0.693 , generator loss=0.672\n",
      "Training progress in epoch #82, step 169, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #82, step 170, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #82, step 171, discriminator loss=0.687 , generator loss=0.717\n",
      "Training progress in epoch #82, step 172, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #82, step 173, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #82, step 174, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #82, step 175, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #82, step 176, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #82, step 177, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #82, step 178, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #82, step 179, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #82, step 180, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #82, step 181, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #82, step 182, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #82, step 183, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #82, step 184, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #82, step 185, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #82, step 186, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #82, step 187, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #82, step 188, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #82, step 189, discriminator loss=0.686 , generator loss=0.698\n",
      "Training progress in epoch #82, step 190, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #82, step 191, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #82, step 192, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #82, step 193, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #82, step 194, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #82, step 195, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #82, step 196, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #82, step 197, discriminator loss=0.694 , generator loss=0.725\n",
      "Training progress in epoch #82, step 198, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #82, step 199, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #82, step 200, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #82, step 201, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #82, step 202, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #82, step 203, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #82, step 204, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #82, step 205, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #82, step 206, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #82, step 207, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #82, step 208, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #82, step 209, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #82, step 210, discriminator loss=0.685 , generator loss=0.682\n",
      "Training progress in epoch #82, step 211, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #82, step 212, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #82, step 213, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #82, step 214, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #82, step 215, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #82, step 216, discriminator loss=0.699 , generator loss=0.715\n",
      "Training progress in epoch #82, step 217, discriminator loss=0.690 , generator loss=0.724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #82, step 218, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #82, step 219, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #82, step 220, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #82, step 221, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #82, step 222, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #82, step 223, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #82, step 224, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #82, step 225, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #82, step 226, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #82, step 227, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #82, step 228, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #82, step 229, discriminator loss=0.685 , generator loss=0.684\n",
      "Training progress in epoch #82, step 230, discriminator loss=0.697 , generator loss=0.686\n",
      "Training progress in epoch #82, step 231, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #82, step 232, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #82, step 233, discriminator loss=0.688 , generator loss=0.699\n",
      "Disciminator Accuracy on real images: 38%, on fake images: 81%\n",
      "Training progress in epoch #83, step 0, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #83, step 1, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #83, step 2, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #83, step 3, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #83, step 4, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #83, step 5, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #83, step 6, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #83, step 7, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #83, step 8, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #83, step 9, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #83, step 10, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #83, step 11, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #83, step 12, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #83, step 13, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #83, step 14, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #83, step 15, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #83, step 16, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #83, step 17, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #83, step 18, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #83, step 19, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #83, step 20, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #83, step 21, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #83, step 22, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #83, step 23, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #83, step 24, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #83, step 25, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #83, step 26, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #83, step 27, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #83, step 28, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #83, step 29, discriminator loss=0.698 , generator loss=0.724\n",
      "Training progress in epoch #83, step 30, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #83, step 31, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #83, step 32, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #83, step 33, discriminator loss=0.689 , generator loss=0.673\n",
      "Training progress in epoch #83, step 34, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #83, step 35, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #83, step 36, discriminator loss=0.697 , generator loss=0.718\n",
      "Training progress in epoch #83, step 37, discriminator loss=0.698 , generator loss=0.720\n",
      "Training progress in epoch #83, step 38, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #83, step 39, discriminator loss=0.685 , generator loss=0.702\n",
      "Training progress in epoch #83, step 40, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #83, step 41, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #83, step 42, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #83, step 43, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #83, step 44, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #83, step 45, discriminator loss=0.697 , generator loss=0.704\n",
      "Training progress in epoch #83, step 46, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #83, step 47, discriminator loss=0.690 , generator loss=0.739\n",
      "Training progress in epoch #83, step 48, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #83, step 49, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #83, step 50, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #83, step 51, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #83, step 52, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #83, step 53, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #83, step 54, discriminator loss=0.698 , generator loss=0.693\n",
      "Training progress in epoch #83, step 55, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #83, step 56, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #83, step 57, discriminator loss=0.685 , generator loss=0.715\n",
      "Training progress in epoch #83, step 58, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #83, step 59, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #83, step 60, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #83, step 61, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #83, step 62, discriminator loss=0.683 , generator loss=0.699\n",
      "Training progress in epoch #83, step 63, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #83, step 64, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #83, step 65, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #83, step 66, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #83, step 67, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #83, step 68, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #83, step 69, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #83, step 70, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #83, step 71, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #83, step 72, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #83, step 73, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #83, step 74, discriminator loss=0.686 , generator loss=0.723\n",
      "Training progress in epoch #83, step 75, discriminator loss=0.688 , generator loss=0.708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #83, step 76, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #83, step 77, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #83, step 78, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #83, step 79, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #83, step 80, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #83, step 81, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #83, step 82, discriminator loss=0.691 , generator loss=0.677\n",
      "Training progress in epoch #83, step 83, discriminator loss=0.696 , generator loss=0.694\n",
      "Training progress in epoch #83, step 84, discriminator loss=0.699 , generator loss=0.719\n",
      "Training progress in epoch #83, step 85, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #83, step 86, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #83, step 87, discriminator loss=0.699 , generator loss=0.715\n",
      "Training progress in epoch #83, step 88, discriminator loss=0.688 , generator loss=0.714\n",
      "Training progress in epoch #83, step 89, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #83, step 90, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #83, step 91, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #83, step 92, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #83, step 93, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #83, step 94, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #83, step 95, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #83, step 96, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #83, step 97, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #83, step 98, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #83, step 99, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #83, step 100, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #83, step 101, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #83, step 102, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #83, step 103, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #83, step 104, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #83, step 105, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #83, step 106, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #83, step 107, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #83, step 108, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #83, step 109, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #83, step 110, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #83, step 111, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #83, step 112, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #83, step 113, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #83, step 114, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #83, step 115, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #83, step 116, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #83, step 117, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #83, step 118, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #83, step 119, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #83, step 120, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #83, step 121, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #83, step 122, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #83, step 123, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #83, step 124, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #83, step 125, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #83, step 126, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #83, step 127, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #83, step 128, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #83, step 129, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #83, step 130, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #83, step 131, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #83, step 132, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #83, step 133, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #83, step 134, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #83, step 135, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #83, step 136, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #83, step 137, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #83, step 138, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #83, step 139, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #83, step 140, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #83, step 141, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #83, step 142, discriminator loss=0.684 , generator loss=0.703\n",
      "Training progress in epoch #83, step 143, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #83, step 144, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #83, step 145, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #83, step 146, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #83, step 147, discriminator loss=0.696 , generator loss=0.682\n",
      "Training progress in epoch #83, step 148, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #83, step 149, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #83, step 150, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #83, step 151, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #83, step 152, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #83, step 153, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #83, step 154, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #83, step 155, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #83, step 156, discriminator loss=0.686 , generator loss=0.684\n",
      "Training progress in epoch #83, step 157, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #83, step 158, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #83, step 159, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #83, step 160, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #83, step 161, discriminator loss=0.687 , generator loss=0.682\n",
      "Training progress in epoch #83, step 162, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #83, step 163, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #83, step 164, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #83, step 165, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #83, step 166, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #83, step 167, discriminator loss=0.694 , generator loss=0.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #83, step 168, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #83, step 169, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #83, step 170, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #83, step 171, discriminator loss=0.698 , generator loss=0.706\n",
      "Training progress in epoch #83, step 172, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #83, step 173, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #83, step 174, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #83, step 175, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #83, step 176, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #83, step 177, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #83, step 178, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #83, step 179, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #83, step 180, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #83, step 181, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #83, step 182, discriminator loss=0.696 , generator loss=0.720\n",
      "Training progress in epoch #83, step 183, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #83, step 184, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #83, step 185, discriminator loss=0.696 , generator loss=0.686\n",
      "Training progress in epoch #83, step 186, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #83, step 187, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #83, step 188, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #83, step 189, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #83, step 190, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #83, step 191, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #83, step 192, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #83, step 193, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #83, step 194, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #83, step 195, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #83, step 196, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #83, step 197, discriminator loss=0.685 , generator loss=0.687\n",
      "Training progress in epoch #83, step 198, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #83, step 199, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #83, step 200, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #83, step 201, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #83, step 202, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #83, step 203, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #83, step 204, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #83, step 205, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #83, step 206, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #83, step 207, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #83, step 208, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #83, step 209, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #83, step 210, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #83, step 211, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #83, step 212, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #83, step 213, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #83, step 214, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #83, step 215, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #83, step 216, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #83, step 217, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #83, step 218, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #83, step 219, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #83, step 220, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #83, step 221, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #83, step 222, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #83, step 223, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #83, step 224, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #83, step 225, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #83, step 226, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #83, step 227, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #83, step 228, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #83, step 229, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #83, step 230, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #83, step 231, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #83, step 232, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #83, step 233, discriminator loss=0.693 , generator loss=0.678\n",
      "Disciminator Accuracy on real images: 83%, on fake images: 39%\n",
      "Training progress in epoch #84, step 0, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #84, step 1, discriminator loss=0.696 , generator loss=0.713\n",
      "Training progress in epoch #84, step 2, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #84, step 3, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #84, step 4, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #84, step 5, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #84, step 6, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #84, step 7, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #84, step 8, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #84, step 9, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #84, step 10, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #84, step 11, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #84, step 12, discriminator loss=0.700 , generator loss=0.704\n",
      "Training progress in epoch #84, step 13, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #84, step 14, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #84, step 15, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #84, step 16, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #84, step 17, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #84, step 18, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #84, step 19, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #84, step 20, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #84, step 21, discriminator loss=0.696 , generator loss=0.718\n",
      "Training progress in epoch #84, step 22, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #84, step 23, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #84, step 24, discriminator loss=0.690 , generator loss=0.695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #84, step 25, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #84, step 26, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #84, step 27, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #84, step 28, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #84, step 29, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #84, step 30, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #84, step 31, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #84, step 32, discriminator loss=0.695 , generator loss=0.723\n",
      "Training progress in epoch #84, step 33, discriminator loss=0.687 , generator loss=0.713\n",
      "Training progress in epoch #84, step 34, discriminator loss=0.696 , generator loss=0.682\n",
      "Training progress in epoch #84, step 35, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #84, step 36, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #84, step 37, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #84, step 38, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #84, step 39, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #84, step 40, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #84, step 41, discriminator loss=0.692 , generator loss=0.675\n",
      "Training progress in epoch #84, step 42, discriminator loss=0.694 , generator loss=0.679\n",
      "Training progress in epoch #84, step 43, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #84, step 44, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #84, step 45, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #84, step 46, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #84, step 47, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #84, step 48, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #84, step 49, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #84, step 50, discriminator loss=0.695 , generator loss=0.689\n",
      "Training progress in epoch #84, step 51, discriminator loss=0.689 , generator loss=0.677\n",
      "Training progress in epoch #84, step 52, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #84, step 53, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #84, step 54, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #84, step 55, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #84, step 56, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #84, step 57, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #84, step 58, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #84, step 59, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #84, step 60, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #84, step 61, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #84, step 62, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #84, step 63, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #84, step 64, discriminator loss=0.698 , generator loss=0.690\n",
      "Training progress in epoch #84, step 65, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #84, step 66, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #84, step 67, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #84, step 68, discriminator loss=0.689 , generator loss=0.725\n",
      "Training progress in epoch #84, step 69, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #84, step 70, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #84, step 71, discriminator loss=0.696 , generator loss=0.679\n",
      "Training progress in epoch #84, step 72, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #84, step 73, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #84, step 74, discriminator loss=0.696 , generator loss=0.707\n",
      "Training progress in epoch #84, step 75, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #84, step 76, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #84, step 77, discriminator loss=0.684 , generator loss=0.683\n",
      "Training progress in epoch #84, step 78, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #84, step 79, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #84, step 80, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #84, step 81, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #84, step 82, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #84, step 83, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #84, step 84, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #84, step 85, discriminator loss=0.698 , generator loss=0.700\n",
      "Training progress in epoch #84, step 86, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #84, step 87, discriminator loss=0.698 , generator loss=0.715\n",
      "Training progress in epoch #84, step 88, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #84, step 89, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #84, step 90, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #84, step 91, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #84, step 92, discriminator loss=0.696 , generator loss=0.694\n",
      "Training progress in epoch #84, step 93, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #84, step 94, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #84, step 95, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #84, step 96, discriminator loss=0.686 , generator loss=0.717\n",
      "Training progress in epoch #84, step 97, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #84, step 98, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #84, step 99, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #84, step 100, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #84, step 101, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #84, step 102, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #84, step 103, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #84, step 104, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #84, step 105, discriminator loss=0.697 , generator loss=0.693\n",
      "Training progress in epoch #84, step 106, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #84, step 107, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #84, step 108, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #84, step 109, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #84, step 110, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #84, step 111, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #84, step 112, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #84, step 113, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #84, step 114, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #84, step 115, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #84, step 116, discriminator loss=0.692 , generator loss=0.694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #84, step 117, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #84, step 118, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #84, step 119, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #84, step 120, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #84, step 121, discriminator loss=0.688 , generator loss=0.680\n",
      "Training progress in epoch #84, step 122, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #84, step 123, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #84, step 124, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #84, step 125, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #84, step 126, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #84, step 127, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #84, step 128, discriminator loss=0.695 , generator loss=0.673\n",
      "Training progress in epoch #84, step 129, discriminator loss=0.697 , generator loss=0.677\n",
      "Training progress in epoch #84, step 130, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #84, step 131, discriminator loss=0.693 , generator loss=0.732\n",
      "Training progress in epoch #84, step 132, discriminator loss=0.696 , generator loss=0.742\n",
      "Training progress in epoch #84, step 133, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #84, step 134, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #84, step 135, discriminator loss=0.695 , generator loss=0.675\n",
      "Training progress in epoch #84, step 136, discriminator loss=0.687 , generator loss=0.680\n",
      "Training progress in epoch #84, step 137, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #84, step 138, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #84, step 139, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #84, step 140, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #84, step 141, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #84, step 142, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #84, step 143, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #84, step 144, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #84, step 145, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #84, step 146, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #84, step 147, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #84, step 148, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #84, step 149, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #84, step 150, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #84, step 151, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #84, step 152, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #84, step 153, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #84, step 154, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #84, step 155, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #84, step 156, discriminator loss=0.696 , generator loss=0.707\n",
      "Training progress in epoch #84, step 157, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #84, step 158, discriminator loss=0.687 , generator loss=0.707\n",
      "Training progress in epoch #84, step 159, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #84, step 160, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #84, step 161, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #84, step 162, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #84, step 163, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #84, step 164, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #84, step 165, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #84, step 166, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #84, step 167, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #84, step 168, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #84, step 169, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #84, step 170, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #84, step 171, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #84, step 172, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #84, step 173, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #84, step 174, discriminator loss=0.686 , generator loss=0.708\n",
      "Training progress in epoch #84, step 175, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #84, step 176, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #84, step 177, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #84, step 178, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #84, step 179, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #84, step 180, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #84, step 181, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #84, step 182, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #84, step 183, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #84, step 184, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #84, step 185, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #84, step 186, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #84, step 187, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #84, step 188, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #84, step 189, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #84, step 190, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #84, step 191, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #84, step 192, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #84, step 193, discriminator loss=0.686 , generator loss=0.686\n",
      "Training progress in epoch #84, step 194, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #84, step 195, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #84, step 196, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #84, step 197, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #84, step 198, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #84, step 199, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #84, step 200, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #84, step 201, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #84, step 202, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #84, step 203, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #84, step 204, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #84, step 205, discriminator loss=0.687 , generator loss=0.720\n",
      "Training progress in epoch #84, step 206, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #84, step 207, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #84, step 208, discriminator loss=0.689 , generator loss=0.674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #84, step 209, discriminator loss=0.688 , generator loss=0.674\n",
      "Training progress in epoch #84, step 210, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #84, step 211, discriminator loss=0.692 , generator loss=0.723\n",
      "Training progress in epoch #84, step 212, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #84, step 213, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #84, step 214, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #84, step 215, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #84, step 216, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #84, step 217, discriminator loss=0.696 , generator loss=0.686\n",
      "Training progress in epoch #84, step 218, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #84, step 219, discriminator loss=0.685 , generator loss=0.688\n",
      "Training progress in epoch #84, step 220, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #84, step 221, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #84, step 222, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #84, step 223, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #84, step 224, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #84, step 225, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #84, step 226, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #84, step 227, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #84, step 228, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #84, step 229, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #84, step 230, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #84, step 231, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #84, step 232, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #84, step 233, discriminator loss=0.696 , generator loss=0.690\n",
      "Disciminator Accuracy on real images: 64%, on fake images: 69%\n",
      "Training progress in epoch #85, step 0, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #85, step 1, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #85, step 2, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #85, step 3, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #85, step 4, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #85, step 5, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #85, step 6, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #85, step 7, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #85, step 8, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #85, step 9, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #85, step 10, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #85, step 11, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #85, step 12, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #85, step 13, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #85, step 14, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #85, step 15, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #85, step 16, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #85, step 17, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #85, step 18, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #85, step 19, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #85, step 20, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #85, step 21, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #85, step 22, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #85, step 23, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #85, step 24, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #85, step 25, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #85, step 26, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #85, step 27, discriminator loss=0.697 , generator loss=0.717\n",
      "Training progress in epoch #85, step 28, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #85, step 29, discriminator loss=0.699 , generator loss=0.697\n",
      "Training progress in epoch #85, step 30, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #85, step 31, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #85, step 32, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #85, step 33, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #85, step 34, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #85, step 35, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #85, step 36, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #85, step 37, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #85, step 38, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #85, step 39, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #85, step 40, discriminator loss=0.696 , generator loss=0.684\n",
      "Training progress in epoch #85, step 41, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #85, step 42, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #85, step 43, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #85, step 44, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #85, step 45, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #85, step 46, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #85, step 47, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #85, step 48, discriminator loss=0.686 , generator loss=0.678\n",
      "Training progress in epoch #85, step 49, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #85, step 50, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #85, step 51, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #85, step 52, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #85, step 53, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #85, step 54, discriminator loss=0.691 , generator loss=0.674\n",
      "Training progress in epoch #85, step 55, discriminator loss=0.696 , generator loss=0.686\n",
      "Training progress in epoch #85, step 56, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #85, step 57, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #85, step 58, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #85, step 59, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #85, step 60, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #85, step 61, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #85, step 62, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #85, step 63, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #85, step 64, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #85, step 65, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #85, step 66, discriminator loss=0.689 , generator loss=0.678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #85, step 67, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #85, step 68, discriminator loss=0.694 , generator loss=0.721\n",
      "Training progress in epoch #85, step 69, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #85, step 70, discriminator loss=0.699 , generator loss=0.723\n",
      "Training progress in epoch #85, step 71, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #85, step 72, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #85, step 73, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #85, step 74, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #85, step 75, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #85, step 76, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #85, step 77, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #85, step 78, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #85, step 79, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #85, step 80, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #85, step 81, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #85, step 82, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #85, step 83, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #85, step 84, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #85, step 85, discriminator loss=0.686 , generator loss=0.685\n",
      "Training progress in epoch #85, step 86, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #85, step 87, discriminator loss=0.688 , generator loss=0.718\n",
      "Training progress in epoch #85, step 88, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #85, step 89, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #85, step 90, discriminator loss=0.689 , generator loss=0.676\n",
      "Training progress in epoch #85, step 91, discriminator loss=0.693 , generator loss=0.674\n",
      "Training progress in epoch #85, step 92, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #85, step 93, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #85, step 94, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #85, step 95, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #85, step 96, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #85, step 97, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #85, step 98, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #85, step 99, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #85, step 100, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #85, step 101, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #85, step 102, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #85, step 103, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #85, step 104, discriminator loss=0.697 , generator loss=0.691\n",
      "Training progress in epoch #85, step 105, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #85, step 106, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #85, step 107, discriminator loss=0.690 , generator loss=0.729\n",
      "Training progress in epoch #85, step 108, discriminator loss=0.690 , generator loss=0.743\n",
      "Training progress in epoch #85, step 109, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #85, step 110, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #85, step 111, discriminator loss=0.692 , generator loss=0.666\n",
      "Training progress in epoch #85, step 112, discriminator loss=0.693 , generator loss=0.672\n",
      "Training progress in epoch #85, step 113, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #85, step 114, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #85, step 115, discriminator loss=0.687 , generator loss=0.735\n",
      "Training progress in epoch #85, step 116, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #85, step 117, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #85, step 118, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #85, step 119, discriminator loss=0.699 , generator loss=0.682\n",
      "Training progress in epoch #85, step 120, discriminator loss=0.697 , generator loss=0.694\n",
      "Training progress in epoch #85, step 121, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #85, step 122, discriminator loss=0.700 , generator loss=0.673\n",
      "Training progress in epoch #85, step 123, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #85, step 124, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #85, step 125, discriminator loss=0.698 , generator loss=0.730\n",
      "Training progress in epoch #85, step 126, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #85, step 127, discriminator loss=0.687 , generator loss=0.729\n",
      "Training progress in epoch #85, step 128, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #85, step 129, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #85, step 130, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #85, step 131, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #85, step 132, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #85, step 133, discriminator loss=0.690 , generator loss=0.676\n",
      "Training progress in epoch #85, step 134, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #85, step 135, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #85, step 136, discriminator loss=0.686 , generator loss=0.724\n",
      "Training progress in epoch #85, step 137, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #85, step 138, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #85, step 139, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #85, step 140, discriminator loss=0.698 , generator loss=0.678\n",
      "Training progress in epoch #85, step 141, discriminator loss=0.699 , generator loss=0.668\n",
      "Training progress in epoch #85, step 142, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #85, step 143, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #85, step 144, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #85, step 145, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #85, step 146, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #85, step 147, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #85, step 148, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #85, step 149, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #85, step 150, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #85, step 151, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #85, step 152, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #85, step 153, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #85, step 154, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #85, step 155, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #85, step 156, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #85, step 157, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #85, step 158, discriminator loss=0.695 , generator loss=0.688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #85, step 159, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #85, step 160, discriminator loss=0.695 , generator loss=0.727\n",
      "Training progress in epoch #85, step 161, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #85, step 162, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #85, step 163, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #85, step 164, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #85, step 165, discriminator loss=0.694 , generator loss=0.672\n",
      "Training progress in epoch #85, step 166, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #85, step 167, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #85, step 168, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #85, step 169, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #85, step 170, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #85, step 171, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #85, step 172, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #85, step 173, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #85, step 174, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #85, step 175, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #85, step 176, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #85, step 177, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #85, step 178, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #85, step 179, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #85, step 180, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #85, step 181, discriminator loss=0.697 , generator loss=0.706\n",
      "Training progress in epoch #85, step 182, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #85, step 183, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #85, step 184, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #85, step 185, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #85, step 186, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #85, step 187, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #85, step 188, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #85, step 189, discriminator loss=0.688 , generator loss=0.721\n",
      "Training progress in epoch #85, step 190, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #85, step 191, discriminator loss=0.698 , generator loss=0.704\n",
      "Training progress in epoch #85, step 192, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #85, step 193, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #85, step 194, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #85, step 195, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #85, step 196, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #85, step 197, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #85, step 198, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #85, step 199, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #85, step 200, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #85, step 201, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #85, step 202, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #85, step 203, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #85, step 204, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #85, step 205, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #85, step 206, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #85, step 207, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #85, step 208, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #85, step 209, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #85, step 210, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #85, step 211, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #85, step 212, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #85, step 213, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #85, step 214, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #85, step 215, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #85, step 216, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #85, step 217, discriminator loss=0.696 , generator loss=0.706\n",
      "Training progress in epoch #85, step 218, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #85, step 219, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #85, step 220, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #85, step 221, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #85, step 222, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #85, step 223, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #85, step 224, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #85, step 225, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #85, step 226, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #85, step 227, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #85, step 228, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #85, step 229, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #85, step 230, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #85, step 231, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #85, step 232, discriminator loss=0.698 , generator loss=0.704\n",
      "Training progress in epoch #85, step 233, discriminator loss=0.687 , generator loss=0.706\n",
      "Disciminator Accuracy on real images: 50%, on fake images: 66%\n",
      "Training progress in epoch #86, step 0, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #86, step 1, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #86, step 2, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #86, step 3, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #86, step 4, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #86, step 5, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #86, step 6, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #86, step 7, discriminator loss=0.687 , generator loss=0.675\n",
      "Training progress in epoch #86, step 8, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #86, step 9, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #86, step 10, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #86, step 11, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #86, step 12, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #86, step 13, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #86, step 14, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #86, step 15, discriminator loss=0.692 , generator loss=0.714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #86, step 16, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #86, step 17, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #86, step 18, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #86, step 19, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #86, step 20, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #86, step 21, discriminator loss=0.692 , generator loss=0.675\n",
      "Training progress in epoch #86, step 22, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #86, step 23, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #86, step 24, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #86, step 25, discriminator loss=0.697 , generator loss=0.707\n",
      "Training progress in epoch #86, step 26, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #86, step 27, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #86, step 28, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #86, step 29, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #86, step 30, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #86, step 31, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #86, step 32, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #86, step 33, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #86, step 34, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #86, step 35, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #86, step 36, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #86, step 37, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #86, step 38, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #86, step 39, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #86, step 40, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #86, step 41, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #86, step 42, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #86, step 43, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #86, step 44, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #86, step 45, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #86, step 46, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #86, step 47, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #86, step 48, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #86, step 49, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #86, step 50, discriminator loss=0.685 , generator loss=0.690\n",
      "Training progress in epoch #86, step 51, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #86, step 52, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #86, step 53, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #86, step 54, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #86, step 55, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #86, step 56, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #86, step 57, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #86, step 58, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #86, step 59, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #86, step 60, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #86, step 61, discriminator loss=0.696 , generator loss=0.707\n",
      "Training progress in epoch #86, step 62, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #86, step 63, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #86, step 64, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #86, step 65, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #86, step 66, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #86, step 67, discriminator loss=0.697 , generator loss=0.701\n",
      "Training progress in epoch #86, step 68, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #86, step 69, discriminator loss=0.697 , generator loss=0.670\n",
      "Training progress in epoch #86, step 70, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #86, step 71, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #86, step 72, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #86, step 73, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #86, step 74, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #86, step 75, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #86, step 76, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #86, step 77, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #86, step 78, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #86, step 79, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #86, step 80, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #86, step 81, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #86, step 82, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #86, step 83, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #86, step 84, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #86, step 85, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #86, step 86, discriminator loss=0.696 , generator loss=0.713\n",
      "Training progress in epoch #86, step 87, discriminator loss=0.693 , generator loss=0.721\n",
      "Training progress in epoch #86, step 88, discriminator loss=0.691 , generator loss=0.734\n",
      "Training progress in epoch #86, step 89, discriminator loss=0.697 , generator loss=0.728\n",
      "Training progress in epoch #86, step 90, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #86, step 91, discriminator loss=0.698 , generator loss=0.682\n",
      "Training progress in epoch #86, step 92, discriminator loss=0.688 , generator loss=0.675\n",
      "Training progress in epoch #86, step 93, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #86, step 94, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #86, step 95, discriminator loss=0.697 , generator loss=0.706\n",
      "Training progress in epoch #86, step 96, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #86, step 97, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #86, step 98, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #86, step 99, discriminator loss=0.697 , generator loss=0.705\n",
      "Training progress in epoch #86, step 100, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #86, step 101, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #86, step 102, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #86, step 103, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #86, step 104, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #86, step 105, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #86, step 106, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #86, step 107, discriminator loss=0.692 , generator loss=0.712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #86, step 108, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #86, step 109, discriminator loss=0.692 , generator loss=0.724\n",
      "Training progress in epoch #86, step 110, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #86, step 111, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #86, step 112, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #86, step 113, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #86, step 114, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #86, step 115, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #86, step 116, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #86, step 117, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #86, step 118, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #86, step 119, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #86, step 120, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #86, step 121, discriminator loss=0.682 , generator loss=0.702\n",
      "Training progress in epoch #86, step 122, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #86, step 123, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #86, step 124, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #86, step 125, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #86, step 126, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #86, step 127, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #86, step 128, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #86, step 129, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #86, step 130, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #86, step 131, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #86, step 132, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #86, step 133, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #86, step 134, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #86, step 135, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #86, step 136, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #86, step 137, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #86, step 138, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #86, step 139, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #86, step 140, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #86, step 141, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #86, step 142, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #86, step 143, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #86, step 144, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #86, step 145, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #86, step 146, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #86, step 147, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #86, step 148, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #86, step 149, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #86, step 150, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #86, step 151, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #86, step 152, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #86, step 153, discriminator loss=0.694 , generator loss=0.677\n",
      "Training progress in epoch #86, step 154, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #86, step 155, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #86, step 156, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #86, step 157, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #86, step 158, discriminator loss=0.693 , generator loss=0.732\n",
      "Training progress in epoch #86, step 159, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #86, step 160, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #86, step 161, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #86, step 162, discriminator loss=0.690 , generator loss=0.677\n",
      "Training progress in epoch #86, step 163, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #86, step 164, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #86, step 165, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #86, step 166, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #86, step 167, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #86, step 168, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #86, step 169, discriminator loss=0.698 , generator loss=0.692\n",
      "Training progress in epoch #86, step 170, discriminator loss=0.697 , generator loss=0.708\n",
      "Training progress in epoch #86, step 171, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #86, step 172, discriminator loss=0.686 , generator loss=0.720\n",
      "Training progress in epoch #86, step 173, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #86, step 174, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #86, step 175, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #86, step 176, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #86, step 177, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #86, step 178, discriminator loss=0.686 , generator loss=0.718\n",
      "Training progress in epoch #86, step 179, discriminator loss=0.687 , generator loss=0.719\n",
      "Training progress in epoch #86, step 180, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #86, step 181, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #86, step 182, discriminator loss=0.687 , generator loss=0.685\n",
      "Training progress in epoch #86, step 183, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #86, step 184, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #86, step 185, discriminator loss=0.696 , generator loss=0.718\n",
      "Training progress in epoch #86, step 186, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #86, step 187, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #86, step 188, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #86, step 189, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #86, step 190, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #86, step 191, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #86, step 192, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #86, step 193, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #86, step 194, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #86, step 195, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #86, step 196, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #86, step 197, discriminator loss=0.696 , generator loss=0.690\n",
      "Training progress in epoch #86, step 198, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #86, step 199, discriminator loss=0.687 , generator loss=0.716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #86, step 200, discriminator loss=0.696 , generator loss=0.713\n",
      "Training progress in epoch #86, step 201, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #86, step 202, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #86, step 203, discriminator loss=0.696 , generator loss=0.694\n",
      "Training progress in epoch #86, step 204, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #86, step 205, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #86, step 206, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #86, step 207, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #86, step 208, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #86, step 209, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #86, step 210, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #86, step 211, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #86, step 212, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #86, step 213, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #86, step 214, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #86, step 215, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #86, step 216, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #86, step 217, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #86, step 218, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #86, step 219, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #86, step 220, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #86, step 221, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #86, step 222, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #86, step 223, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #86, step 224, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #86, step 225, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #86, step 226, discriminator loss=0.691 , generator loss=0.737\n",
      "Training progress in epoch #86, step 227, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #86, step 228, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #86, step 229, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #86, step 230, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #86, step 231, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #86, step 232, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #86, step 233, discriminator loss=0.691 , generator loss=0.710\n",
      "Disciminator Accuracy on real images: 43%, on fake images: 79%\n",
      "Training progress in epoch #87, step 0, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #87, step 1, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #87, step 2, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #87, step 3, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #87, step 4, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #87, step 5, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #87, step 6, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #87, step 7, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #87, step 8, discriminator loss=0.685 , generator loss=0.703\n",
      "Training progress in epoch #87, step 9, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #87, step 10, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #87, step 11, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #87, step 12, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #87, step 13, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #87, step 14, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #87, step 15, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #87, step 16, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #87, step 17, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #87, step 18, discriminator loss=0.684 , generator loss=0.711\n",
      "Training progress in epoch #87, step 19, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #87, step 20, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #87, step 21, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #87, step 22, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #87, step 23, discriminator loss=0.684 , generator loss=0.697\n",
      "Training progress in epoch #87, step 24, discriminator loss=0.696 , generator loss=0.717\n",
      "Training progress in epoch #87, step 25, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #87, step 26, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #87, step 27, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #87, step 28, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #87, step 29, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #87, step 30, discriminator loss=0.696 , generator loss=0.680\n",
      "Training progress in epoch #87, step 31, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #87, step 32, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #87, step 33, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #87, step 34, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #87, step 35, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #87, step 36, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #87, step 37, discriminator loss=0.696 , generator loss=0.690\n",
      "Training progress in epoch #87, step 38, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #87, step 39, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #87, step 40, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #87, step 41, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #87, step 42, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #87, step 43, discriminator loss=0.697 , generator loss=0.714\n",
      "Training progress in epoch #87, step 44, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #87, step 45, discriminator loss=0.685 , generator loss=0.714\n",
      "Training progress in epoch #87, step 46, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #87, step 47, discriminator loss=0.688 , generator loss=0.678\n",
      "Training progress in epoch #87, step 48, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #87, step 49, discriminator loss=0.683 , generator loss=0.704\n",
      "Training progress in epoch #87, step 50, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #87, step 51, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #87, step 52, discriminator loss=0.691 , generator loss=0.742\n",
      "Training progress in epoch #87, step 53, discriminator loss=0.692 , generator loss=0.735\n",
      "Training progress in epoch #87, step 54, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #87, step 55, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #87, step 56, discriminator loss=0.694 , generator loss=0.670\n",
      "Training progress in epoch #87, step 57, discriminator loss=0.697 , generator loss=0.682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #87, step 58, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #87, step 59, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #87, step 60, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #87, step 61, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #87, step 62, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #87, step 63, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #87, step 64, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #87, step 65, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #87, step 66, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #87, step 67, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #87, step 68, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #87, step 69, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #87, step 70, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #87, step 71, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #87, step 72, discriminator loss=0.686 , generator loss=0.716\n",
      "Training progress in epoch #87, step 73, discriminator loss=0.697 , generator loss=0.718\n",
      "Training progress in epoch #87, step 74, discriminator loss=0.700 , generator loss=0.693\n",
      "Training progress in epoch #87, step 75, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #87, step 76, discriminator loss=0.689 , generator loss=0.674\n",
      "Training progress in epoch #87, step 77, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #87, step 78, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #87, step 79, discriminator loss=0.693 , generator loss=0.729\n",
      "Training progress in epoch #87, step 80, discriminator loss=0.696 , generator loss=0.724\n",
      "Training progress in epoch #87, step 81, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #87, step 82, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #87, step 83, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #87, step 84, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #87, step 85, discriminator loss=0.694 , generator loss=0.677\n",
      "Training progress in epoch #87, step 86, discriminator loss=0.690 , generator loss=0.673\n",
      "Training progress in epoch #87, step 87, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #87, step 88, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #87, step 89, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #87, step 90, discriminator loss=0.690 , generator loss=0.730\n",
      "Training progress in epoch #87, step 91, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #87, step 92, discriminator loss=0.697 , generator loss=0.699\n",
      "Training progress in epoch #87, step 93, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #87, step 94, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #87, step 95, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #87, step 96, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #87, step 97, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #87, step 98, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #87, step 99, discriminator loss=0.696 , generator loss=0.684\n",
      "Training progress in epoch #87, step 100, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #87, step 101, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #87, step 102, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #87, step 103, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #87, step 104, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #87, step 105, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #87, step 106, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #87, step 107, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #87, step 108, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #87, step 109, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #87, step 110, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #87, step 111, discriminator loss=0.697 , generator loss=0.688\n",
      "Training progress in epoch #87, step 112, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #87, step 113, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #87, step 114, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #87, step 115, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #87, step 116, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #87, step 117, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #87, step 118, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #87, step 119, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #87, step 120, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #87, step 121, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #87, step 122, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #87, step 123, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #87, step 124, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #87, step 125, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #87, step 126, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #87, step 127, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #87, step 128, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #87, step 129, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #87, step 130, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #87, step 131, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #87, step 132, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #87, step 133, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #87, step 134, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #87, step 135, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #87, step 136, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #87, step 137, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #87, step 138, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #87, step 139, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #87, step 140, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #87, step 141, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #87, step 142, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #87, step 143, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #87, step 144, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #87, step 145, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #87, step 146, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #87, step 147, discriminator loss=0.695 , generator loss=0.681\n",
      "Training progress in epoch #87, step 148, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #87, step 149, discriminator loss=0.689 , generator loss=0.689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #87, step 150, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #87, step 151, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #87, step 152, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #87, step 153, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #87, step 154, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #87, step 155, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #87, step 156, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #87, step 157, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #87, step 158, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #87, step 159, discriminator loss=0.698 , generator loss=0.690\n",
      "Training progress in epoch #87, step 160, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #87, step 161, discriminator loss=0.697 , generator loss=0.718\n",
      "Training progress in epoch #87, step 162, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #87, step 163, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #87, step 164, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #87, step 165, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #87, step 166, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #87, step 167, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #87, step 168, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #87, step 169, discriminator loss=0.684 , generator loss=0.694\n",
      "Training progress in epoch #87, step 170, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #87, step 171, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #87, step 172, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #87, step 173, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #87, step 174, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #87, step 175, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #87, step 176, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #87, step 177, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #87, step 178, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #87, step 179, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #87, step 180, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #87, step 181, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #87, step 182, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #87, step 183, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #87, step 184, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #87, step 185, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #87, step 186, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #87, step 187, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #87, step 188, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #87, step 189, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #87, step 190, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #87, step 191, discriminator loss=0.684 , generator loss=0.691\n",
      "Training progress in epoch #87, step 192, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #87, step 193, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #87, step 194, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #87, step 195, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #87, step 196, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #87, step 197, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #87, step 198, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #87, step 199, discriminator loss=0.697 , generator loss=0.700\n",
      "Training progress in epoch #87, step 200, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #87, step 201, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #87, step 202, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #87, step 203, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #87, step 204, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #87, step 205, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #87, step 206, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #87, step 207, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #87, step 208, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #87, step 209, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #87, step 210, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #87, step 211, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #87, step 212, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #87, step 213, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #87, step 214, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #87, step 215, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #87, step 216, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #87, step 217, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #87, step 218, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #87, step 219, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #87, step 220, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #87, step 221, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #87, step 222, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #87, step 223, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #87, step 224, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #87, step 225, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #87, step 226, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #87, step 227, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #87, step 228, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #87, step 229, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #87, step 230, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #87, step 231, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #87, step 232, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #87, step 233, discriminator loss=0.695 , generator loss=0.697\n",
      "Disciminator Accuracy on real images: 58%, on fake images: 68%\n",
      "Training progress in epoch #88, step 0, discriminator loss=0.693 , generator loss=0.676\n",
      "Training progress in epoch #88, step 1, discriminator loss=0.686 , generator loss=0.677\n",
      "Training progress in epoch #88, step 2, discriminator loss=0.684 , generator loss=0.686\n",
      "Training progress in epoch #88, step 3, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #88, step 4, discriminator loss=0.697 , generator loss=0.708\n",
      "Training progress in epoch #88, step 5, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #88, step 6, discriminator loss=0.690 , generator loss=0.697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #88, step 7, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #88, step 8, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #88, step 9, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #88, step 10, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #88, step 11, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #88, step 12, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #88, step 13, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #88, step 14, discriminator loss=0.687 , generator loss=0.676\n",
      "Training progress in epoch #88, step 15, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #88, step 16, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #88, step 17, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #88, step 18, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #88, step 19, discriminator loss=0.697 , generator loss=0.709\n",
      "Training progress in epoch #88, step 20, discriminator loss=0.685 , generator loss=0.722\n",
      "Training progress in epoch #88, step 21, discriminator loss=0.691 , generator loss=0.723\n",
      "Training progress in epoch #88, step 22, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #88, step 23, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #88, step 24, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #88, step 25, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #88, step 26, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #88, step 27, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #88, step 28, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #88, step 29, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #88, step 30, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #88, step 31, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #88, step 32, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #88, step 33, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #88, step 34, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #88, step 35, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #88, step 36, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #88, step 37, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #88, step 38, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #88, step 39, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #88, step 40, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #88, step 41, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #88, step 42, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #88, step 43, discriminator loss=0.688 , generator loss=0.678\n",
      "Training progress in epoch #88, step 44, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #88, step 45, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #88, step 46, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #88, step 47, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #88, step 48, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #88, step 49, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #88, step 50, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #88, step 51, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #88, step 52, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #88, step 53, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #88, step 54, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #88, step 55, discriminator loss=0.697 , generator loss=0.706\n",
      "Training progress in epoch #88, step 56, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #88, step 57, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #88, step 58, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #88, step 59, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #88, step 60, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #88, step 61, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #88, step 62, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #88, step 63, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #88, step 64, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #88, step 65, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #88, step 66, discriminator loss=0.696 , generator loss=0.716\n",
      "Training progress in epoch #88, step 67, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #88, step 68, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #88, step 69, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #88, step 70, discriminator loss=0.685 , generator loss=0.692\n",
      "Training progress in epoch #88, step 71, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #88, step 72, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #88, step 73, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #88, step 74, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #88, step 75, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #88, step 76, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #88, step 77, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #88, step 78, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #88, step 79, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #88, step 80, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #88, step 81, discriminator loss=0.691 , generator loss=0.742\n",
      "Training progress in epoch #88, step 82, discriminator loss=0.690 , generator loss=0.723\n",
      "Training progress in epoch #88, step 83, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #88, step 84, discriminator loss=0.696 , generator loss=0.689\n",
      "Training progress in epoch #88, step 85, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #88, step 86, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #88, step 87, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #88, step 88, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #88, step 89, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #88, step 90, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #88, step 91, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #88, step 92, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #88, step 93, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #88, step 94, discriminator loss=0.686 , generator loss=0.684\n",
      "Training progress in epoch #88, step 95, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #88, step 96, discriminator loss=0.695 , generator loss=0.674\n",
      "Training progress in epoch #88, step 97, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #88, step 98, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #88, step 99, discriminator loss=0.694 , generator loss=0.735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #88, step 100, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #88, step 101, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #88, step 102, discriminator loss=0.697 , generator loss=0.692\n",
      "Training progress in epoch #88, step 103, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #88, step 104, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #88, step 105, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #88, step 106, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #88, step 107, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #88, step 108, discriminator loss=0.694 , generator loss=0.726\n",
      "Training progress in epoch #88, step 109, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #88, step 110, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #88, step 111, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #88, step 112, discriminator loss=0.698 , generator loss=0.696\n",
      "Training progress in epoch #88, step 113, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #88, step 114, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #88, step 115, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #88, step 116, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #88, step 117, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #88, step 118, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #88, step 119, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #88, step 120, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #88, step 121, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #88, step 122, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #88, step 123, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #88, step 124, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #88, step 125, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #88, step 126, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #88, step 127, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #88, step 128, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #88, step 129, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #88, step 130, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #88, step 131, discriminator loss=0.690 , generator loss=0.678\n",
      "Training progress in epoch #88, step 132, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #88, step 133, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #88, step 134, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #88, step 135, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #88, step 136, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #88, step 137, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #88, step 138, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #88, step 139, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #88, step 140, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #88, step 141, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #88, step 142, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #88, step 143, discriminator loss=0.687 , generator loss=0.676\n",
      "Training progress in epoch #88, step 144, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #88, step 145, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #88, step 146, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #88, step 147, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #88, step 148, discriminator loss=0.697 , generator loss=0.715\n",
      "Training progress in epoch #88, step 149, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #88, step 150, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #88, step 151, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #88, step 152, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #88, step 153, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #88, step 154, discriminator loss=0.691 , generator loss=0.677\n",
      "Training progress in epoch #88, step 155, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #88, step 156, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #88, step 157, discriminator loss=0.697 , generator loss=0.721\n",
      "Training progress in epoch #88, step 158, discriminator loss=0.696 , generator loss=0.722\n",
      "Training progress in epoch #88, step 159, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #88, step 160, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #88, step 161, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #88, step 162, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #88, step 163, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #88, step 164, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #88, step 165, discriminator loss=0.684 , generator loss=0.707\n",
      "Training progress in epoch #88, step 166, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #88, step 167, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #88, step 168, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #88, step 169, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #88, step 170, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #88, step 171, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #88, step 172, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #88, step 173, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #88, step 174, discriminator loss=0.694 , generator loss=0.671\n",
      "Training progress in epoch #88, step 175, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #88, step 176, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #88, step 177, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #88, step 178, discriminator loss=0.692 , generator loss=0.729\n",
      "Training progress in epoch #88, step 179, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #88, step 180, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #88, step 181, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #88, step 182, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #88, step 183, discriminator loss=0.695 , generator loss=0.674\n",
      "Training progress in epoch #88, step 184, discriminator loss=0.696 , generator loss=0.681\n",
      "Training progress in epoch #88, step 185, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #88, step 186, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #88, step 187, discriminator loss=0.688 , generator loss=0.723\n",
      "Training progress in epoch #88, step 188, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #88, step 189, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #88, step 190, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #88, step 191, discriminator loss=0.695 , generator loss=0.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #88, step 192, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #88, step 193, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #88, step 194, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #88, step 195, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #88, step 196, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #88, step 197, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #88, step 198, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #88, step 199, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #88, step 200, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #88, step 201, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #88, step 202, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #88, step 203, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #88, step 204, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #88, step 205, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #88, step 206, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #88, step 207, discriminator loss=0.698 , generator loss=0.704\n",
      "Training progress in epoch #88, step 208, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #88, step 209, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #88, step 210, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #88, step 211, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #88, step 212, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #88, step 213, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #88, step 214, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #88, step 215, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #88, step 216, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #88, step 217, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #88, step 218, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #88, step 219, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #88, step 220, discriminator loss=0.696 , generator loss=0.688\n",
      "Training progress in epoch #88, step 221, discriminator loss=0.697 , generator loss=0.686\n",
      "Training progress in epoch #88, step 222, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #88, step 223, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #88, step 224, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #88, step 225, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #88, step 226, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #88, step 227, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #88, step 228, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #88, step 229, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #88, step 230, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #88, step 231, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #88, step 232, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #88, step 233, discriminator loss=0.690 , generator loss=0.711\n",
      "Disciminator Accuracy on real images: 24%, on fake images: 86%\n",
      "Training progress in epoch #89, step 0, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #89, step 1, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #89, step 2, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #89, step 3, discriminator loss=0.687 , generator loss=0.682\n",
      "Training progress in epoch #89, step 4, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #89, step 5, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #89, step 6, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #89, step 7, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #89, step 8, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #89, step 9, discriminator loss=0.696 , generator loss=0.716\n",
      "Training progress in epoch #89, step 10, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #89, step 11, discriminator loss=0.694 , generator loss=0.673\n",
      "Training progress in epoch #89, step 12, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #89, step 13, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #89, step 14, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #89, step 15, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #89, step 16, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #89, step 17, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #89, step 18, discriminator loss=0.684 , generator loss=0.703\n",
      "Training progress in epoch #89, step 19, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #89, step 20, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #89, step 21, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #89, step 22, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #89, step 23, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #89, step 24, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #89, step 25, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #89, step 26, discriminator loss=0.692 , generator loss=0.710\n",
      "Training progress in epoch #89, step 27, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #89, step 28, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #89, step 29, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #89, step 30, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #89, step 31, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #89, step 32, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #89, step 33, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #89, step 34, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #89, step 35, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #89, step 36, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #89, step 37, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #89, step 38, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #89, step 39, discriminator loss=0.685 , generator loss=0.717\n",
      "Training progress in epoch #89, step 40, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #89, step 41, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #89, step 42, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #89, step 43, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #89, step 44, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #89, step 45, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #89, step 46, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #89, step 47, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #89, step 48, discriminator loss=0.689 , generator loss=0.710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #89, step 49, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #89, step 50, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #89, step 51, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #89, step 52, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #89, step 53, discriminator loss=0.690 , generator loss=0.679\n",
      "Training progress in epoch #89, step 54, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #89, step 55, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #89, step 56, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #89, step 57, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #89, step 58, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #89, step 59, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #89, step 60, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #89, step 61, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #89, step 62, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #89, step 63, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #89, step 64, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #89, step 65, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #89, step 66, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #89, step 67, discriminator loss=0.687 , generator loss=0.711\n",
      "Training progress in epoch #89, step 68, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #89, step 69, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #89, step 70, discriminator loss=0.694 , generator loss=0.670\n",
      "Training progress in epoch #89, step 71, discriminator loss=0.691 , generator loss=0.672\n",
      "Training progress in epoch #89, step 72, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #89, step 73, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #89, step 74, discriminator loss=0.692 , generator loss=0.727\n",
      "Training progress in epoch #89, step 75, discriminator loss=0.697 , generator loss=0.731\n",
      "Training progress in epoch #89, step 76, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #89, step 77, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #89, step 78, discriminator loss=0.685 , generator loss=0.691\n",
      "Training progress in epoch #89, step 79, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #89, step 80, discriminator loss=0.686 , generator loss=0.676\n",
      "Training progress in epoch #89, step 81, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #89, step 82, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #89, step 83, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #89, step 84, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #89, step 85, discriminator loss=0.689 , generator loss=0.720\n",
      "Training progress in epoch #89, step 86, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #89, step 87, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #89, step 88, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #89, step 89, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #89, step 90, discriminator loss=0.688 , generator loss=0.679\n",
      "Training progress in epoch #89, step 91, discriminator loss=0.696 , generator loss=0.686\n",
      "Training progress in epoch #89, step 92, discriminator loss=0.697 , generator loss=0.725\n",
      "Training progress in epoch #89, step 93, discriminator loss=0.693 , generator loss=0.726\n",
      "Training progress in epoch #89, step 94, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #89, step 95, discriminator loss=0.699 , generator loss=0.709\n",
      "Training progress in epoch #89, step 96, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #89, step 97, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #89, step 98, discriminator loss=0.693 , generator loss=0.678\n",
      "Training progress in epoch #89, step 99, discriminator loss=0.703 , generator loss=0.687\n",
      "Training progress in epoch #89, step 100, discriminator loss=0.698 , generator loss=0.718\n",
      "Training progress in epoch #89, step 101, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #89, step 102, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #89, step 103, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #89, step 104, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #89, step 105, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #89, step 106, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #89, step 107, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #89, step 108, discriminator loss=0.688 , generator loss=0.679\n",
      "Training progress in epoch #89, step 109, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #89, step 110, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #89, step 111, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #89, step 112, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #89, step 113, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #89, step 114, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #89, step 115, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #89, step 116, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #89, step 117, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #89, step 118, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #89, step 119, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #89, step 120, discriminator loss=0.688 , generator loss=0.687\n",
      "Training progress in epoch #89, step 121, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #89, step 122, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #89, step 123, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #89, step 124, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #89, step 125, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #89, step 126, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #89, step 127, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #89, step 128, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #89, step 129, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #89, step 130, discriminator loss=0.695 , generator loss=0.683\n",
      "Training progress in epoch #89, step 131, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #89, step 132, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #89, step 133, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #89, step 134, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #89, step 135, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #89, step 136, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #89, step 137, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #89, step 138, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #89, step 139, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #89, step 140, discriminator loss=0.693 , generator loss=0.691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #89, step 141, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #89, step 142, discriminator loss=0.696 , generator loss=0.686\n",
      "Training progress in epoch #89, step 143, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #89, step 144, discriminator loss=0.696 , generator loss=0.692\n",
      "Training progress in epoch #89, step 145, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #89, step 146, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #89, step 147, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #89, step 148, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #89, step 149, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #89, step 150, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #89, step 151, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #89, step 152, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #89, step 153, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #89, step 154, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #89, step 155, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #89, step 156, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #89, step 157, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #89, step 158, discriminator loss=0.687 , generator loss=0.682\n",
      "Training progress in epoch #89, step 159, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #89, step 160, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #89, step 161, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #89, step 162, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #89, step 163, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #89, step 164, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #89, step 165, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #89, step 166, discriminator loss=0.684 , generator loss=0.703\n",
      "Training progress in epoch #89, step 167, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #89, step 168, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #89, step 169, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #89, step 170, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #89, step 171, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #89, step 172, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #89, step 173, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #89, step 174, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #89, step 175, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #89, step 176, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #89, step 177, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #89, step 178, discriminator loss=0.699 , generator loss=0.687\n",
      "Training progress in epoch #89, step 179, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #89, step 180, discriminator loss=0.698 , generator loss=0.679\n",
      "Training progress in epoch #89, step 181, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #89, step 182, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #89, step 183, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #89, step 184, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #89, step 185, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #89, step 186, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #89, step 187, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #89, step 188, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #89, step 189, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #89, step 190, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #89, step 191, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #89, step 192, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #89, step 193, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #89, step 194, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #89, step 195, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #89, step 196, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #89, step 197, discriminator loss=0.698 , generator loss=0.722\n",
      "Training progress in epoch #89, step 198, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #89, step 199, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #89, step 200, discriminator loss=0.699 , generator loss=0.699\n",
      "Training progress in epoch #89, step 201, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #89, step 202, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #89, step 203, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #89, step 204, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #89, step 205, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #89, step 206, discriminator loss=0.688 , generator loss=0.715\n",
      "Training progress in epoch #89, step 207, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #89, step 208, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #89, step 209, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #89, step 210, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #89, step 211, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #89, step 212, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #89, step 213, discriminator loss=0.698 , generator loss=0.677\n",
      "Training progress in epoch #89, step 214, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #89, step 215, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #89, step 216, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #89, step 217, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #89, step 218, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #89, step 219, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #89, step 220, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #89, step 221, discriminator loss=0.693 , generator loss=0.678\n",
      "Training progress in epoch #89, step 222, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #89, step 223, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #89, step 224, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #89, step 225, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #89, step 226, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #89, step 227, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #89, step 228, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #89, step 229, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #89, step 230, discriminator loss=0.690 , generator loss=0.675\n",
      "Training progress in epoch #89, step 231, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #89, step 232, discriminator loss=0.688 , generator loss=0.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #89, step 233, discriminator loss=0.690 , generator loss=0.716\n",
      "Disciminator Accuracy on real images: 26%, on fake images: 92%\n",
      "Training progress in epoch #90, step 0, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #90, step 1, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #90, step 2, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #90, step 3, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #90, step 4, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #90, step 5, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #90, step 6, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #90, step 7, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #90, step 8, discriminator loss=0.695 , generator loss=0.682\n",
      "Training progress in epoch #90, step 9, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #90, step 10, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #90, step 11, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #90, step 12, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #90, step 13, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #90, step 14, discriminator loss=0.686 , generator loss=0.713\n",
      "Training progress in epoch #90, step 15, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #90, step 16, discriminator loss=0.695 , generator loss=0.683\n",
      "Training progress in epoch #90, step 17, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #90, step 18, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #90, step 19, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #90, step 20, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #90, step 21, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #90, step 22, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #90, step 23, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #90, step 24, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #90, step 25, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #90, step 26, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #90, step 27, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #90, step 28, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #90, step 29, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #90, step 30, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #90, step 31, discriminator loss=0.696 , generator loss=0.702\n",
      "Training progress in epoch #90, step 32, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #90, step 33, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #90, step 34, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #90, step 35, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #90, step 36, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #90, step 37, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #90, step 38, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #90, step 39, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #90, step 40, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #90, step 41, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #90, step 42, discriminator loss=0.696 , generator loss=0.713\n",
      "Training progress in epoch #90, step 43, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #90, step 44, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #90, step 45, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #90, step 46, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #90, step 47, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #90, step 48, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #90, step 49, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #90, step 50, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #90, step 51, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #90, step 52, discriminator loss=0.699 , generator loss=0.707\n",
      "Training progress in epoch #90, step 53, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #90, step 54, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #90, step 55, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #90, step 56, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #90, step 57, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #90, step 58, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #90, step 59, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #90, step 60, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #90, step 61, discriminator loss=0.698 , generator loss=0.698\n",
      "Training progress in epoch #90, step 62, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #90, step 63, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #90, step 64, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #90, step 65, discriminator loss=0.696 , generator loss=0.690\n",
      "Training progress in epoch #90, step 66, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #90, step 67, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #90, step 68, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #90, step 69, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #90, step 70, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #90, step 71, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #90, step 72, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #90, step 73, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #90, step 74, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #90, step 75, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #90, step 76, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #90, step 77, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #90, step 78, discriminator loss=0.685 , generator loss=0.699\n",
      "Training progress in epoch #90, step 79, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #90, step 80, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #90, step 81, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #90, step 82, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #90, step 83, discriminator loss=0.689 , generator loss=0.682\n",
      "Training progress in epoch #90, step 84, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #90, step 85, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #90, step 86, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #90, step 87, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #90, step 88, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #90, step 89, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #90, step 90, discriminator loss=0.693 , generator loss=0.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #90, step 91, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #90, step 92, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #90, step 93, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #90, step 94, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #90, step 95, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #90, step 96, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #90, step 97, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #90, step 98, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #90, step 99, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #90, step 100, discriminator loss=0.688 , generator loss=0.726\n",
      "Training progress in epoch #90, step 101, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #90, step 102, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #90, step 103, discriminator loss=0.688 , generator loss=0.685\n",
      "Training progress in epoch #90, step 104, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #90, step 105, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #90, step 106, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #90, step 107, discriminator loss=0.695 , generator loss=0.689\n",
      "Training progress in epoch #90, step 108, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #90, step 109, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #90, step 110, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #90, step 111, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #90, step 112, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #90, step 113, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #90, step 114, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #90, step 115, discriminator loss=0.689 , generator loss=0.675\n",
      "Training progress in epoch #90, step 116, discriminator loss=0.695 , generator loss=0.669\n",
      "Training progress in epoch #90, step 117, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #90, step 118, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #90, step 119, discriminator loss=0.693 , generator loss=0.723\n",
      "Training progress in epoch #90, step 120, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #90, step 121, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #90, step 122, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #90, step 123, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #90, step 124, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #90, step 125, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #90, step 126, discriminator loss=0.685 , generator loss=0.705\n",
      "Training progress in epoch #90, step 127, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #90, step 128, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #90, step 129, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #90, step 130, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #90, step 131, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #90, step 132, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #90, step 133, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #90, step 134, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #90, step 135, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #90, step 136, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #90, step 137, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #90, step 138, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #90, step 139, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #90, step 140, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #90, step 141, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #90, step 142, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #90, step 143, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #90, step 144, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #90, step 145, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #90, step 146, discriminator loss=0.688 , generator loss=0.678\n",
      "Training progress in epoch #90, step 147, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #90, step 148, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #90, step 149, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #90, step 150, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #90, step 151, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #90, step 152, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #90, step 153, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #90, step 154, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #90, step 155, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #90, step 156, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #90, step 157, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #90, step 158, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #90, step 159, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #90, step 160, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #90, step 161, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #90, step 162, discriminator loss=0.693 , generator loss=0.734\n",
      "Training progress in epoch #90, step 163, discriminator loss=0.694 , generator loss=0.730\n",
      "Training progress in epoch #90, step 164, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #90, step 165, discriminator loss=0.696 , generator loss=0.680\n",
      "Training progress in epoch #90, step 166, discriminator loss=0.694 , generator loss=0.675\n",
      "Training progress in epoch #90, step 167, discriminator loss=0.692 , generator loss=0.669\n",
      "Training progress in epoch #90, step 168, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #90, step 169, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #90, step 170, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #90, step 171, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #90, step 172, discriminator loss=0.692 , generator loss=0.721\n",
      "Training progress in epoch #90, step 173, discriminator loss=0.689 , generator loss=0.723\n",
      "Training progress in epoch #90, step 174, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #90, step 175, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #90, step 176, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #90, step 177, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #90, step 178, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #90, step 179, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #90, step 180, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #90, step 181, discriminator loss=0.689 , generator loss=0.727\n",
      "Training progress in epoch #90, step 182, discriminator loss=0.693 , generator loss=0.722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #90, step 183, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #90, step 184, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #90, step 185, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #90, step 186, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #90, step 187, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #90, step 188, discriminator loss=0.685 , generator loss=0.692\n",
      "Training progress in epoch #90, step 189, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #90, step 190, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #90, step 191, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #90, step 192, discriminator loss=0.688 , generator loss=0.733\n",
      "Training progress in epoch #90, step 193, discriminator loss=0.691 , generator loss=0.736\n",
      "Training progress in epoch #90, step 194, discriminator loss=0.688 , generator loss=0.725\n",
      "Training progress in epoch #90, step 195, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #90, step 196, discriminator loss=0.692 , generator loss=0.675\n",
      "Training progress in epoch #90, step 197, discriminator loss=0.690 , generator loss=0.672\n",
      "Training progress in epoch #90, step 198, discriminator loss=0.690 , generator loss=0.673\n",
      "Training progress in epoch #90, step 199, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #90, step 200, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #90, step 201, discriminator loss=0.692 , generator loss=0.729\n",
      "Training progress in epoch #90, step 202, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #90, step 203, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #90, step 204, discriminator loss=0.697 , generator loss=0.689\n",
      "Training progress in epoch #90, step 205, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #90, step 206, discriminator loss=0.687 , generator loss=0.680\n",
      "Training progress in epoch #90, step 207, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #90, step 208, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #90, step 209, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #90, step 210, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #90, step 211, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #90, step 212, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #90, step 213, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #90, step 214, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #90, step 215, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #90, step 216, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #90, step 217, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #90, step 218, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #90, step 219, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #90, step 220, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #90, step 221, discriminator loss=0.697 , generator loss=0.702\n",
      "Training progress in epoch #90, step 222, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #90, step 223, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #90, step 224, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #90, step 225, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #90, step 226, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #90, step 227, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #90, step 228, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #90, step 229, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #90, step 230, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #90, step 231, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #90, step 232, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #90, step 233, discriminator loss=0.694 , generator loss=0.718\n",
      "Disciminator Accuracy on real images: 20%, on fake images: 92%\n",
      "Training progress in epoch #91, step 0, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #91, step 1, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #91, step 2, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #91, step 3, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #91, step 4, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #91, step 5, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #91, step 6, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #91, step 7, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #91, step 8, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #91, step 9, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #91, step 10, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #91, step 11, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #91, step 12, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #91, step 13, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #91, step 14, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #91, step 15, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #91, step 16, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #91, step 17, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #91, step 18, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #91, step 19, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #91, step 20, discriminator loss=0.689 , generator loss=0.678\n",
      "Training progress in epoch #91, step 21, discriminator loss=0.686 , generator loss=0.689\n",
      "Training progress in epoch #91, step 22, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #91, step 23, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #91, step 24, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #91, step 25, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #91, step 26, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #91, step 27, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #91, step 28, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #91, step 29, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #91, step 30, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #91, step 31, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #91, step 32, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #91, step 33, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #91, step 34, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #91, step 35, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #91, step 36, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #91, step 37, discriminator loss=0.695 , generator loss=0.682\n",
      "Training progress in epoch #91, step 38, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #91, step 39, discriminator loss=0.692 , generator loss=0.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #91, step 40, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #91, step 41, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #91, step 42, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #91, step 43, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #91, step 44, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #91, step 45, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #91, step 46, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #91, step 47, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #91, step 48, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #91, step 49, discriminator loss=0.686 , generator loss=0.680\n",
      "Training progress in epoch #91, step 50, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #91, step 51, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #91, step 52, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #91, step 53, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #91, step 54, discriminator loss=0.697 , generator loss=0.719\n",
      "Training progress in epoch #91, step 55, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #91, step 56, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #91, step 57, discriminator loss=0.686 , generator loss=0.700\n",
      "Training progress in epoch #91, step 58, discriminator loss=0.687 , generator loss=0.698\n",
      "Training progress in epoch #91, step 59, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #91, step 60, discriminator loss=0.691 , generator loss=0.682\n",
      "Training progress in epoch #91, step 61, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #91, step 62, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #91, step 63, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #91, step 64, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #91, step 65, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #91, step 66, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #91, step 67, discriminator loss=0.699 , generator loss=0.688\n",
      "Training progress in epoch #91, step 68, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #91, step 69, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #91, step 70, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #91, step 71, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #91, step 72, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #91, step 73, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #91, step 74, discriminator loss=0.685 , generator loss=0.688\n",
      "Training progress in epoch #91, step 75, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #91, step 76, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #91, step 77, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #91, step 78, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #91, step 79, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #91, step 80, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #91, step 81, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #91, step 82, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #91, step 83, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #91, step 84, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #91, step 85, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #91, step 86, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #91, step 87, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #91, step 88, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #91, step 89, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #91, step 90, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #91, step 91, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #91, step 92, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #91, step 93, discriminator loss=0.682 , generator loss=0.691\n",
      "Training progress in epoch #91, step 94, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #91, step 95, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #91, step 96, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #91, step 97, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #91, step 98, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #91, step 99, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #91, step 100, discriminator loss=0.696 , generator loss=0.694\n",
      "Training progress in epoch #91, step 101, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #91, step 102, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #91, step 103, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #91, step 104, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #91, step 105, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #91, step 106, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #91, step 107, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #91, step 108, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #91, step 109, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #91, step 110, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #91, step 111, discriminator loss=0.696 , generator loss=0.707\n",
      "Training progress in epoch #91, step 112, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #91, step 113, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #91, step 114, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #91, step 115, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #91, step 116, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #91, step 117, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #91, step 118, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #91, step 119, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #91, step 120, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #91, step 121, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #91, step 122, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #91, step 123, discriminator loss=0.687 , generator loss=0.682\n",
      "Training progress in epoch #91, step 124, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #91, step 125, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #91, step 126, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #91, step 127, discriminator loss=0.696 , generator loss=0.722\n",
      "Training progress in epoch #91, step 128, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #91, step 129, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #91, step 130, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #91, step 131, discriminator loss=0.692 , generator loss=0.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #91, step 132, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #91, step 133, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #91, step 134, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #91, step 135, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #91, step 136, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #91, step 137, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #91, step 138, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #91, step 139, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #91, step 140, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #91, step 141, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #91, step 142, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #91, step 143, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #91, step 144, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #91, step 145, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #91, step 146, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #91, step 147, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #91, step 148, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #91, step 149, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #91, step 150, discriminator loss=0.696 , generator loss=0.689\n",
      "Training progress in epoch #91, step 151, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #91, step 152, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #91, step 153, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #91, step 154, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #91, step 155, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #91, step 156, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #91, step 157, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #91, step 158, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #91, step 159, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #91, step 160, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #91, step 161, discriminator loss=0.686 , generator loss=0.707\n",
      "Training progress in epoch #91, step 162, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #91, step 163, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #91, step 164, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #91, step 165, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #91, step 166, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #91, step 167, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #91, step 168, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #91, step 169, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #91, step 170, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #91, step 171, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #91, step 172, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #91, step 173, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #91, step 174, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #91, step 175, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #91, step 176, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #91, step 177, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #91, step 178, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #91, step 179, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #91, step 180, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #91, step 181, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #91, step 182, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #91, step 183, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #91, step 184, discriminator loss=0.697 , generator loss=0.694\n",
      "Training progress in epoch #91, step 185, discriminator loss=0.696 , generator loss=0.703\n",
      "Training progress in epoch #91, step 186, discriminator loss=0.695 , generator loss=0.711\n",
      "Training progress in epoch #91, step 187, discriminator loss=0.698 , generator loss=0.704\n",
      "Training progress in epoch #91, step 188, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #91, step 189, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #91, step 190, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #91, step 191, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #91, step 192, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #91, step 193, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #91, step 194, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #91, step 195, discriminator loss=0.696 , generator loss=0.718\n",
      "Training progress in epoch #91, step 196, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #91, step 197, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #91, step 198, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #91, step 199, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #91, step 200, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #91, step 201, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #91, step 202, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #91, step 203, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #91, step 204, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #91, step 205, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #91, step 206, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #91, step 207, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #91, step 208, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #91, step 209, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #91, step 210, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #91, step 211, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #91, step 212, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #91, step 213, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #91, step 214, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #91, step 215, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #91, step 216, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #91, step 217, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #91, step 218, discriminator loss=0.691 , generator loss=0.674\n",
      "Training progress in epoch #91, step 219, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #91, step 220, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #91, step 221, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #91, step 222, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #91, step 223, discriminator loss=0.689 , generator loss=0.684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #91, step 224, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #91, step 225, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #91, step 226, discriminator loss=0.698 , generator loss=0.695\n",
      "Training progress in epoch #91, step 227, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #91, step 228, discriminator loss=0.697 , generator loss=0.721\n",
      "Training progress in epoch #91, step 229, discriminator loss=0.697 , generator loss=0.712\n",
      "Training progress in epoch #91, step 230, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #91, step 231, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #91, step 232, discriminator loss=0.695 , generator loss=0.689\n",
      "Training progress in epoch #91, step 233, discriminator loss=0.692 , generator loss=0.699\n",
      "Disciminator Accuracy on real images: 42%, on fake images: 85%\n",
      "Training progress in epoch #92, step 0, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #92, step 1, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #92, step 2, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #92, step 3, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #92, step 4, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #92, step 5, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #92, step 6, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #92, step 7, discriminator loss=0.688 , generator loss=0.699\n",
      "Training progress in epoch #92, step 8, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #92, step 9, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #92, step 10, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #92, step 11, discriminator loss=0.695 , generator loss=0.721\n",
      "Training progress in epoch #92, step 12, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #92, step 13, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #92, step 14, discriminator loss=0.691 , generator loss=0.676\n",
      "Training progress in epoch #92, step 15, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #92, step 16, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #92, step 17, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #92, step 18, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #92, step 19, discriminator loss=0.695 , generator loss=0.722\n",
      "Training progress in epoch #92, step 20, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #92, step 21, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #92, step 22, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #92, step 23, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #92, step 24, discriminator loss=0.688 , generator loss=0.689\n",
      "Training progress in epoch #92, step 25, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #92, step 26, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #92, step 27, discriminator loss=0.691 , generator loss=0.725\n",
      "Training progress in epoch #92, step 28, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #92, step 29, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #92, step 30, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #92, step 31, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #92, step 32, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #92, step 33, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #92, step 34, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #92, step 35, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #92, step 36, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #92, step 37, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #92, step 38, discriminator loss=0.700 , generator loss=0.693\n",
      "Training progress in epoch #92, step 39, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #92, step 40, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #92, step 41, discriminator loss=0.687 , generator loss=0.692\n",
      "Training progress in epoch #92, step 42, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #92, step 43, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #92, step 44, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #92, step 45, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #92, step 46, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #92, step 47, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #92, step 48, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #92, step 49, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #92, step 50, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #92, step 51, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #92, step 52, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #92, step 53, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #92, step 54, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #92, step 55, discriminator loss=0.697 , generator loss=0.674\n",
      "Training progress in epoch #92, step 56, discriminator loss=0.691 , generator loss=0.670\n",
      "Training progress in epoch #92, step 57, discriminator loss=0.698 , generator loss=0.686\n",
      "Training progress in epoch #92, step 58, discriminator loss=0.696 , generator loss=0.724\n",
      "Training progress in epoch #92, step 59, discriminator loss=0.697 , generator loss=0.742\n",
      "Training progress in epoch #92, step 60, discriminator loss=0.691 , generator loss=0.745\n",
      "Training progress in epoch #92, step 61, discriminator loss=0.691 , generator loss=0.720\n",
      "Training progress in epoch #92, step 62, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #92, step 63, discriminator loss=0.694 , generator loss=0.678\n",
      "Training progress in epoch #92, step 64, discriminator loss=0.695 , generator loss=0.678\n",
      "Training progress in epoch #92, step 65, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #92, step 66, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #92, step 67, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #92, step 68, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #92, step 69, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #92, step 70, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #92, step 71, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #92, step 72, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #92, step 73, discriminator loss=0.696 , generator loss=0.719\n",
      "Training progress in epoch #92, step 74, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #92, step 75, discriminator loss=0.695 , generator loss=0.671\n",
      "Training progress in epoch #92, step 76, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #92, step 77, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #92, step 78, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #92, step 79, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #92, step 80, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #92, step 81, discriminator loss=0.693 , generator loss=0.693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #92, step 82, discriminator loss=0.689 , generator loss=0.672\n",
      "Training progress in epoch #92, step 83, discriminator loss=0.690 , generator loss=0.683\n",
      "Training progress in epoch #92, step 84, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #92, step 85, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #92, step 86, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #92, step 87, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #92, step 88, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #92, step 89, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #92, step 90, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #92, step 91, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #92, step 92, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #92, step 93, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #92, step 94, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #92, step 95, discriminator loss=0.698 , generator loss=0.697\n",
      "Training progress in epoch #92, step 96, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #92, step 97, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #92, step 98, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #92, step 99, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #92, step 100, discriminator loss=0.695 , generator loss=0.683\n",
      "Training progress in epoch #92, step 101, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #92, step 102, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #92, step 103, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #92, step 104, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #92, step 105, discriminator loss=0.697 , generator loss=0.692\n",
      "Training progress in epoch #92, step 106, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #92, step 107, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #92, step 108, discriminator loss=0.685 , generator loss=0.719\n",
      "Training progress in epoch #92, step 109, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #92, step 110, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #92, step 111, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #92, step 112, discriminator loss=0.685 , generator loss=0.681\n",
      "Training progress in epoch #92, step 113, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #92, step 114, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #92, step 115, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #92, step 116, discriminator loss=0.695 , generator loss=0.716\n",
      "Training progress in epoch #92, step 117, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #92, step 118, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #92, step 119, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #92, step 120, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #92, step 121, discriminator loss=0.699 , generator loss=0.701\n",
      "Training progress in epoch #92, step 122, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #92, step 123, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #92, step 124, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #92, step 125, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #92, step 126, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #92, step 127, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #92, step 128, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #92, step 129, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #92, step 130, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #92, step 131, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #92, step 132, discriminator loss=0.687 , generator loss=0.681\n",
      "Training progress in epoch #92, step 133, discriminator loss=0.696 , generator loss=0.682\n",
      "Training progress in epoch #92, step 134, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #92, step 135, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #92, step 136, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #92, step 137, discriminator loss=0.694 , generator loss=0.723\n",
      "Training progress in epoch #92, step 138, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #92, step 139, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #92, step 140, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #92, step 141, discriminator loss=0.694 , generator loss=0.666\n",
      "Training progress in epoch #92, step 142, discriminator loss=0.695 , generator loss=0.683\n",
      "Training progress in epoch #92, step 143, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #92, step 144, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #92, step 145, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #92, step 146, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #92, step 147, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #92, step 148, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #92, step 149, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #92, step 150, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #92, step 151, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #92, step 152, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #92, step 153, discriminator loss=0.694 , generator loss=0.710\n",
      "Training progress in epoch #92, step 154, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #92, step 155, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #92, step 156, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #92, step 157, discriminator loss=0.697 , generator loss=0.700\n",
      "Training progress in epoch #92, step 158, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #92, step 159, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #92, step 160, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #92, step 161, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #92, step 162, discriminator loss=0.689 , generator loss=0.707\n",
      "Training progress in epoch #92, step 163, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #92, step 164, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #92, step 165, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #92, step 166, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #92, step 167, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #92, step 168, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #92, step 169, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #92, step 170, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #92, step 171, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #92, step 172, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #92, step 173, discriminator loss=0.695 , generator loss=0.737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #92, step 174, discriminator loss=0.695 , generator loss=0.756\n",
      "Training progress in epoch #92, step 175, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #92, step 176, discriminator loss=0.697 , generator loss=0.681\n",
      "Training progress in epoch #92, step 177, discriminator loss=0.694 , generator loss=0.677\n",
      "Training progress in epoch #92, step 178, discriminator loss=0.695 , generator loss=0.681\n",
      "Training progress in epoch #92, step 179, discriminator loss=0.684 , generator loss=0.683\n",
      "Training progress in epoch #92, step 180, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #92, step 181, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #92, step 182, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #92, step 183, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #92, step 184, discriminator loss=0.687 , generator loss=0.704\n",
      "Training progress in epoch #92, step 185, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #92, step 186, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #92, step 187, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #92, step 188, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #92, step 189, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #92, step 190, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #92, step 191, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #92, step 192, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #92, step 193, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #92, step 194, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #92, step 195, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #92, step 196, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #92, step 197, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #92, step 198, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #92, step 199, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #92, step 200, discriminator loss=0.687 , generator loss=0.702\n",
      "Training progress in epoch #92, step 201, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #92, step 202, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #92, step 203, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #92, step 204, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #92, step 205, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #92, step 206, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #92, step 207, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #92, step 208, discriminator loss=0.700 , generator loss=0.698\n",
      "Training progress in epoch #92, step 209, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #92, step 210, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #92, step 211, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #92, step 212, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #92, step 213, discriminator loss=0.689 , generator loss=0.726\n",
      "Training progress in epoch #92, step 214, discriminator loss=0.685 , generator loss=0.715\n",
      "Training progress in epoch #92, step 215, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #92, step 216, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #92, step 217, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #92, step 218, discriminator loss=0.691 , generator loss=0.673\n",
      "Training progress in epoch #92, step 219, discriminator loss=0.693 , generator loss=0.676\n",
      "Training progress in epoch #92, step 220, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #92, step 221, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #92, step 222, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #92, step 223, discriminator loss=0.694 , generator loss=0.719\n",
      "Training progress in epoch #92, step 224, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #92, step 225, discriminator loss=0.697 , generator loss=0.696\n",
      "Training progress in epoch #92, step 226, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #92, step 227, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #92, step 228, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #92, step 229, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #92, step 230, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #92, step 231, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #92, step 232, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #92, step 233, discriminator loss=0.693 , generator loss=0.707\n",
      "Disciminator Accuracy on real images: 33%, on fake images: 82%\n",
      "Training progress in epoch #93, step 0, discriminator loss=0.692 , generator loss=0.719\n",
      "Training progress in epoch #93, step 1, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #93, step 2, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #93, step 3, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #93, step 4, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #93, step 5, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #93, step 6, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #93, step 7, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #93, step 8, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #93, step 9, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #93, step 10, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #93, step 11, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #93, step 12, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #93, step 13, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #93, step 14, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #93, step 15, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #93, step 16, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #93, step 17, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #93, step 18, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #93, step 19, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #93, step 20, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #93, step 21, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #93, step 22, discriminator loss=0.695 , generator loss=0.685\n",
      "Training progress in epoch #93, step 23, discriminator loss=0.692 , generator loss=0.669\n",
      "Training progress in epoch #93, step 24, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #93, step 25, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #93, step 26, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #93, step 27, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #93, step 28, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #93, step 29, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #93, step 30, discriminator loss=0.693 , generator loss=0.693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #93, step 31, discriminator loss=0.698 , generator loss=0.682\n",
      "Training progress in epoch #93, step 32, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #93, step 33, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #93, step 34, discriminator loss=0.686 , generator loss=0.705\n",
      "Training progress in epoch #93, step 35, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #93, step 36, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #93, step 37, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #93, step 38, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #93, step 39, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #93, step 40, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #93, step 41, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #93, step 42, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #93, step 43, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #93, step 44, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #93, step 45, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #93, step 46, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #93, step 47, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #93, step 48, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #93, step 49, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #93, step 50, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #93, step 51, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #93, step 52, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #93, step 53, discriminator loss=0.697 , generator loss=0.683\n",
      "Training progress in epoch #93, step 54, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #93, step 55, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #93, step 56, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #93, step 57, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #93, step 58, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #93, step 59, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #93, step 60, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #93, step 61, discriminator loss=0.686 , generator loss=0.699\n",
      "Training progress in epoch #93, step 62, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #93, step 63, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #93, step 64, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #93, step 65, discriminator loss=0.699 , generator loss=0.716\n",
      "Training progress in epoch #93, step 66, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #93, step 67, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #93, step 68, discriminator loss=0.694 , generator loss=0.675\n",
      "Training progress in epoch #93, step 69, discriminator loss=0.690 , generator loss=0.671\n",
      "Training progress in epoch #93, step 70, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #93, step 71, discriminator loss=0.698 , generator loss=0.701\n",
      "Training progress in epoch #93, step 72, discriminator loss=0.696 , generator loss=0.694\n",
      "Training progress in epoch #93, step 73, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #93, step 74, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #93, step 75, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #93, step 76, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #93, step 77, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #93, step 78, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #93, step 79, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #93, step 80, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #93, step 81, discriminator loss=0.697 , generator loss=0.707\n",
      "Training progress in epoch #93, step 82, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #93, step 83, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #93, step 84, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #93, step 85, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #93, step 86, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #93, step 87, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #93, step 88, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #93, step 89, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #93, step 90, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #93, step 91, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #93, step 92, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #93, step 93, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #93, step 94, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #93, step 95, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #93, step 96, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #93, step 97, discriminator loss=0.697 , generator loss=0.697\n",
      "Training progress in epoch #93, step 98, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #93, step 99, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #93, step 100, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #93, step 101, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #93, step 102, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #93, step 103, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #93, step 104, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #93, step 105, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #93, step 106, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #93, step 107, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #93, step 108, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #93, step 109, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #93, step 110, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #93, step 111, discriminator loss=0.689 , generator loss=0.721\n",
      "Training progress in epoch #93, step 112, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #93, step 113, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #93, step 114, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #93, step 115, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #93, step 116, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #93, step 117, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #93, step 118, discriminator loss=0.695 , generator loss=0.716\n",
      "Training progress in epoch #93, step 119, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #93, step 120, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #93, step 121, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #93, step 122, discriminator loss=0.697 , generator loss=0.703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #93, step 123, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #93, step 124, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #93, step 125, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #93, step 126, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #93, step 127, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #93, step 128, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #93, step 129, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #93, step 130, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #93, step 131, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #93, step 132, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #93, step 133, discriminator loss=0.694 , generator loss=0.717\n",
      "Training progress in epoch #93, step 134, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #93, step 135, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #93, step 136, discriminator loss=0.698 , generator loss=0.697\n",
      "Training progress in epoch #93, step 137, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #93, step 138, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #93, step 139, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #93, step 140, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #93, step 141, discriminator loss=0.689 , generator loss=0.677\n",
      "Training progress in epoch #93, step 142, discriminator loss=0.689 , generator loss=0.679\n",
      "Training progress in epoch #93, step 143, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #93, step 144, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #93, step 145, discriminator loss=0.694 , generator loss=0.716\n",
      "Training progress in epoch #93, step 146, discriminator loss=0.683 , generator loss=0.716\n",
      "Training progress in epoch #93, step 147, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #93, step 148, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #93, step 149, discriminator loss=0.695 , generator loss=0.693\n",
      "Training progress in epoch #93, step 150, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #93, step 151, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #93, step 152, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #93, step 153, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #93, step 154, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #93, step 155, discriminator loss=0.695 , generator loss=0.729\n",
      "Training progress in epoch #93, step 156, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #93, step 157, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #93, step 158, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #93, step 159, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #93, step 160, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #93, step 161, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #93, step 162, discriminator loss=0.693 , generator loss=0.682\n",
      "Training progress in epoch #93, step 163, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #93, step 164, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #93, step 165, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #93, step 166, discriminator loss=0.690 , generator loss=0.728\n",
      "Training progress in epoch #93, step 167, discriminator loss=0.694 , generator loss=0.735\n",
      "Training progress in epoch #93, step 168, discriminator loss=0.697 , generator loss=0.713\n",
      "Training progress in epoch #93, step 169, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #93, step 170, discriminator loss=0.696 , generator loss=0.679\n",
      "Training progress in epoch #93, step 171, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #93, step 172, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #93, step 173, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #93, step 174, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #93, step 175, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #93, step 176, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #93, step 177, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #93, step 178, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #93, step 179, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #93, step 180, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #93, step 181, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #93, step 182, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #93, step 183, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #93, step 184, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #93, step 185, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #93, step 186, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #93, step 187, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #93, step 188, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #93, step 189, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #93, step 190, discriminator loss=0.694 , generator loss=0.714\n",
      "Training progress in epoch #93, step 191, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #93, step 192, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #93, step 193, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #93, step 194, discriminator loss=0.697 , generator loss=0.694\n",
      "Training progress in epoch #93, step 195, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #93, step 196, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #93, step 197, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #93, step 198, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #93, step 199, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #93, step 200, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #93, step 201, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #93, step 202, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #93, step 203, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #93, step 204, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #93, step 205, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #93, step 206, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #93, step 207, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #93, step 208, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #93, step 209, discriminator loss=0.686 , generator loss=0.691\n",
      "Training progress in epoch #93, step 210, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #93, step 211, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #93, step 212, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #93, step 213, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #93, step 214, discriminator loss=0.692 , generator loss=0.701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #93, step 215, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #93, step 216, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #93, step 217, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #93, step 218, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #93, step 219, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #93, step 220, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #93, step 221, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #93, step 222, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #93, step 223, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #93, step 224, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #93, step 225, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #93, step 226, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #93, step 227, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #93, step 228, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #93, step 229, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #93, step 230, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #93, step 231, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #93, step 232, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #93, step 233, discriminator loss=0.693 , generator loss=0.694\n",
      "Disciminator Accuracy on real images: 50%, on fake images: 61%\n",
      "Training progress in epoch #94, step 0, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #94, step 1, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #94, step 2, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #94, step 3, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #94, step 4, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #94, step 5, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #94, step 6, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #94, step 7, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #94, step 8, discriminator loss=0.695 , generator loss=0.702\n",
      "Training progress in epoch #94, step 9, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #94, step 10, discriminator loss=0.686 , generator loss=0.694\n",
      "Training progress in epoch #94, step 11, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #94, step 12, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #94, step 13, discriminator loss=0.687 , generator loss=0.706\n",
      "Training progress in epoch #94, step 14, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #94, step 15, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #94, step 16, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #94, step 17, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #94, step 18, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #94, step 19, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #94, step 20, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #94, step 21, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #94, step 22, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #94, step 23, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #94, step 24, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #94, step 25, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #94, step 26, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #94, step 27, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #94, step 28, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #94, step 29, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #94, step 30, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #94, step 31, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #94, step 32, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #94, step 33, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #94, step 34, discriminator loss=0.694 , generator loss=0.677\n",
      "Training progress in epoch #94, step 35, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #94, step 36, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #94, step 37, discriminator loss=0.698 , generator loss=0.712\n",
      "Training progress in epoch #94, step 38, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #94, step 39, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #94, step 40, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #94, step 41, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #94, step 42, discriminator loss=0.690 , generator loss=0.678\n",
      "Training progress in epoch #94, step 43, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #94, step 44, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #94, step 45, discriminator loss=0.690 , generator loss=0.707\n",
      "Training progress in epoch #94, step 46, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #94, step 47, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #94, step 48, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #94, step 49, discriminator loss=0.689 , generator loss=0.687\n",
      "Training progress in epoch #94, step 50, discriminator loss=0.688 , generator loss=0.680\n",
      "Training progress in epoch #94, step 51, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #94, step 52, discriminator loss=0.692 , generator loss=0.717\n",
      "Training progress in epoch #94, step 53, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #94, step 54, discriminator loss=0.687 , generator loss=0.721\n",
      "Training progress in epoch #94, step 55, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #94, step 56, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #94, step 57, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #94, step 58, discriminator loss=0.699 , generator loss=0.696\n",
      "Training progress in epoch #94, step 59, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #94, step 60, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #94, step 61, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #94, step 62, discriminator loss=0.691 , generator loss=0.708\n",
      "Training progress in epoch #94, step 63, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #94, step 64, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #94, step 65, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #94, step 66, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #94, step 67, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #94, step 68, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #94, step 69, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #94, step 70, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #94, step 71, discriminator loss=0.687 , generator loss=0.714\n",
      "Training progress in epoch #94, step 72, discriminator loss=0.691 , generator loss=0.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #94, step 73, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #94, step 74, discriminator loss=0.695 , generator loss=0.680\n",
      "Training progress in epoch #94, step 75, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #94, step 76, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #94, step 77, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #94, step 78, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #94, step 79, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #94, step 80, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #94, step 81, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #94, step 82, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #94, step 83, discriminator loss=0.687 , generator loss=0.696\n",
      "Training progress in epoch #94, step 84, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #94, step 85, discriminator loss=0.689 , generator loss=0.683\n",
      "Training progress in epoch #94, step 86, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #94, step 87, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #94, step 88, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #94, step 89, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #94, step 90, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #94, step 91, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #94, step 92, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #94, step 93, discriminator loss=0.686 , generator loss=0.701\n",
      "Training progress in epoch #94, step 94, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #94, step 95, discriminator loss=0.695 , generator loss=0.717\n",
      "Training progress in epoch #94, step 96, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #94, step 97, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #94, step 98, discriminator loss=0.691 , generator loss=0.677\n",
      "Training progress in epoch #94, step 99, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #94, step 100, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #94, step 101, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #94, step 102, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #94, step 103, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #94, step 104, discriminator loss=0.684 , generator loss=0.716\n",
      "Training progress in epoch #94, step 105, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #94, step 106, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #94, step 107, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #94, step 108, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #94, step 109, discriminator loss=0.687 , generator loss=0.682\n",
      "Training progress in epoch #94, step 110, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #94, step 111, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #94, step 112, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #94, step 113, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #94, step 114, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #94, step 115, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #94, step 116, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #94, step 117, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #94, step 118, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #94, step 119, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #94, step 120, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #94, step 121, discriminator loss=0.690 , generator loss=0.714\n",
      "Training progress in epoch #94, step 122, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #94, step 123, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #94, step 124, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #94, step 125, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #94, step 126, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #94, step 127, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #94, step 128, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #94, step 129, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #94, step 130, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #94, step 131, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #94, step 132, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #94, step 133, discriminator loss=0.699 , generator loss=0.699\n",
      "Training progress in epoch #94, step 134, discriminator loss=0.699 , generator loss=0.710\n",
      "Training progress in epoch #94, step 135, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #94, step 136, discriminator loss=0.686 , generator loss=0.693\n",
      "Training progress in epoch #94, step 137, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #94, step 138, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #94, step 139, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #94, step 140, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #94, step 141, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #94, step 142, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #94, step 143, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #94, step 144, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #94, step 145, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #94, step 146, discriminator loss=0.687 , generator loss=0.672\n",
      "Training progress in epoch #94, step 147, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #94, step 148, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #94, step 149, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #94, step 150, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #94, step 151, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #94, step 152, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #94, step 153, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #94, step 154, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #94, step 155, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #94, step 156, discriminator loss=0.697 , generator loss=0.693\n",
      "Training progress in epoch #94, step 157, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #94, step 158, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #94, step 159, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #94, step 160, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #94, step 161, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #94, step 162, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #94, step 163, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #94, step 164, discriminator loss=0.692 , generator loss=0.714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #94, step 165, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #94, step 166, discriminator loss=0.698 , generator loss=0.697\n",
      "Training progress in epoch #94, step 167, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #94, step 168, discriminator loss=0.691 , generator loss=0.691\n",
      "Training progress in epoch #94, step 169, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #94, step 170, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #94, step 171, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #94, step 172, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #94, step 173, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #94, step 174, discriminator loss=0.696 , generator loss=0.713\n",
      "Training progress in epoch #94, step 175, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #94, step 176, discriminator loss=0.698 , generator loss=0.698\n",
      "Training progress in epoch #94, step 177, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #94, step 178, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #94, step 179, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #94, step 180, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #94, step 181, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #94, step 182, discriminator loss=0.698 , generator loss=0.686\n",
      "Training progress in epoch #94, step 183, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #94, step 184, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #94, step 185, discriminator loss=0.688 , generator loss=0.724\n",
      "Training progress in epoch #94, step 186, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #94, step 187, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #94, step 188, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #94, step 189, discriminator loss=0.688 , generator loss=0.688\n",
      "Training progress in epoch #94, step 190, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #94, step 191, discriminator loss=0.692 , generator loss=0.679\n",
      "Training progress in epoch #94, step 192, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #94, step 193, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #94, step 194, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #94, step 195, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #94, step 196, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #94, step 197, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #94, step 198, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #94, step 199, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #94, step 200, discriminator loss=0.692 , generator loss=0.676\n",
      "Training progress in epoch #94, step 201, discriminator loss=0.690 , generator loss=0.678\n",
      "Training progress in epoch #94, step 202, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #94, step 203, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #94, step 204, discriminator loss=0.693 , generator loss=0.733\n",
      "Training progress in epoch #94, step 205, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #94, step 206, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #94, step 207, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #94, step 208, discriminator loss=0.694 , generator loss=0.677\n",
      "Training progress in epoch #94, step 209, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #94, step 210, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #94, step 211, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #94, step 212, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #94, step 213, discriminator loss=0.687 , generator loss=0.708\n",
      "Training progress in epoch #94, step 214, discriminator loss=0.696 , generator loss=0.720\n",
      "Training progress in epoch #94, step 215, discriminator loss=0.691 , generator loss=0.727\n",
      "Training progress in epoch #94, step 216, discriminator loss=0.695 , generator loss=0.725\n",
      "Training progress in epoch #94, step 217, discriminator loss=0.687 , generator loss=0.697\n",
      "Training progress in epoch #94, step 218, discriminator loss=0.689 , generator loss=0.681\n",
      "Training progress in epoch #94, step 219, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #94, step 220, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #94, step 221, discriminator loss=0.687 , generator loss=0.683\n",
      "Training progress in epoch #94, step 222, discriminator loss=0.700 , generator loss=0.703\n",
      "Training progress in epoch #94, step 223, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #94, step 224, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #94, step 225, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #94, step 226, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #94, step 227, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #94, step 228, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #94, step 229, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #94, step 230, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #94, step 231, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #94, step 232, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #94, step 233, discriminator loss=0.692 , generator loss=0.707\n",
      "Disciminator Accuracy on real images: 29%, on fake images: 87%\n",
      "Training progress in epoch #95, step 0, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #95, step 1, discriminator loss=0.685 , generator loss=0.689\n",
      "Training progress in epoch #95, step 2, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #95, step 3, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #95, step 4, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #95, step 5, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #95, step 6, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #95, step 7, discriminator loss=0.688 , generator loss=0.676\n",
      "Training progress in epoch #95, step 8, discriminator loss=0.692 , generator loss=0.680\n",
      "Training progress in epoch #95, step 9, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #95, step 10, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #95, step 11, discriminator loss=0.699 , generator loss=0.708\n",
      "Training progress in epoch #95, step 12, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #95, step 13, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #95, step 14, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #95, step 15, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #95, step 16, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #95, step 17, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #95, step 18, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #95, step 19, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #95, step 20, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #95, step 21, discriminator loss=0.692 , generator loss=0.694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #95, step 22, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #95, step 23, discriminator loss=0.686 , generator loss=0.695\n",
      "Training progress in epoch #95, step 24, discriminator loss=0.698 , generator loss=0.698\n",
      "Training progress in epoch #95, step 25, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #95, step 26, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #95, step 27, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #95, step 28, discriminator loss=0.688 , generator loss=0.719\n",
      "Training progress in epoch #95, step 29, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #95, step 30, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #95, step 31, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #95, step 32, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #95, step 33, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #95, step 34, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #95, step 35, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #95, step 36, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #95, step 37, discriminator loss=0.693 , generator loss=0.734\n",
      "Training progress in epoch #95, step 38, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #95, step 39, discriminator loss=0.686 , generator loss=0.678\n",
      "Training progress in epoch #95, step 40, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #95, step 41, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #95, step 42, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #95, step 43, discriminator loss=0.687 , generator loss=0.728\n",
      "Training progress in epoch #95, step 44, discriminator loss=0.691 , generator loss=0.719\n",
      "Training progress in epoch #95, step 45, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #95, step 46, discriminator loss=0.695 , generator loss=0.688\n",
      "Training progress in epoch #95, step 47, discriminator loss=0.693 , generator loss=0.673\n",
      "Training progress in epoch #95, step 48, discriminator loss=0.690 , generator loss=0.681\n",
      "Training progress in epoch #95, step 49, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #95, step 50, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #95, step 51, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #95, step 52, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #95, step 53, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #95, step 54, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #95, step 55, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #95, step 56, discriminator loss=0.688 , generator loss=0.693\n",
      "Training progress in epoch #95, step 57, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #95, step 58, discriminator loss=0.698 , generator loss=0.709\n",
      "Training progress in epoch #95, step 59, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #95, step 60, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #95, step 61, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #95, step 62, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #95, step 63, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #95, step 64, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #95, step 65, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #95, step 66, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #95, step 67, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #95, step 68, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #95, step 69, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #95, step 70, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #95, step 71, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #95, step 72, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #95, step 73, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #95, step 74, discriminator loss=0.691 , generator loss=0.711\n",
      "Training progress in epoch #95, step 75, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #95, step 76, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #95, step 77, discriminator loss=0.685 , generator loss=0.690\n",
      "Training progress in epoch #95, step 78, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #95, step 79, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #95, step 80, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #95, step 81, discriminator loss=0.695 , generator loss=0.715\n",
      "Training progress in epoch #95, step 82, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #95, step 83, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #95, step 84, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #95, step 85, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #95, step 86, discriminator loss=0.688 , generator loss=0.711\n",
      "Training progress in epoch #95, step 87, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #95, step 88, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #95, step 89, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #95, step 90, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #95, step 91, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #95, step 92, discriminator loss=0.693 , generator loss=0.671\n",
      "Training progress in epoch #95, step 93, discriminator loss=0.693 , generator loss=0.673\n",
      "Training progress in epoch #95, step 94, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #95, step 95, discriminator loss=0.692 , generator loss=0.728\n",
      "Training progress in epoch #95, step 96, discriminator loss=0.695 , generator loss=0.724\n",
      "Training progress in epoch #95, step 97, discriminator loss=0.687 , generator loss=0.705\n",
      "Training progress in epoch #95, step 98, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #95, step 99, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #95, step 100, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #95, step 101, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #95, step 102, discriminator loss=0.692 , generator loss=0.677\n",
      "Training progress in epoch #95, step 103, discriminator loss=0.695 , generator loss=0.669\n",
      "Training progress in epoch #95, step 104, discriminator loss=0.695 , generator loss=0.673\n",
      "Training progress in epoch #95, step 105, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #95, step 106, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #95, step 107, discriminator loss=0.688 , generator loss=0.729\n",
      "Training progress in epoch #95, step 108, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #95, step 109, discriminator loss=0.692 , generator loss=0.732\n",
      "Training progress in epoch #95, step 110, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #95, step 111, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #95, step 112, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #95, step 113, discriminator loss=0.689 , generator loss=0.669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #95, step 114, discriminator loss=0.690 , generator loss=0.672\n",
      "Training progress in epoch #95, step 115, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #95, step 116, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #95, step 117, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #95, step 118, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #95, step 119, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #95, step 120, discriminator loss=0.693 , generator loss=0.724\n",
      "Training progress in epoch #95, step 121, discriminator loss=0.694 , generator loss=0.722\n",
      "Training progress in epoch #95, step 122, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #95, step 123, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #95, step 124, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #95, step 125, discriminator loss=0.691 , generator loss=0.675\n",
      "Training progress in epoch #95, step 126, discriminator loss=0.688 , generator loss=0.683\n",
      "Training progress in epoch #95, step 127, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #95, step 128, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #95, step 129, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #95, step 130, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #95, step 131, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #95, step 132, discriminator loss=0.699 , generator loss=0.706\n",
      "Training progress in epoch #95, step 133, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #95, step 134, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #95, step 135, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #95, step 136, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #95, step 137, discriminator loss=0.694 , generator loss=0.678\n",
      "Training progress in epoch #95, step 138, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #95, step 139, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #95, step 140, discriminator loss=0.688 , generator loss=0.698\n",
      "Training progress in epoch #95, step 141, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #95, step 142, discriminator loss=0.697 , generator loss=0.710\n",
      "Training progress in epoch #95, step 143, discriminator loss=0.688 , generator loss=0.710\n",
      "Training progress in epoch #95, step 144, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #95, step 145, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #95, step 146, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #95, step 147, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #95, step 148, discriminator loss=0.686 , generator loss=0.681\n",
      "Training progress in epoch #95, step 149, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #95, step 150, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #95, step 151, discriminator loss=0.692 , generator loss=0.720\n",
      "Training progress in epoch #95, step 152, discriminator loss=0.696 , generator loss=0.743\n",
      "Training progress in epoch #95, step 153, discriminator loss=0.687 , generator loss=0.723\n",
      "Training progress in epoch #95, step 154, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #95, step 155, discriminator loss=0.693 , generator loss=0.679\n",
      "Training progress in epoch #95, step 156, discriminator loss=0.693 , generator loss=0.680\n",
      "Training progress in epoch #95, step 157, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #95, step 158, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #95, step 159, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #95, step 160, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #95, step 161, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #95, step 162, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #95, step 163, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #95, step 164, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #95, step 165, discriminator loss=0.693 , generator loss=0.711\n",
      "Training progress in epoch #95, step 166, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #95, step 167, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #95, step 168, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #95, step 169, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #95, step 170, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #95, step 171, discriminator loss=0.689 , generator loss=0.711\n",
      "Training progress in epoch #95, step 172, discriminator loss=0.687 , generator loss=0.712\n",
      "Training progress in epoch #95, step 173, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #95, step 174, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #95, step 175, discriminator loss=0.688 , generator loss=0.694\n",
      "Training progress in epoch #95, step 176, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #95, step 177, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #95, step 178, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #95, step 179, discriminator loss=0.695 , generator loss=0.716\n",
      "Training progress in epoch #95, step 180, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #95, step 181, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #95, step 182, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #95, step 183, discriminator loss=0.696 , generator loss=0.679\n",
      "Training progress in epoch #95, step 184, discriminator loss=0.693 , generator loss=0.677\n",
      "Training progress in epoch #95, step 185, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #95, step 186, discriminator loss=0.687 , generator loss=0.710\n",
      "Training progress in epoch #95, step 187, discriminator loss=0.691 , generator loss=0.723\n",
      "Training progress in epoch #95, step 188, discriminator loss=0.684 , generator loss=0.714\n",
      "Training progress in epoch #95, step 189, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #95, step 190, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #95, step 191, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #95, step 192, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #95, step 193, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #95, step 194, discriminator loss=0.691 , generator loss=0.683\n",
      "Training progress in epoch #95, step 195, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #95, step 196, discriminator loss=0.694 , generator loss=0.729\n",
      "Training progress in epoch #95, step 197, discriminator loss=0.689 , generator loss=0.729\n",
      "Training progress in epoch #95, step 198, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #95, step 199, discriminator loss=0.691 , generator loss=0.681\n",
      "Training progress in epoch #95, step 200, discriminator loss=0.692 , generator loss=0.661\n",
      "Training progress in epoch #95, step 201, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #95, step 202, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #95, step 203, discriminator loss=0.693 , generator loss=0.718\n",
      "Training progress in epoch #95, step 204, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #95, step 205, discriminator loss=0.697 , generator loss=0.722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #95, step 206, discriminator loss=0.697 , generator loss=0.719\n",
      "Training progress in epoch #95, step 207, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #95, step 208, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #95, step 209, discriminator loss=0.694 , generator loss=0.681\n",
      "Training progress in epoch #95, step 210, discriminator loss=0.690 , generator loss=0.684\n",
      "Training progress in epoch #95, step 211, discriminator loss=0.690 , generator loss=0.679\n",
      "Training progress in epoch #95, step 212, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #95, step 213, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #95, step 214, discriminator loss=0.691 , generator loss=0.715\n",
      "Training progress in epoch #95, step 215, discriminator loss=0.694 , generator loss=0.720\n",
      "Training progress in epoch #95, step 216, discriminator loss=0.683 , generator loss=0.717\n",
      "Training progress in epoch #95, step 217, discriminator loss=0.684 , generator loss=0.704\n",
      "Training progress in epoch #95, step 218, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #95, step 219, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #95, step 220, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #95, step 221, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #95, step 222, discriminator loss=0.694 , generator loss=0.678\n",
      "Training progress in epoch #95, step 223, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #95, step 224, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #95, step 225, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #95, step 226, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #95, step 227, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #95, step 228, discriminator loss=0.696 , generator loss=0.708\n",
      "Training progress in epoch #95, step 229, discriminator loss=0.689 , generator loss=0.719\n",
      "Training progress in epoch #95, step 230, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #95, step 231, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #95, step 232, discriminator loss=0.686 , generator loss=0.690\n",
      "Training progress in epoch #95, step 233, discriminator loss=0.688 , generator loss=0.694\n",
      "Disciminator Accuracy on real images: 55%, on fake images: 71%\n",
      "Training progress in epoch #96, step 0, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #96, step 1, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #96, step 2, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #96, step 3, discriminator loss=0.698 , generator loss=0.702\n",
      "Training progress in epoch #96, step 4, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #96, step 5, discriminator loss=0.699 , generator loss=0.701\n",
      "Training progress in epoch #96, step 6, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #96, step 7, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #96, step 8, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #96, step 9, discriminator loss=0.700 , generator loss=0.694\n",
      "Training progress in epoch #96, step 10, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #96, step 11, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #96, step 12, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #96, step 13, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #96, step 14, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #96, step 15, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #96, step 16, discriminator loss=0.689 , generator loss=0.718\n",
      "Training progress in epoch #96, step 17, discriminator loss=0.697 , generator loss=0.712\n",
      "Training progress in epoch #96, step 18, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #96, step 19, discriminator loss=0.689 , generator loss=0.697\n",
      "Training progress in epoch #96, step 20, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #96, step 21, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #96, step 22, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #96, step 23, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #96, step 24, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #96, step 25, discriminator loss=0.694 , generator loss=0.680\n",
      "Training progress in epoch #96, step 26, discriminator loss=0.690 , generator loss=0.672\n",
      "Training progress in epoch #96, step 27, discriminator loss=0.694 , generator loss=0.688\n",
      "Training progress in epoch #96, step 28, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #96, step 29, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #96, step 30, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #96, step 31, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #96, step 32, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #96, step 33, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #96, step 34, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #96, step 35, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #96, step 36, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #96, step 37, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #96, step 38, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #96, step 39, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #96, step 40, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #96, step 41, discriminator loss=0.693 , generator loss=0.719\n",
      "Training progress in epoch #96, step 42, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #96, step 43, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #96, step 44, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #96, step 45, discriminator loss=0.698 , generator loss=0.693\n",
      "Training progress in epoch #96, step 46, discriminator loss=0.692 , generator loss=0.672\n",
      "Training progress in epoch #96, step 47, discriminator loss=0.692 , generator loss=0.683\n",
      "Training progress in epoch #96, step 48, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #96, step 49, discriminator loss=0.686 , generator loss=0.712\n",
      "Training progress in epoch #96, step 50, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #96, step 51, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #96, step 52, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #96, step 53, discriminator loss=0.684 , generator loss=0.706\n",
      "Training progress in epoch #96, step 54, discriminator loss=0.690 , generator loss=0.722\n",
      "Training progress in epoch #96, step 55, discriminator loss=0.692 , generator loss=0.711\n",
      "Training progress in epoch #96, step 56, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #96, step 57, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #96, step 58, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #96, step 59, discriminator loss=0.691 , generator loss=0.675\n",
      "Training progress in epoch #96, step 60, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #96, step 61, discriminator loss=0.688 , generator loss=0.681\n",
      "Training progress in epoch #96, step 62, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #96, step 63, discriminator loss=0.696 , generator loss=0.699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #96, step 64, discriminator loss=0.697 , generator loss=0.707\n",
      "Training progress in epoch #96, step 65, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #96, step 66, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #96, step 67, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #96, step 68, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #96, step 69, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #96, step 70, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #96, step 71, discriminator loss=0.691 , generator loss=0.687\n",
      "Training progress in epoch #96, step 72, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #96, step 73, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #96, step 74, discriminator loss=0.696 , generator loss=0.691\n",
      "Training progress in epoch #96, step 75, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #96, step 76, discriminator loss=0.694 , generator loss=0.733\n",
      "Training progress in epoch #96, step 77, discriminator loss=0.691 , generator loss=0.741\n",
      "Training progress in epoch #96, step 78, discriminator loss=0.691 , generator loss=0.728\n",
      "Training progress in epoch #96, step 79, discriminator loss=0.688 , generator loss=0.709\n",
      "Training progress in epoch #96, step 80, discriminator loss=0.692 , generator loss=0.696\n",
      "Training progress in epoch #96, step 81, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #96, step 82, discriminator loss=0.695 , generator loss=0.668\n",
      "Training progress in epoch #96, step 83, discriminator loss=0.694 , generator loss=0.660\n",
      "Training progress in epoch #96, step 84, discriminator loss=0.695 , generator loss=0.671\n",
      "Training progress in epoch #96, step 85, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #96, step 86, discriminator loss=0.692 , generator loss=0.737\n",
      "Training progress in epoch #96, step 87, discriminator loss=0.690 , generator loss=0.746\n",
      "Training progress in epoch #96, step 88, discriminator loss=0.688 , generator loss=0.749\n",
      "Training progress in epoch #96, step 89, discriminator loss=0.688 , generator loss=0.720\n",
      "Training progress in epoch #96, step 90, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #96, step 91, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #96, step 92, discriminator loss=0.694 , generator loss=0.678\n",
      "Training progress in epoch #96, step 93, discriminator loss=0.691 , generator loss=0.667\n",
      "Training progress in epoch #96, step 94, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #96, step 95, discriminator loss=0.685 , generator loss=0.690\n",
      "Training progress in epoch #96, step 96, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #96, step 97, discriminator loss=0.687 , generator loss=0.724\n",
      "Training progress in epoch #96, step 98, discriminator loss=0.690 , generator loss=0.713\n",
      "Training progress in epoch #96, step 99, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #96, step 100, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #96, step 101, discriminator loss=0.689 , generator loss=0.686\n",
      "Training progress in epoch #96, step 102, discriminator loss=0.695 , generator loss=0.682\n",
      "Training progress in epoch #96, step 103, discriminator loss=0.697 , generator loss=0.686\n",
      "Training progress in epoch #96, step 104, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #96, step 105, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #96, step 106, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #96, step 107, discriminator loss=0.688 , generator loss=0.706\n",
      "Training progress in epoch #96, step 108, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #96, step 109, discriminator loss=0.688 , generator loss=0.713\n",
      "Training progress in epoch #96, step 110, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #96, step 111, discriminator loss=0.697 , generator loss=0.699\n",
      "Training progress in epoch #96, step 112, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #96, step 113, discriminator loss=0.697 , generator loss=0.688\n",
      "Training progress in epoch #96, step 114, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #96, step 115, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #96, step 116, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #96, step 117, discriminator loss=0.698 , generator loss=0.705\n",
      "Training progress in epoch #96, step 118, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #96, step 119, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #96, step 120, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #96, step 121, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #96, step 122, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #96, step 123, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #96, step 124, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #96, step 125, discriminator loss=0.686 , generator loss=0.697\n",
      "Training progress in epoch #96, step 126, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #96, step 127, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #96, step 128, discriminator loss=0.698 , generator loss=0.695\n",
      "Training progress in epoch #96, step 129, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #96, step 130, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #96, step 131, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #96, step 132, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #96, step 133, discriminator loss=0.697 , generator loss=0.729\n",
      "Training progress in epoch #96, step 134, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #96, step 135, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #96, step 136, discriminator loss=0.690 , generator loss=0.674\n",
      "Training progress in epoch #96, step 137, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #96, step 138, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #96, step 139, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #96, step 140, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #96, step 141, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #96, step 142, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #96, step 143, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #96, step 144, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #96, step 145, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #96, step 146, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #96, step 147, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #96, step 148, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #96, step 149, discriminator loss=0.688 , generator loss=0.678\n",
      "Training progress in epoch #96, step 150, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #96, step 151, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #96, step 152, discriminator loss=0.695 , generator loss=0.720\n",
      "Training progress in epoch #96, step 153, discriminator loss=0.690 , generator loss=0.719\n",
      "Training progress in epoch #96, step 154, discriminator loss=0.694 , generator loss=0.713\n",
      "Training progress in epoch #96, step 155, discriminator loss=0.694 , generator loss=0.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #96, step 156, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #96, step 157, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #96, step 158, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #96, step 159, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #96, step 160, discriminator loss=0.697 , generator loss=0.699\n",
      "Training progress in epoch #96, step 161, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #96, step 162, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #96, step 163, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #96, step 164, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #96, step 165, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #96, step 166, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #96, step 167, discriminator loss=0.696 , generator loss=0.714\n",
      "Training progress in epoch #96, step 168, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #96, step 169, discriminator loss=0.689 , generator loss=0.688\n",
      "Training progress in epoch #96, step 170, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #96, step 171, discriminator loss=0.692 , generator loss=0.687\n",
      "Training progress in epoch #96, step 172, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #96, step 173, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #96, step 174, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #96, step 175, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #96, step 176, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #96, step 177, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #96, step 178, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #96, step 179, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #96, step 180, discriminator loss=0.697 , generator loss=0.698\n",
      "Training progress in epoch #96, step 181, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #96, step 182, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #96, step 183, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #96, step 184, discriminator loss=0.685 , generator loss=0.712\n",
      "Training progress in epoch #96, step 185, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #96, step 186, discriminator loss=0.688 , generator loss=0.678\n",
      "Training progress in epoch #96, step 187, discriminator loss=0.695 , generator loss=0.677\n",
      "Training progress in epoch #96, step 188, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #96, step 189, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #96, step 190, discriminator loss=0.695 , generator loss=0.715\n",
      "Training progress in epoch #96, step 191, discriminator loss=0.691 , generator loss=0.730\n",
      "Training progress in epoch #96, step 192, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #96, step 193, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #96, step 194, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #96, step 195, discriminator loss=0.699 , generator loss=0.678\n",
      "Training progress in epoch #96, step 196, discriminator loss=0.694 , generator loss=0.684\n",
      "Training progress in epoch #96, step 197, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #96, step 198, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #96, step 199, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #96, step 200, discriminator loss=0.696 , generator loss=0.711\n",
      "Training progress in epoch #96, step 201, discriminator loss=0.698 , generator loss=0.715\n",
      "Training progress in epoch #96, step 202, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #96, step 203, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #96, step 204, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #96, step 205, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #96, step 206, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #96, step 207, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #96, step 208, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #96, step 209, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #96, step 210, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #96, step 211, discriminator loss=0.695 , generator loss=0.680\n",
      "Training progress in epoch #96, step 212, discriminator loss=0.693 , generator loss=0.675\n",
      "Training progress in epoch #96, step 213, discriminator loss=0.694 , generator loss=0.695\n",
      "Training progress in epoch #96, step 214, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #96, step 215, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #96, step 216, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #96, step 217, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #96, step 218, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #96, step 219, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #96, step 220, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #96, step 221, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #96, step 222, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #96, step 223, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #96, step 224, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #96, step 225, discriminator loss=0.696 , generator loss=0.684\n",
      "Training progress in epoch #96, step 226, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #96, step 227, discriminator loss=0.697 , generator loss=0.725\n",
      "Training progress in epoch #96, step 228, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #96, step 229, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #96, step 230, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #96, step 231, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #96, step 232, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #96, step 233, discriminator loss=0.691 , generator loss=0.707\n",
      "Disciminator Accuracy on real images: 9%, on fake images: 98%\n",
      "Training progress in epoch #97, step 0, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #97, step 1, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #97, step 2, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #97, step 3, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #97, step 4, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #97, step 5, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #97, step 6, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #97, step 7, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #97, step 8, discriminator loss=0.684 , generator loss=0.701\n",
      "Training progress in epoch #97, step 9, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #97, step 10, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #97, step 11, discriminator loss=0.696 , generator loss=0.694\n",
      "Training progress in epoch #97, step 12, discriminator loss=0.695 , generator loss=0.697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #97, step 13, discriminator loss=0.695 , generator loss=0.705\n",
      "Training progress in epoch #97, step 14, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #97, step 15, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #97, step 16, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #97, step 17, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #97, step 18, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #97, step 19, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #97, step 20, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #97, step 21, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #97, step 22, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #97, step 23, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #97, step 24, discriminator loss=0.695 , generator loss=0.716\n",
      "Training progress in epoch #97, step 25, discriminator loss=0.694 , generator loss=0.718\n",
      "Training progress in epoch #97, step 26, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #97, step 27, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #97, step 28, discriminator loss=0.691 , generator loss=0.672\n",
      "Training progress in epoch #97, step 29, discriminator loss=0.688 , generator loss=0.667\n",
      "Training progress in epoch #97, step 30, discriminator loss=0.688 , generator loss=0.682\n",
      "Training progress in epoch #97, step 31, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #97, step 32, discriminator loss=0.693 , generator loss=0.734\n",
      "Training progress in epoch #97, step 33, discriminator loss=0.692 , generator loss=0.730\n",
      "Training progress in epoch #97, step 34, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #97, step 35, discriminator loss=0.688 , generator loss=0.690\n",
      "Training progress in epoch #97, step 36, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #97, step 37, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #97, step 38, discriminator loss=0.689 , generator loss=0.701\n",
      "Training progress in epoch #97, step 39, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #97, step 40, discriminator loss=0.697 , generator loss=0.708\n",
      "Training progress in epoch #97, step 41, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #97, step 42, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #97, step 43, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #97, step 44, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #97, step 45, discriminator loss=0.697 , generator loss=0.700\n",
      "Training progress in epoch #97, step 46, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #97, step 47, discriminator loss=0.691 , generator loss=0.703\n",
      "Training progress in epoch #97, step 48, discriminator loss=0.689 , generator loss=0.694\n",
      "Training progress in epoch #97, step 49, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #97, step 50, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #97, step 51, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #97, step 52, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #97, step 53, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #97, step 54, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #97, step 55, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #97, step 56, discriminator loss=0.692 , generator loss=0.700\n",
      "Training progress in epoch #97, step 57, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #97, step 58, discriminator loss=0.689 , generator loss=0.685\n",
      "Training progress in epoch #97, step 59, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #97, step 60, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #97, step 61, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #97, step 62, discriminator loss=0.696 , generator loss=0.701\n",
      "Training progress in epoch #97, step 63, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #97, step 64, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #97, step 65, discriminator loss=0.687 , generator loss=0.690\n",
      "Training progress in epoch #97, step 66, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #97, step 67, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #97, step 68, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #97, step 69, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #97, step 70, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #97, step 71, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #97, step 72, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #97, step 73, discriminator loss=0.688 , generator loss=0.686\n",
      "Training progress in epoch #97, step 74, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #97, step 75, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #97, step 76, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #97, step 77, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #97, step 78, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #97, step 79, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #97, step 80, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #97, step 81, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #97, step 82, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #97, step 83, discriminator loss=0.692 , generator loss=0.673\n",
      "Training progress in epoch #97, step 84, discriminator loss=0.691 , generator loss=0.684\n",
      "Training progress in epoch #97, step 85, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #97, step 86, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #97, step 87, discriminator loss=0.689 , generator loss=0.722\n",
      "Training progress in epoch #97, step 88, discriminator loss=0.691 , generator loss=0.731\n",
      "Training progress in epoch #97, step 89, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #97, step 90, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #97, step 91, discriminator loss=0.690 , generator loss=0.700\n",
      "Training progress in epoch #97, step 92, discriminator loss=0.685 , generator loss=0.686\n",
      "Training progress in epoch #97, step 93, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #97, step 94, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #97, step 95, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #97, step 96, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #97, step 97, discriminator loss=0.691 , generator loss=0.726\n",
      "Training progress in epoch #97, step 98, discriminator loss=0.691 , generator loss=0.723\n",
      "Training progress in epoch #97, step 99, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #97, step 100, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #97, step 101, discriminator loss=0.694 , generator loss=0.686\n",
      "Training progress in epoch #97, step 102, discriminator loss=0.696 , generator loss=0.685\n",
      "Training progress in epoch #97, step 103, discriminator loss=0.693 , generator loss=0.688\n",
      "Training progress in epoch #97, step 104, discriminator loss=0.692 , generator loss=0.691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #97, step 105, discriminator loss=0.690 , generator loss=0.696\n",
      "Training progress in epoch #97, step 106, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #97, step 107, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #97, step 108, discriminator loss=0.689 , generator loss=0.708\n",
      "Training progress in epoch #97, step 109, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #97, step 110, discriminator loss=0.692 , generator loss=0.726\n",
      "Training progress in epoch #97, step 111, discriminator loss=0.689 , generator loss=0.717\n",
      "Training progress in epoch #97, step 112, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #97, step 113, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #97, step 114, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #97, step 115, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #97, step 116, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #97, step 117, discriminator loss=0.695 , generator loss=0.690\n",
      "Training progress in epoch #97, step 118, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #97, step 119, discriminator loss=0.694 , generator loss=0.715\n",
      "Training progress in epoch #97, step 120, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #97, step 121, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #97, step 122, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #97, step 123, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #97, step 124, discriminator loss=0.700 , generator loss=0.709\n",
      "Training progress in epoch #97, step 125, discriminator loss=0.689 , generator loss=0.709\n",
      "Training progress in epoch #97, step 126, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #97, step 127, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #97, step 128, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #97, step 129, discriminator loss=0.685 , generator loss=0.708\n",
      "Training progress in epoch #97, step 130, discriminator loss=0.685 , generator loss=0.706\n",
      "Training progress in epoch #97, step 131, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #97, step 132, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #97, step 133, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #97, step 134, discriminator loss=0.690 , generator loss=0.682\n",
      "Training progress in epoch #97, step 135, discriminator loss=0.694 , generator loss=0.678\n",
      "Training progress in epoch #97, step 136, discriminator loss=0.696 , generator loss=0.685\n",
      "Training progress in epoch #97, step 137, discriminator loss=0.689 , generator loss=0.714\n",
      "Training progress in epoch #97, step 138, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #97, step 139, discriminator loss=0.697 , generator loss=0.718\n",
      "Training progress in epoch #97, step 140, discriminator loss=0.688 , generator loss=0.705\n",
      "Training progress in epoch #97, step 141, discriminator loss=0.689 , generator loss=0.693\n",
      "Training progress in epoch #97, step 142, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #97, step 143, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #97, step 144, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #97, step 145, discriminator loss=0.685 , generator loss=0.684\n",
      "Training progress in epoch #97, step 146, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #97, step 147, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #97, step 148, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #97, step 149, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #97, step 150, discriminator loss=0.693 , generator loss=0.712\n",
      "Training progress in epoch #97, step 151, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #97, step 152, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #97, step 153, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #97, step 154, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #97, step 155, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #97, step 156, discriminator loss=0.698 , generator loss=0.706\n",
      "Training progress in epoch #97, step 157, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #97, step 158, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #97, step 159, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #97, step 160, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #97, step 161, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #97, step 162, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #97, step 163, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #97, step 164, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #97, step 165, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #97, step 166, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #97, step 167, discriminator loss=0.695 , generator loss=0.687\n",
      "Training progress in epoch #97, step 168, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #97, step 169, discriminator loss=0.686 , generator loss=0.715\n",
      "Training progress in epoch #97, step 170, discriminator loss=0.690 , generator loss=0.712\n",
      "Training progress in epoch #97, step 171, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #97, step 172, discriminator loss=0.690 , generator loss=0.677\n",
      "Training progress in epoch #97, step 173, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #97, step 174, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #97, step 175, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #97, step 176, discriminator loss=0.697 , generator loss=0.706\n",
      "Training progress in epoch #97, step 177, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #97, step 178, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #97, step 179, discriminator loss=0.699 , generator loss=0.700\n",
      "Training progress in epoch #97, step 180, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #97, step 181, discriminator loss=0.692 , generator loss=0.684\n",
      "Training progress in epoch #97, step 182, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #97, step 183, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #97, step 184, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #97, step 185, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #97, step 186, discriminator loss=0.693 , generator loss=0.696\n",
      "Training progress in epoch #97, step 187, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #97, step 188, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #97, step 189, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #97, step 190, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #97, step 191, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #97, step 192, discriminator loss=0.684 , generator loss=0.701\n",
      "Training progress in epoch #97, step 193, discriminator loss=0.689 , generator loss=0.702\n",
      "Training progress in epoch #97, step 194, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #97, step 195, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #97, step 196, discriminator loss=0.692 , generator loss=0.696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #97, step 197, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #97, step 198, discriminator loss=0.689 , generator loss=0.703\n",
      "Training progress in epoch #97, step 199, discriminator loss=0.696 , generator loss=0.705\n",
      "Training progress in epoch #97, step 200, discriminator loss=0.694 , generator loss=0.694\n",
      "Training progress in epoch #97, step 201, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #97, step 202, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #97, step 203, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #97, step 204, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #97, step 205, discriminator loss=0.685 , generator loss=0.712\n",
      "Training progress in epoch #97, step 206, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #97, step 207, discriminator loss=0.694 , generator loss=0.699\n",
      "Training progress in epoch #97, step 208, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #97, step 209, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #97, step 210, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #97, step 211, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #97, step 212, discriminator loss=0.694 , generator loss=0.693\n",
      "Training progress in epoch #97, step 213, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #97, step 214, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #97, step 215, discriminator loss=0.687 , generator loss=0.695\n",
      "Training progress in epoch #97, step 216, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #97, step 217, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #97, step 218, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #97, step 219, discriminator loss=0.689 , generator loss=0.706\n",
      "Training progress in epoch #97, step 220, discriminator loss=0.690 , generator loss=0.716\n",
      "Training progress in epoch #97, step 221, discriminator loss=0.691 , generator loss=0.721\n",
      "Training progress in epoch #97, step 222, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #97, step 223, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #97, step 224, discriminator loss=0.692 , generator loss=0.678\n",
      "Training progress in epoch #97, step 225, discriminator loss=0.692 , generator loss=0.670\n",
      "Training progress in epoch #97, step 226, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #97, step 227, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #97, step 228, discriminator loss=0.695 , generator loss=0.701\n",
      "Training progress in epoch #97, step 229, discriminator loss=0.697 , generator loss=0.713\n",
      "Training progress in epoch #97, step 230, discriminator loss=0.690 , generator loss=0.720\n",
      "Training progress in epoch #97, step 231, discriminator loss=0.698 , generator loss=0.724\n",
      "Training progress in epoch #97, step 232, discriminator loss=0.699 , generator loss=0.700\n",
      "Training progress in epoch #97, step 233, discriminator loss=0.689 , generator loss=0.684\n",
      "Disciminator Accuracy on real images: 55%, on fake images: 55%\n",
      "Training progress in epoch #98, step 0, discriminator loss=0.685 , generator loss=0.696\n",
      "Training progress in epoch #98, step 1, discriminator loss=0.691 , generator loss=0.692\n",
      "Training progress in epoch #98, step 2, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #98, step 3, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #98, step 4, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #98, step 5, discriminator loss=0.687 , generator loss=0.688\n",
      "Training progress in epoch #98, step 6, discriminator loss=0.692 , generator loss=0.685\n",
      "Training progress in epoch #98, step 7, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #98, step 8, discriminator loss=0.694 , generator loss=0.707\n",
      "Training progress in epoch #98, step 9, discriminator loss=0.688 , generator loss=0.708\n",
      "Training progress in epoch #98, step 10, discriminator loss=0.697 , generator loss=0.720\n",
      "Training progress in epoch #98, step 11, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #98, step 12, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #98, step 13, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #98, step 14, discriminator loss=0.687 , generator loss=0.684\n",
      "Training progress in epoch #98, step 15, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #98, step 16, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #98, step 17, discriminator loss=0.693 , generator loss=0.720\n",
      "Training progress in epoch #98, step 18, discriminator loss=0.692 , generator loss=0.713\n",
      "Training progress in epoch #98, step 19, discriminator loss=0.690 , generator loss=0.708\n",
      "Training progress in epoch #98, step 20, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #98, step 21, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #98, step 22, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #98, step 23, discriminator loss=0.692 , generator loss=0.686\n",
      "Training progress in epoch #98, step 24, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #98, step 25, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #98, step 26, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #98, step 27, discriminator loss=0.687 , generator loss=0.686\n",
      "Training progress in epoch #98, step 28, discriminator loss=0.689 , generator loss=0.690\n",
      "Training progress in epoch #98, step 29, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #98, step 30, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #98, step 31, discriminator loss=0.697 , generator loss=0.712\n",
      "Training progress in epoch #98, step 32, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #98, step 33, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #98, step 34, discriminator loss=0.693 , generator loss=0.689\n",
      "Training progress in epoch #98, step 35, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #98, step 36, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #98, step 37, discriminator loss=0.693 , generator loss=0.702\n",
      "Training progress in epoch #98, step 38, discriminator loss=0.692 , generator loss=0.718\n",
      "Training progress in epoch #98, step 39, discriminator loss=0.690 , generator loss=0.710\n",
      "Training progress in epoch #98, step 40, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #98, step 41, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #98, step 42, discriminator loss=0.696 , generator loss=0.687\n",
      "Training progress in epoch #98, step 43, discriminator loss=0.695 , generator loss=0.698\n",
      "Training progress in epoch #98, step 44, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #98, step 45, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #98, step 46, discriminator loss=0.691 , generator loss=0.693\n",
      "Training progress in epoch #98, step 47, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #98, step 48, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #98, step 49, discriminator loss=0.692 , generator loss=0.694\n",
      "Training progress in epoch #98, step 50, discriminator loss=0.691 , generator loss=0.702\n",
      "Training progress in epoch #98, step 51, discriminator loss=0.696 , generator loss=0.715\n",
      "Training progress in epoch #98, step 52, discriminator loss=0.683 , generator loss=0.703\n",
      "Training progress in epoch #98, step 53, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #98, step 54, discriminator loss=0.691 , generator loss=0.693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #98, step 55, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #98, step 56, discriminator loss=0.689 , generator loss=0.716\n",
      "Training progress in epoch #98, step 57, discriminator loss=0.690 , generator loss=0.721\n",
      "Training progress in epoch #98, step 58, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #98, step 59, discriminator loss=0.698 , generator loss=0.685\n",
      "Training progress in epoch #98, step 60, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #98, step 61, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #98, step 62, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #98, step 63, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #98, step 64, discriminator loss=0.694 , generator loss=0.700\n",
      "Training progress in epoch #98, step 65, discriminator loss=0.694 , generator loss=0.680\n",
      "Training progress in epoch #98, step 66, discriminator loss=0.691 , generator loss=0.680\n",
      "Training progress in epoch #98, step 67, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #98, step 68, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #98, step 69, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #98, step 70, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #98, step 71, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #98, step 72, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #98, step 73, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #98, step 74, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #98, step 75, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #98, step 76, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #98, step 77, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #98, step 78, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #98, step 79, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #98, step 80, discriminator loss=0.694 , generator loss=0.682\n",
      "Training progress in epoch #98, step 81, discriminator loss=0.694 , generator loss=0.692\n",
      "Training progress in epoch #98, step 82, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #98, step 83, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #98, step 84, discriminator loss=0.691 , generator loss=0.704\n",
      "Training progress in epoch #98, step 85, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #98, step 86, discriminator loss=0.694 , generator loss=0.708\n",
      "Training progress in epoch #98, step 87, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #98, step 88, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #98, step 89, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #98, step 90, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #98, step 91, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #98, step 92, discriminator loss=0.687 , generator loss=0.694\n",
      "Training progress in epoch #98, step 93, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #98, step 94, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #98, step 95, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #98, step 96, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #98, step 97, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #98, step 98, discriminator loss=0.688 , generator loss=0.702\n",
      "Training progress in epoch #98, step 99, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #98, step 100, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #98, step 101, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #98, step 102, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #98, step 103, discriminator loss=0.698 , generator loss=0.709\n",
      "Training progress in epoch #98, step 104, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #98, step 105, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #98, step 106, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #98, step 107, discriminator loss=0.691 , generator loss=0.705\n",
      "Training progress in epoch #98, step 108, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #98, step 109, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #98, step 110, discriminator loss=0.694 , generator loss=0.706\n",
      "Training progress in epoch #98, step 111, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #98, step 112, discriminator loss=0.696 , generator loss=0.700\n",
      "Training progress in epoch #98, step 113, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #98, step 114, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #98, step 115, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #98, step 116, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #98, step 117, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #98, step 118, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #98, step 119, discriminator loss=0.689 , generator loss=0.704\n",
      "Training progress in epoch #98, step 120, discriminator loss=0.695 , generator loss=0.714\n",
      "Training progress in epoch #98, step 121, discriminator loss=0.694 , generator loss=0.709\n",
      "Training progress in epoch #98, step 122, discriminator loss=0.688 , generator loss=0.692\n",
      "Training progress in epoch #98, step 123, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #98, step 124, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #98, step 125, discriminator loss=0.691 , generator loss=0.694\n",
      "Training progress in epoch #98, step 126, discriminator loss=0.691 , generator loss=0.689\n",
      "Training progress in epoch #98, step 127, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #98, step 128, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #98, step 129, discriminator loss=0.690 , generator loss=0.717\n",
      "Training progress in epoch #98, step 130, discriminator loss=0.694 , generator loss=0.711\n",
      "Training progress in epoch #98, step 131, discriminator loss=0.695 , generator loss=0.718\n",
      "Training progress in epoch #98, step 132, discriminator loss=0.696 , generator loss=0.710\n",
      "Training progress in epoch #98, step 133, discriminator loss=0.695 , generator loss=0.700\n",
      "Training progress in epoch #98, step 134, discriminator loss=0.699 , generator loss=0.686\n",
      "Training progress in epoch #98, step 135, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #98, step 136, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #98, step 137, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #98, step 138, discriminator loss=0.692 , generator loss=0.705\n",
      "Training progress in epoch #98, step 139, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #98, step 140, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #98, step 141, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #98, step 142, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #98, step 143, discriminator loss=0.695 , generator loss=0.708\n",
      "Training progress in epoch #98, step 144, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #98, step 145, discriminator loss=0.685 , generator loss=0.695\n",
      "Training progress in epoch #98, step 146, discriminator loss=0.693 , generator loss=0.694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #98, step 147, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #98, step 148, discriminator loss=0.690 , generator loss=0.709\n",
      "Training progress in epoch #98, step 149, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #98, step 150, discriminator loss=0.692 , generator loss=0.722\n",
      "Training progress in epoch #98, step 151, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #98, step 152, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #98, step 153, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #98, step 154, discriminator loss=0.696 , generator loss=0.686\n",
      "Training progress in epoch #98, step 155, discriminator loss=0.692 , generator loss=0.676\n",
      "Training progress in epoch #98, step 156, discriminator loss=0.694 , generator loss=0.679\n",
      "Training progress in epoch #98, step 157, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #98, step 158, discriminator loss=0.696 , generator loss=0.720\n",
      "Training progress in epoch #98, step 159, discriminator loss=0.694 , generator loss=0.737\n",
      "Training progress in epoch #98, step 160, discriminator loss=0.687 , generator loss=0.718\n",
      "Training progress in epoch #98, step 161, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #98, step 162, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #98, step 163, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #98, step 164, discriminator loss=0.694 , generator loss=0.687\n",
      "Training progress in epoch #98, step 165, discriminator loss=0.693 , generator loss=0.691\n",
      "Training progress in epoch #98, step 166, discriminator loss=0.692 , generator loss=0.690\n",
      "Training progress in epoch #98, step 167, discriminator loss=0.692 , generator loss=0.688\n",
      "Training progress in epoch #98, step 168, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #98, step 169, discriminator loss=0.690 , generator loss=0.726\n",
      "Training progress in epoch #98, step 170, discriminator loss=0.697 , generator loss=0.739\n",
      "Training progress in epoch #98, step 171, discriminator loss=0.687 , generator loss=0.716\n",
      "Training progress in epoch #98, step 172, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #98, step 173, discriminator loss=0.689 , generator loss=0.684\n",
      "Training progress in epoch #98, step 174, discriminator loss=0.700 , generator loss=0.685\n",
      "Training progress in epoch #98, step 175, discriminator loss=0.693 , generator loss=0.692\n",
      "Training progress in epoch #98, step 176, discriminator loss=0.688 , generator loss=0.701\n",
      "Training progress in epoch #98, step 177, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #98, step 178, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #98, step 179, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #98, step 180, discriminator loss=0.689 , generator loss=0.713\n",
      "Training progress in epoch #98, step 181, discriminator loss=0.690 , generator loss=0.689\n",
      "Training progress in epoch #98, step 182, discriminator loss=0.693 , generator loss=0.674\n",
      "Training progress in epoch #98, step 183, discriminator loss=0.696 , generator loss=0.693\n",
      "Training progress in epoch #98, step 184, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #98, step 185, discriminator loss=0.688 , generator loss=0.728\n",
      "Training progress in epoch #98, step 186, discriminator loss=0.692 , generator loss=0.725\n",
      "Training progress in epoch #98, step 187, discriminator loss=0.694 , generator loss=0.698\n",
      "Training progress in epoch #98, step 188, discriminator loss=0.697 , generator loss=0.696\n",
      "Training progress in epoch #98, step 189, discriminator loss=0.690 , generator loss=0.690\n",
      "Training progress in epoch #98, step 190, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #98, step 191, discriminator loss=0.692 , generator loss=0.698\n",
      "Training progress in epoch #98, step 192, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #98, step 193, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #98, step 194, discriminator loss=0.694 , generator loss=0.712\n",
      "Training progress in epoch #98, step 195, discriminator loss=0.692 , generator loss=0.716\n",
      "Training progress in epoch #98, step 196, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #98, step 197, discriminator loss=0.695 , generator loss=0.691\n",
      "Training progress in epoch #98, step 198, discriminator loss=0.691 , generator loss=0.685\n",
      "Training progress in epoch #98, step 199, discriminator loss=0.697 , generator loss=0.684\n",
      "Training progress in epoch #98, step 200, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #98, step 201, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #98, step 202, discriminator loss=0.689 , generator loss=0.705\n",
      "Training progress in epoch #98, step 203, discriminator loss=0.697 , generator loss=0.712\n",
      "Training progress in epoch #98, step 204, discriminator loss=0.694 , generator loss=0.727\n",
      "Training progress in epoch #98, step 205, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #98, step 206, discriminator loss=0.697 , generator loss=0.689\n",
      "Training progress in epoch #98, step 207, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #98, step 208, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #98, step 209, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #98, step 210, discriminator loss=0.687 , generator loss=0.693\n",
      "Training progress in epoch #98, step 211, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #98, step 212, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #98, step 213, discriminator loss=0.688 , generator loss=0.707\n",
      "Training progress in epoch #98, step 214, discriminator loss=0.690 , generator loss=0.725\n",
      "Training progress in epoch #98, step 215, discriminator loss=0.694 , generator loss=0.721\n",
      "Training progress in epoch #98, step 216, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #98, step 217, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #98, step 218, discriminator loss=0.691 , generator loss=0.679\n",
      "Training progress in epoch #98, step 219, discriminator loss=0.694 , generator loss=0.680\n",
      "Training progress in epoch #98, step 220, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #98, step 221, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #98, step 222, discriminator loss=0.695 , generator loss=0.719\n",
      "Training progress in epoch #98, step 223, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #98, step 224, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #98, step 225, discriminator loss=0.692 , generator loss=0.689\n",
      "Training progress in epoch #98, step 226, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #98, step 227, discriminator loss=0.693 , generator loss=0.684\n",
      "Training progress in epoch #98, step 228, discriminator loss=0.690 , generator loss=0.686\n",
      "Training progress in epoch #98, step 229, discriminator loss=0.690 , generator loss=0.702\n",
      "Training progress in epoch #98, step 230, discriminator loss=0.693 , generator loss=0.705\n",
      "Training progress in epoch #98, step 231, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #98, step 232, discriminator loss=0.686 , generator loss=0.703\n",
      "Training progress in epoch #98, step 233, discriminator loss=0.692 , generator loss=0.714\n",
      "Disciminator Accuracy on real images: 13%, on fake images: 97%\n",
      "Training progress in epoch #99, step 0, discriminator loss=0.687 , generator loss=0.725\n",
      "Training progress in epoch #99, step 1, discriminator loss=0.687 , generator loss=0.703\n",
      "Training progress in epoch #99, step 2, discriminator loss=0.687 , generator loss=0.689\n",
      "Training progress in epoch #99, step 3, discriminator loss=0.692 , generator loss=0.679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #99, step 4, discriminator loss=0.690 , generator loss=0.685\n",
      "Training progress in epoch #99, step 5, discriminator loss=0.692 , generator loss=0.695\n",
      "Training progress in epoch #99, step 6, discriminator loss=0.691 , generator loss=0.696\n",
      "Training progress in epoch #99, step 7, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #99, step 8, discriminator loss=0.696 , generator loss=0.712\n",
      "Training progress in epoch #99, step 9, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #99, step 10, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #99, step 11, discriminator loss=0.697 , generator loss=0.690\n",
      "Training progress in epoch #99, step 12, discriminator loss=0.694 , generator loss=0.691\n",
      "Training progress in epoch #99, step 13, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #99, step 14, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #99, step 15, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #99, step 16, discriminator loss=0.692 , generator loss=0.691\n",
      "Training progress in epoch #99, step 17, discriminator loss=0.695 , generator loss=0.686\n",
      "Training progress in epoch #99, step 18, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #99, step 19, discriminator loss=0.690 , generator loss=0.718\n",
      "Training progress in epoch #99, step 20, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #99, step 21, discriminator loss=0.692 , generator loss=0.712\n",
      "Training progress in epoch #99, step 22, discriminator loss=0.693 , generator loss=0.698\n",
      "Training progress in epoch #99, step 23, discriminator loss=0.693 , generator loss=0.685\n",
      "Training progress in epoch #99, step 24, discriminator loss=0.690 , generator loss=0.691\n",
      "Training progress in epoch #99, step 25, discriminator loss=0.688 , generator loss=0.703\n",
      "Training progress in epoch #99, step 26, discriminator loss=0.691 , generator loss=0.716\n",
      "Training progress in epoch #99, step 27, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #99, step 28, discriminator loss=0.690 , generator loss=0.701\n",
      "Training progress in epoch #99, step 29, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #99, step 30, discriminator loss=0.687 , generator loss=0.701\n",
      "Training progress in epoch #99, step 31, discriminator loss=0.703 , generator loss=0.691\n",
      "Training progress in epoch #99, step 32, discriminator loss=0.696 , generator loss=0.685\n",
      "Training progress in epoch #99, step 33, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #99, step 34, discriminator loss=0.697 , generator loss=0.708\n",
      "Training progress in epoch #99, step 35, discriminator loss=0.696 , generator loss=0.699\n",
      "Training progress in epoch #99, step 36, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #99, step 37, discriminator loss=0.694 , generator loss=0.702\n",
      "Training progress in epoch #99, step 38, discriminator loss=0.696 , generator loss=0.697\n",
      "Training progress in epoch #99, step 39, discriminator loss=0.693 , generator loss=0.690\n",
      "Training progress in epoch #99, step 40, discriminator loss=0.693 , generator loss=0.707\n",
      "Training progress in epoch #99, step 41, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #99, step 42, discriminator loss=0.691 , generator loss=0.706\n",
      "Training progress in epoch #99, step 43, discriminator loss=0.685 , generator loss=0.697\n",
      "Training progress in epoch #99, step 44, discriminator loss=0.690 , generator loss=0.692\n",
      "Training progress in epoch #99, step 45, discriminator loss=0.688 , generator loss=0.700\n",
      "Training progress in epoch #99, step 46, discriminator loss=0.686 , generator loss=0.702\n",
      "Training progress in epoch #99, step 47, discriminator loss=0.688 , generator loss=0.697\n",
      "Training progress in epoch #99, step 48, discriminator loss=0.688 , generator loss=0.695\n",
      "Training progress in epoch #99, step 49, discriminator loss=0.690 , generator loss=0.698\n",
      "Training progress in epoch #99, step 50, discriminator loss=0.689 , generator loss=0.692\n",
      "Training progress in epoch #99, step 51, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #99, step 52, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #99, step 53, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #99, step 54, discriminator loss=0.695 , generator loss=0.695\n",
      "Training progress in epoch #99, step 55, discriminator loss=0.691 , generator loss=0.695\n",
      "Training progress in epoch #99, step 56, discriminator loss=0.695 , generator loss=0.697\n",
      "Training progress in epoch #99, step 57, discriminator loss=0.693 , generator loss=0.703\n",
      "Training progress in epoch #99, step 58, discriminator loss=0.695 , generator loss=0.712\n",
      "Training progress in epoch #99, step 59, discriminator loss=0.695 , generator loss=0.706\n",
      "Training progress in epoch #99, step 60, discriminator loss=0.693 , generator loss=0.704\n",
      "Training progress in epoch #99, step 61, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #99, step 62, discriminator loss=0.687 , generator loss=0.687\n",
      "Training progress in epoch #99, step 63, discriminator loss=0.689 , generator loss=0.680\n",
      "Training progress in epoch #99, step 64, discriminator loss=0.688 , generator loss=0.696\n",
      "Training progress in epoch #99, step 65, discriminator loss=0.692 , generator loss=0.709\n",
      "Training progress in epoch #99, step 66, discriminator loss=0.689 , generator loss=0.715\n",
      "Training progress in epoch #99, step 67, discriminator loss=0.693 , generator loss=0.729\n",
      "Training progress in epoch #99, step 68, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #99, step 69, discriminator loss=0.690 , generator loss=0.715\n",
      "Training progress in epoch #99, step 70, discriminator loss=0.686 , generator loss=0.692\n",
      "Training progress in epoch #99, step 71, discriminator loss=0.692 , generator loss=0.682\n",
      "Training progress in epoch #99, step 72, discriminator loss=0.694 , generator loss=0.675\n",
      "Training progress in epoch #99, step 73, discriminator loss=0.687 , generator loss=0.675\n",
      "Training progress in epoch #99, step 74, discriminator loss=0.690 , generator loss=0.705\n",
      "Training progress in epoch #99, step 75, discriminator loss=0.687 , generator loss=0.709\n",
      "Training progress in epoch #99, step 76, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #99, step 77, discriminator loss=0.691 , generator loss=0.712\n",
      "Training progress in epoch #99, step 78, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #99, step 79, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #99, step 80, discriminator loss=0.695 , generator loss=0.713\n",
      "Training progress in epoch #99, step 81, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #99, step 82, discriminator loss=0.693 , generator loss=0.686\n",
      "Training progress in epoch #99, step 83, discriminator loss=0.686 , generator loss=0.681\n",
      "Training progress in epoch #99, step 84, discriminator loss=0.690 , generator loss=0.680\n",
      "Training progress in epoch #99, step 85, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #99, step 86, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #99, step 87, discriminator loss=0.688 , generator loss=0.712\n",
      "Training progress in epoch #99, step 88, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #99, step 89, discriminator loss=0.693 , generator loss=0.706\n",
      "Training progress in epoch #99, step 90, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #99, step 91, discriminator loss=0.690 , generator loss=0.711\n",
      "Training progress in epoch #99, step 92, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #99, step 93, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #99, step 94, discriminator loss=0.689 , generator loss=0.689\n",
      "Training progress in epoch #99, step 95, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #99, step 96, discriminator loss=0.687 , generator loss=0.680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #99, step 97, discriminator loss=0.690 , generator loss=0.688\n",
      "Training progress in epoch #99, step 98, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #99, step 99, discriminator loss=0.686 , generator loss=0.704\n",
      "Training progress in epoch #99, step 100, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #99, step 101, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #99, step 102, discriminator loss=0.691 , generator loss=0.713\n",
      "Training progress in epoch #99, step 103, discriminator loss=0.696 , generator loss=0.709\n",
      "Training progress in epoch #99, step 104, discriminator loss=0.698 , generator loss=0.687\n",
      "Training progress in epoch #99, step 105, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #99, step 106, discriminator loss=0.693 , generator loss=0.683\n",
      "Training progress in epoch #99, step 107, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #99, step 108, discriminator loss=0.693 , generator loss=0.700\n",
      "Training progress in epoch #99, step 109, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #99, step 110, discriminator loss=0.690 , generator loss=0.699\n",
      "Training progress in epoch #99, step 111, discriminator loss=0.695 , generator loss=0.694\n",
      "Training progress in epoch #99, step 112, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #99, step 113, discriminator loss=0.693 , generator loss=0.722\n",
      "Training progress in epoch #99, step 114, discriminator loss=0.691 , generator loss=0.717\n",
      "Training progress in epoch #99, step 115, discriminator loss=0.689 , generator loss=0.712\n",
      "Training progress in epoch #99, step 116, discriminator loss=0.687 , generator loss=0.691\n",
      "Training progress in epoch #99, step 117, discriminator loss=0.692 , generator loss=0.693\n",
      "Training progress in epoch #99, step 118, discriminator loss=0.691 , generator loss=0.686\n",
      "Training progress in epoch #99, step 119, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #99, step 120, discriminator loss=0.692 , generator loss=0.704\n",
      "Training progress in epoch #99, step 121, discriminator loss=0.692 , generator loss=0.707\n",
      "Training progress in epoch #99, step 122, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #99, step 123, discriminator loss=0.691 , generator loss=0.701\n",
      "Training progress in epoch #99, step 124, discriminator loss=0.692 , generator loss=0.714\n",
      "Training progress in epoch #99, step 125, discriminator loss=0.693 , generator loss=0.710\n",
      "Training progress in epoch #99, step 126, discriminator loss=0.693 , generator loss=0.693\n",
      "Training progress in epoch #99, step 127, discriminator loss=0.693 , generator loss=0.681\n",
      "Training progress in epoch #99, step 128, discriminator loss=0.691 , generator loss=0.688\n",
      "Training progress in epoch #99, step 129, discriminator loss=0.689 , generator loss=0.699\n",
      "Training progress in epoch #99, step 130, discriminator loss=0.694 , generator loss=0.704\n",
      "Training progress in epoch #99, step 131, discriminator loss=0.696 , generator loss=0.704\n",
      "Training progress in epoch #99, step 132, discriminator loss=0.691 , generator loss=0.697\n",
      "Training progress in epoch #99, step 133, discriminator loss=0.695 , generator loss=0.689\n",
      "Training progress in epoch #99, step 134, discriminator loss=0.695 , generator loss=0.682\n",
      "Training progress in epoch #99, step 135, discriminator loss=0.694 , generator loss=0.689\n",
      "Training progress in epoch #99, step 136, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #99, step 137, discriminator loss=0.693 , generator loss=0.699\n",
      "Training progress in epoch #99, step 138, discriminator loss=0.693 , generator loss=0.713\n",
      "Training progress in epoch #99, step 139, discriminator loss=0.695 , generator loss=0.710\n",
      "Training progress in epoch #99, step 140, discriminator loss=0.698 , generator loss=0.710\n",
      "Training progress in epoch #99, step 141, discriminator loss=0.694 , generator loss=0.685\n",
      "Training progress in epoch #99, step 142, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #99, step 143, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #99, step 144, discriminator loss=0.691 , generator loss=0.709\n",
      "Training progress in epoch #99, step 145, discriminator loss=0.689 , generator loss=0.700\n",
      "Training progress in epoch #99, step 146, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #99, step 147, discriminator loss=0.691 , generator loss=0.699\n",
      "Training progress in epoch #99, step 148, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #99, step 149, discriminator loss=0.693 , generator loss=0.715\n",
      "Training progress in epoch #99, step 150, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #99, step 151, discriminator loss=0.692 , generator loss=0.681\n",
      "Training progress in epoch #99, step 152, discriminator loss=0.694 , generator loss=0.683\n",
      "Training progress in epoch #99, step 153, discriminator loss=0.693 , generator loss=0.687\n",
      "Training progress in epoch #99, step 154, discriminator loss=0.686 , generator loss=0.706\n",
      "Training progress in epoch #99, step 155, discriminator loss=0.689 , generator loss=0.698\n",
      "Training progress in epoch #99, step 156, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #99, step 157, discriminator loss=0.693 , generator loss=0.708\n",
      "Training progress in epoch #99, step 158, discriminator loss=0.691 , generator loss=0.714\n",
      "Training progress in epoch #99, step 159, discriminator loss=0.695 , generator loss=0.704\n",
      "Training progress in epoch #99, step 160, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #99, step 161, discriminator loss=0.688 , generator loss=0.691\n",
      "Training progress in epoch #99, step 162, discriminator loss=0.691 , generator loss=0.698\n",
      "Training progress in epoch #99, step 163, discriminator loss=0.690 , generator loss=0.695\n",
      "Training progress in epoch #99, step 164, discriminator loss=0.696 , generator loss=0.698\n",
      "Training progress in epoch #99, step 165, discriminator loss=0.694 , generator loss=0.690\n",
      "Training progress in epoch #99, step 166, discriminator loss=0.694 , generator loss=0.696\n",
      "Training progress in epoch #99, step 167, discriminator loss=0.688 , generator loss=0.704\n",
      "Training progress in epoch #99, step 168, discriminator loss=0.695 , generator loss=0.703\n",
      "Training progress in epoch #99, step 169, discriminator loss=0.694 , generator loss=0.703\n",
      "Training progress in epoch #99, step 170, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #99, step 171, discriminator loss=0.695 , generator loss=0.709\n",
      "Training progress in epoch #99, step 172, discriminator loss=0.692 , generator loss=0.706\n",
      "Training progress in epoch #99, step 173, discriminator loss=0.684 , generator loss=0.687\n",
      "Training progress in epoch #99, step 174, discriminator loss=0.691 , generator loss=0.700\n",
      "Training progress in epoch #99, step 175, discriminator loss=0.692 , generator loss=0.702\n",
      "Training progress in epoch #99, step 176, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #99, step 177, discriminator loss=0.691 , generator loss=0.678\n",
      "Training progress in epoch #99, step 178, discriminator loss=0.696 , generator loss=0.696\n",
      "Training progress in epoch #99, step 179, discriminator loss=0.687 , generator loss=0.699\n",
      "Training progress in epoch #99, step 180, discriminator loss=0.689 , generator loss=0.710\n",
      "Training progress in epoch #99, step 181, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #99, step 182, discriminator loss=0.690 , generator loss=0.704\n",
      "Training progress in epoch #99, step 183, discriminator loss=0.692 , generator loss=0.692\n",
      "Training progress in epoch #99, step 184, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #99, step 185, discriminator loss=0.691 , generator loss=0.710\n",
      "Training progress in epoch #99, step 186, discriminator loss=0.692 , generator loss=0.703\n",
      "Training progress in epoch #99, step 187, discriminator loss=0.694 , generator loss=0.705\n",
      "Training progress in epoch #99, step 188, discriminator loss=0.700 , generator loss=0.700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress in epoch #99, step 189, discriminator loss=0.690 , generator loss=0.693\n",
      "Training progress in epoch #99, step 190, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #99, step 191, discriminator loss=0.697 , generator loss=0.691\n",
      "Training progress in epoch #99, step 192, discriminator loss=0.695 , generator loss=0.684\n",
      "Training progress in epoch #99, step 193, discriminator loss=0.690 , generator loss=0.687\n",
      "Training progress in epoch #99, step 194, discriminator loss=0.692 , generator loss=0.699\n",
      "Training progress in epoch #99, step 195, discriminator loss=0.693 , generator loss=0.717\n",
      "Training progress in epoch #99, step 196, discriminator loss=0.693 , generator loss=0.716\n",
      "Training progress in epoch #99, step 197, discriminator loss=0.692 , generator loss=0.708\n",
      "Training progress in epoch #99, step 198, discriminator loss=0.701 , generator loss=0.704\n",
      "Training progress in epoch #99, step 199, discriminator loss=0.695 , generator loss=0.696\n",
      "Training progress in epoch #99, step 200, discriminator loss=0.689 , generator loss=0.696\n",
      "Training progress in epoch #99, step 201, discriminator loss=0.690 , generator loss=0.697\n",
      "Training progress in epoch #99, step 202, discriminator loss=0.690 , generator loss=0.703\n",
      "Training progress in epoch #99, step 203, discriminator loss=0.695 , generator loss=0.699\n",
      "Training progress in epoch #99, step 204, discriminator loss=0.694 , generator loss=0.701\n",
      "Training progress in epoch #99, step 205, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #99, step 206, discriminator loss=0.692 , generator loss=0.701\n",
      "Training progress in epoch #99, step 207, discriminator loss=0.691 , generator loss=0.707\n",
      "Training progress in epoch #99, step 208, discriminator loss=0.694 , generator loss=0.697\n",
      "Training progress in epoch #99, step 209, discriminator loss=0.693 , generator loss=0.694\n",
      "Training progress in epoch #99, step 210, discriminator loss=0.691 , generator loss=0.690\n",
      "Training progress in epoch #99, step 211, discriminator loss=0.693 , generator loss=0.697\n",
      "Training progress in epoch #99, step 212, discriminator loss=0.693 , generator loss=0.709\n",
      "Training progress in epoch #99, step 213, discriminator loss=0.695 , generator loss=0.707\n",
      "Training progress in epoch #99, step 214, discriminator loss=0.693 , generator loss=0.701\n",
      "Training progress in epoch #99, step 215, discriminator loss=0.689 , generator loss=0.695\n",
      "Training progress in epoch #99, step 216, discriminator loss=0.693 , generator loss=0.695\n",
      "Training progress in epoch #99, step 217, discriminator loss=0.695 , generator loss=0.692\n",
      "Training progress in epoch #99, step 218, discriminator loss=0.690 , generator loss=0.694\n",
      "Training progress in epoch #99, step 219, discriminator loss=0.689 , generator loss=0.691\n",
      "Training progress in epoch #99, step 220, discriminator loss=0.693 , generator loss=0.714\n",
      "Training progress in epoch #99, step 221, discriminator loss=0.688 , generator loss=0.717\n",
      "Training progress in epoch #99, step 222, discriminator loss=0.688 , generator loss=0.716\n",
      "Training progress in epoch #99, step 223, discriminator loss=0.685 , generator loss=0.721\n",
      "Training progress in epoch #99, step 224, discriminator loss=0.692 , generator loss=0.715\n",
      "Training progress in epoch #99, step 225, discriminator loss=0.696 , generator loss=0.682\n",
      "Training progress in epoch #99, step 226, discriminator loss=0.687 , generator loss=0.677\n",
      "Training progress in epoch #99, step 227, discriminator loss=0.695 , generator loss=0.679\n",
      "Training progress in epoch #99, step 228, discriminator loss=0.692 , generator loss=0.697\n",
      "Training progress in epoch #99, step 229, discriminator loss=0.690 , generator loss=0.706\n",
      "Training progress in epoch #99, step 230, discriminator loss=0.691 , generator loss=0.718\n",
      "Training progress in epoch #99, step 231, discriminator loss=0.693 , generator loss=0.725\n",
      "Training progress in epoch #99, step 232, discriminator loss=0.687 , generator loss=0.700\n",
      "Training progress in epoch #99, step 233, discriminator loss=0.692 , generator loss=0.694\n",
      "Disciminator Accuracy on real images: 73%, on fake images: 52%\n"
     ]
    }
   ],
   "source": [
    "training_gan(gan_model, discriminator, generator, batch_size, epochs, epoch_steps, noise_dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples of the Gnerator outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mohammedalhamid/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/mohammedalhamid/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Loading the generator model\n",
    "model = tf.keras.models.load_model('generator_model_0099.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random noise for the generator\n",
    "X_gan = np.random.randn(100 * 10)\n",
    "X_gan = X_gan.reshape(10, 100)\n",
    "X_gan = model.predict(X_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 28, 28, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_gan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us visualize 10 examples: \n",
    "# set the subplot\n",
    "fig, axs = plt.subplots(2, 5)\n",
    "for i in range(2):\n",
    "    for j in range(5):  \n",
    "    # plot image pixesles\n",
    "        axs[i,j].imshow(np.reshape(X_gan[i+j], (28,28)), cmap='binary')\n",
    "# Display the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Generative Adversarial Network in Keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
